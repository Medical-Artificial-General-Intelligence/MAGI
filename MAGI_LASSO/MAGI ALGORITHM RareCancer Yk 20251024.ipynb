{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import math\n",
    "from typing import Dict, List, Union, Any, Optional, Set, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, re\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = os.getenv(\"MAGI_DB_PATH\", \"/projects/klybarge/pcori_ad/magi/magi_db/magi.db\")  # read-only; override via env\n",
    "OUT_DIR = \"./Mesothelioma\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_top500\"   \n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "\n",
    "TOP_K = 500  # 500 for others\n",
    "\n",
    "TARGETS = [\n",
    "    \"dx_SNOMED_254645002\",\n",
    "]\n",
    "\n",
    "# Updated MAGI AUC=0.7172\n",
    "# T_{k,Y}  (target=Y, concept=k).\n",
    "# λ_{k,j}  (target=j, concept=k).\n",
    "# ===== SNOMED parents+descendants support =====\n",
    "IS_A = \"116680003\"\n",
    "CHAR_TYPES = {\n",
    "    \"inferred\": \"900000000000011006\",  # classification hierarchy\n",
    "    \"stated\":   \"900000000000010007\",\n",
    "}\n",
    "\n",
    "SNOMED_REL_FULL_US = \"/projects/klybarge/pcori_ad/magi/Test/Test/Mesothelioma/sct2_Relationship_Full_US1000124_20250901.txt\"\n",
    "\n",
    "def build_is_a_snapshot(full_rel_path: str, characteristic: str = \"inferred\") -> pd.DataFrame:\n",
    "    \"\"\"From a Full RF2 file, return the *current* active IS-A rows.\"\"\"\n",
    "    use_cols = [\n",
    "        \"id\",\"effectiveTime\",\"active\",\"moduleId\",\n",
    "        \"sourceId\",\"destinationId\",\"relationshipGroup\",\n",
    "        \"typeId\",\"characteristicTypeId\",\"modifierId\"\n",
    "    ]\n",
    "    df = pd.read_csv(full_rel_path, sep=\"\\t\", dtype=str, usecols=use_cols)\n",
    "    df = df[(df[\"typeId\"] == IS_A) & (df[\"characteristicTypeId\"] == CHAR_TYPES[characteristic])]\n",
    "    df[\"effectiveTime_num\"] = df[\"effectiveTime\"].astype(int)\n",
    "    idx = df.groupby(\"id\")[\"effectiveTime_num\"].idxmax()\n",
    "    snap = df.loc[idx]\n",
    "    snap = snap[snap[\"active\"] == \"1\"][[\"sourceId\",\"destinationId\"]].reset_index(drop=True)\n",
    "    return snap\n",
    "\n",
    "# Build parent→children and child→parent maps once (lazy init so the script still runs if file missing)\n",
    "__SNAP_REL__ = None\n",
    "__P2C__ = None\n",
    "__C2P__ = None\n",
    "\n",
    "def _ensure_graph():\n",
    "    global __SNAP_REL__, __P2C__, __C2P__\n",
    "    if __SNAP_REL__ is None:\n",
    "        __SNAP_REL__ = build_is_a_snapshot(SNOMED_REL_FULL_US, characteristic=\"inferred\")\n",
    "    if __P2C__ is None or __C2P__ is None:\n",
    "        __P2C__ = defaultdict(set)\n",
    "        __C2P__ = defaultdict(set)\n",
    "        # RF2: destinationId = parent, sourceId = child\n",
    "        for parent, child in zip(__SNAP_REL__[\"destinationId\"], __SNAP_REL__[\"sourceId\"]):\n",
    "            __P2C__[parent].add(child)\n",
    "            __C2P__[child].add(parent)\n",
    "\n",
    "def find_descendants_sct(concept_id: str) -> set:\n",
    "    \"\"\"All is-a descendants (children, grandchildren, ...) for a SNOMED conceptId.\"\"\"\n",
    "    _ensure_graph()\n",
    "    out = set()\n",
    "    q = deque([concept_id])\n",
    "    while q:\n",
    "        cur = q.popleft()\n",
    "        for kid in __P2C__.get(cur, ()):\n",
    "            if kid not in out:\n",
    "                out.add(kid)\n",
    "                q.append(kid)\n",
    "    return out\n",
    "\n",
    "def find_ancestors_sct(concept_id: str) -> set:\n",
    "    \"\"\"All is-a ancestors (parents, grandparents, ...) for a SNOMED conceptId.\"\"\"\n",
    "    _ensure_graph()\n",
    "    out = set()\n",
    "    q = deque([concept_id])\n",
    "    while q:\n",
    "        cur = q.popleft()\n",
    "        for mom in __C2P__.get(cur, ()):\n",
    "            if mom not in out:\n",
    "                out.add(mom)\n",
    "                q.append(mom)\n",
    "    return out\n",
    "\n",
    "def extract_snomed_id(code: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    If code looks like 'dx_SNOMED_<digits>' return '<digits>', else None.\n",
    "    \"\"\"\n",
    "    m = re.fullmatch(r\"dx_SNOMED_(\\d+)\", str(code))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def snomed_aliases_for_outcome(outcome_code: str, *, include_parents: bool = True) -> Tuple[Optional[Set[str]], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Given an outcome like 'dx_SNOMED_254645002', return:\n",
    "      - aliases_codes: {'dx_SNOMED_<id>', ...} including the root itself (or None if not SNOMED)\n",
    "      - name_map: mapping every alias_code -> outcome_code (canonical)\n",
    "    If include_parents=True, add all ancestors to the alias family as well.\n",
    "    \"\"\"\n",
    "    root_id = extract_snomed_id(outcome_code)\n",
    "    if root_id is None:\n",
    "        return None, {}\n",
    "\n",
    "    # descendants\n",
    "    desc = find_descendants_sct(root_id)\n",
    "    all_ids = {root_id} | set(desc)\n",
    "\n",
    "    # parents/ancestors (optional)\n",
    "    if include_parents:\n",
    "        ancs = find_ancestors_sct(root_id)\n",
    "        all_ids |= set(ancs)\n",
    "\n",
    "    aliases_codes: Set[str] = {f\"dx_SNOMED_{sid}\" for sid in all_ids}\n",
    "    # map every alias (except the canonical root code string itself) back to outcome_code\n",
    "    name_map: Dict[str, str] = {alias: outcome_code for alias in aliases_codes if alias != outcome_code}\n",
    "    return aliases_codes, name_map\n",
    "\n",
    "\n",
    "def analyze_causal_sequence_py(\n",
    "    df_in: Union[str, pd.DataFrame],\n",
    "    *,\n",
    "    name_map: Dict[str, str] = None,     # raw_code -> friendly label (applied to BOTH columns)\n",
    "    events: List[str] = None,            # event names to KEEP (AFTER recoding). If None: auto-detect\n",
    "    force_outcome: str = None,           # if provided and present, force this to be the FINAL node (Y)\n",
    "    lambda_min_count: int = 15           # L-threshold for λ: if n_code < L ⇒ λ_{k,j}=0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    MAGI (Python): Reference routine with explicit comments.\n",
    "\n",
    "    ─────────────────────────────────────────────────────────────────────────────\n",
    "    COLUMN CONVENTION PER ROW:\n",
    "      Left  column: target_concept_code  (call this X for this row)\n",
    "      Right column: concept_code         (call this k for this row)\n",
    "\n",
    "      n_code_target        ≡  k ∧ X\n",
    "      n_code_no_target     ≡  k ∧ ¬X\n",
    "      n_target_no_code     ≡  ¬k ∧ X\n",
    "      n_no_target          ≡  total(¬X)\n",
    "      n_target             ≡  total(X)\n",
    "      n_code               ≡  total(k)\n",
    "      n_code_before_target ≡  count(k before X)\n",
    "      n_target_before_code ≡  count(X before k)\n",
    "\n",
    "    ORIENTATION (locked to your spec):\n",
    "      • Total effect T_{kY}:   read row (target = Y, code = k).\n",
    "      • Lambda     λ_{k,j}:    read row (target = j, code = k), and compute\n",
    "                               λ_{k,j} = n_code_target / n_code  with L-threshold on n_code.\n",
    "\n",
    "    TEMPORAL SCORE for each node Zi:\n",
    "      Score(Z_i) = Σ_{j≠i} [ C(Z_i≺Z_j) - C(Z_j≺Z_i) + C(Z_i∧¬Z_j) - C(Z_j∧¬Z_i) ]\n",
    "      Read from row (target=Z_i, code=Z_j):\n",
    "        - n_code_before_target      → C(Z_j≺Z_i)\n",
    "        - n_target_before_code      → C(Z_i≺Z_j)\n",
    "        - n_code_no_target          → C(Z_j∧¬Z_i)\n",
    "        - n_target_no_code          → C(Z_i∧¬Z_j)\n",
    "\n",
    "    T_{kY} from row (Y, k):\n",
    "      a = n_code_target        (k ∧ Y)\n",
    "      b = n_code_no_target     (k ∧ ¬Y)\n",
    "      c = n_target_no_code     (¬k ∧ Y)\n",
    "      d = n_no_target - b      (¬k ∧ ¬Y)   ← computed on the fly (no extra column needed)\n",
    "      With sample-size–anchored odds:\n",
    "         odds_k1 = a/b with guards; odds_k0 = c/d with guards; T = odds_k1 / odds_k0\n",
    "\n",
    "    DIRECT EFFECTS via backward recursion:\n",
    "      D_{k,Y} = ( T_{k,Y} - Σ_j λ_{k,j} D_{j,Y} ) / ( 1 - Σ_j λ_{k,j} ),\n",
    "      where j are downstream nodes between k and Y in the temporal order.\n",
    "\n",
    "    LOGISTIC LINK:\n",
    "      logit P(Y=1 | Z) = β0 + Σ_k β_k Z_k  with β_k = log D_{k,Y};\n",
    "      invalid/nonpositive D map to β_k=0.\n",
    "\n",
    "    RETURNS a dict with:\n",
    "      sorted_scores, temporal_order, order_used,\n",
    "      T_val, D_val, lambda_l, coef_df, beta_0, beta, logit_predictors, predict_proba\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 0) Ingest & validate ───────────────────────────────────────────────────\n",
    "    df = pd.read_csv(df_in) if isinstance(df_in, str) else df_in.copy()\n",
    "\n",
    "    need_cols = [\n",
    "        \"target_concept_code\", \"concept_code\",\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\",\n",
    "        \"n_target_no_code\",\n",
    "        \"n_code\",                                 # for λ denominator\n",
    "        \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # Optional recoding to friendly labels (applied to BOTH endpoints)\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "\n",
    "    # Limit to selected events (union if auto)\n",
    "    if events is None:\n",
    "        ev_t = df[\"target_concept_code\"].astype(str).unique().tolist()\n",
    "        ev_c = df[\"concept_code\"].astype(str).unique().tolist()\n",
    "        events = sorted(set(ev_t) | set(ev_c))\n",
    "    else:\n",
    "        # Normalize types to avoid silent mismatches\n",
    "        events = [str(e) for e in events]\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].astype(str)\n",
    "        df[\"concept_code\"] = df[\"concept_code\"].astype(str)\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events.\")\n",
    "\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numerics; NA→0 to allow safe sums/max\n",
    "    num_cols = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\", \"n_target_no_code\",\n",
    "        \"n_code\", \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # ── 1) Temporal score (read rows: target=Z_i, code=Z_j) ────────────────────\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zi in events:\n",
    "        s = 0.0\n",
    "        for zj in (x for x in events if x != zi):\n",
    "            # Row oriented as (Zi, Zj)\n",
    "            pair = df[(df[\"target_concept_code\"] == zi) & (df[\"concept_code\"] == zj)]\n",
    "            if pair.empty:\n",
    "                continue\n",
    "            c_i_before_j = float(pair[\"n_target_before_code\"].sum(skipna=True))   # Zi before Zj\n",
    "            c_j_before_i = float(pair[\"n_code_before_target\"].sum(skipna=True))   # Zj before Zi\n",
    "            c_i_and_not_j = float(pair[\"n_target_no_code\"].sum(skipna=True))      # Zi ∧ ¬Zj\n",
    "            c_j_and_not_i = float(pair[\"n_code_no_target\"].sum(skipna=True))      # Zj ∧ ¬Zi\n",
    "            s += (c_i_before_j - c_j_before_i + c_i_and_not_j - c_j_and_not_i)\n",
    "        scores[zi] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Choose outcome Y: either forced or top-scoring node\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "    else:\n",
    "        outcome = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "\n",
    "    events_order = temporal_order              # earliest … → Y\n",
    "    nodes = events_order[:-1]            # everything before Y\n",
    "\n",
    "    # ── 2) T and λ (row orientations locked) ───────────────────────────────────\n",
    "    T_val = pd.Series(0.0, index=nodes, dtype=float)\n",
    "    D_val = pd.Series(np.nan, index=nodes, dtype=float)\n",
    "    lambda_l: Dict[str, pd.Series] = {}\n",
    "\n",
    "    for k in nodes:\n",
    "        # ---- T_{kY} from the single row (target=Y, code=k) ----\n",
    "        row_Yk = df[(df[\"target_concept_code\"] == outcome) & (df[\"concept_code\"] == k)]\n",
    "\n",
    "        # 2×2 cells (a,b,c,d) at (Y,k):\n",
    "        a = float(row_Yk[\"n_code_target\"].sum(skipna=True))        # k ∧ Y\n",
    "        b = float(row_Yk[\"n_code_no_target\"].sum(skipna=True))     # k ∧ ¬Y\n",
    "        c = float(row_Yk[\"n_target_no_code\"].sum(skipna=True))     # ¬k ∧ Y\n",
    "        # d is not in data; compute from the same row: d = total(¬Y) − (k ∧ ¬Y)\n",
    "        n_noY = float(row_Yk[\"n_no_target\"].max(skipna=True)) if not row_Yk.empty else 0.0\n",
    "        d = max(n_noY - b, 0.0)                                    # ¬k ∧ ¬Y\n",
    "\n",
    "        # Stratum sizes\n",
    "        N1 = a + b                   # k = 1\n",
    "        N0 = c + d                   # k = 0\n",
    "\n",
    "        # Sample-size–anchored odds for k=1 and k=0\n",
    "        if N1 == 0:\n",
    "            odds_k1 = 1.0\n",
    "        else:\n",
    "            if a == 0:\n",
    "                odds_k1 = 1.0 / (N1 + 1.0)\n",
    "            elif b == 0:\n",
    "                odds_k1 = (N1 + 1.0)\n",
    "            else:\n",
    "                odds_k1 = a / b\n",
    "\n",
    "        if N0 == 0:\n",
    "            odds_k0 = 1.0\n",
    "        else:\n",
    "            if c == 0:\n",
    "                odds_k0 = 1.0 / (N0 + 1.0)\n",
    "            elif d == 0:\n",
    "                odds_k0 = (N0 + 1.0)\n",
    "            else:\n",
    "                odds_k0 = c / d\n",
    "\n",
    "        T_val.loc[k] = float(odds_k1 / odds_k0) if odds_k0 > 0 else (N1 + 1.0)\n",
    "\n",
    "        # ---- λ_{k,j} from rows (target=j, code=k): λ = n_code_target / n_code ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_jk = df[(df[\"target_concept_code\"] == j) & (df[\"concept_code\"] == k)]\n",
    "            if row_jk.empty:\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            num = float(pd.to_numeric(row_jk[\"n_code_target\"], errors=\"coerce\").fillna(0.0).sum())\n",
    "            den = float(pd.to_numeric(row_jk[\"n_code\"],        errors=\"coerce\").fillna(0.0).sum())\n",
    "\n",
    "            # L-threshold on the conditioning size n_code (count of k)\n",
    "            if (den <= 0) or (den < lambda_min_count):\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            lam = num / den\n",
    "            lam = 0.0 if not np.isfinite(lam) else float(min(max(lam, 0.0), 1.0))\n",
    "            lam_pairs.append((j, lam))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ── 3) Backward recursion for direct effects D ─────────────────────────────\n",
    "    # Last node (just before Y): no downstream → D = T\n",
    "    if len(nodes) >= 1:\n",
    "        last_anc = nodes[-1]\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "\n",
    "    # Walk backward for the rest\n",
    "    if len(nodes) > 1:\n",
    "        for k in list(reversed(nodes[:-1])):\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            downstream = list(lam_vec.index)  # j nodes after k (already resolved)\n",
    "            lam_vals = lam_vec.reindex(downstream).fillna(0.0).to_numpy()\n",
    "            D_down  = pd.to_numeric(D_val.reindex(downstream), errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vals * D_down))\n",
    "            den = 1.0 - float(np.nansum(lam_vals))\n",
    "\n",
    "            if (not np.isfinite(den)) or den == 0.0:\n",
    "                D_val.loc[k] = T_val.loc[k]            # neutralize if pathological\n",
    "            else:\n",
    "                tmp = num / den\n",
    "                D_val.loc[k] = tmp if np.isfinite(tmp) else T_val.loc[k]\n",
    "\n",
    "    # ── 4) Logistic link (β) and predict_proba ─────────────────────────────────\n",
    "    # Intercept β0 from marginal prevalence of Y (rows with target == Y)\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    n_t = float(resp_rows[\"n_target\"].max(skipna=True)) if not resp_rows.empty else np.nan\n",
    "    n_n = float(resp_rows[\"n_no_target\"].max(skipna=True)) if not resp_rows.empty else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    p_y = min(max(p_y, 1e-12), 1 - 1e-12)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))\n",
    "\n",
    "    # β_k = log D_{k,Y}; invalid/nonpositive → 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    beta_vals = np.log(D_clean.where(D_clean > 0.0)).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_vals.index) + [\"(intercept)\"],\n",
    "        \"beta\":      list(beta_vals.values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # Vectorized predict_proba\n",
    "    predictors = list(beta_vals.index)\n",
    "    beta_vec = beta_vals.values\n",
    "\n",
    "    def predict_proba(Z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Compute P(Y=1|Z) using: logit P = β0 + Σ_k β_k Z_k.\n",
    "        Z can be:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame containing any/all of `predictors` (others ignored)\n",
    "        \"\"\"\n",
    "        def sigmoid(x):\n",
    "            x = np.clip(x, -700, 700)  # numerical stability for large |η|\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "        if isinstance(Z, pd.DataFrame):\n",
    "            M = Z.reindex(columns=predictors, fill_value=0.0).astype(float).to_numpy()\n",
    "            return sigmoid(beta_0 + M @ beta_vec)\n",
    "\n",
    "        if isinstance(Z, (dict, pd.Series)):\n",
    "            v = np.array([float(Z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            return float(sigmoid(beta_0 + float(v @ beta_vec)))\n",
    "\n",
    "        arr = np.asarray(Z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            return float(sigmoid(beta_0 + float(arr @ beta_vec)))\n",
    "        if arr.ndim == 2:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*,{len(predictors)}), got {arr.shape}\")\n",
    "            return sigmoid(beta_0 + arr @ beta_vec)\n",
    "\n",
    "        raise ValueError(\"Unsupported input for predict_proba\")\n",
    "\n",
    "    # ── 5) Package results ─────────────────────────────────────────────────────\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "\n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only coerce numerics and compute total_effect if absent, using provided cells and\n",
    "    computing d = n_no_target - n_code_no_target on the fly (since d isn't in data).\n",
    "    \"\"\"\n",
    "    required = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target_no_code\", \"n_target\", \"n_no_target\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns for TE: {', '.join(missing)}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    for c in required:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    if \"total_effect\" not in out.columns:\n",
    "        a = out[\"n_code_target\"].astype(float)\n",
    "        b = out[\"n_code_no_target\"].astype(float)\n",
    "        c = out[\"n_target_no_code\"].astype(float)\n",
    "        d = (out[\"n_no_target\"] - out[\"n_code_no_target\"]).astype(float)\n",
    "        d = np.maximum(d, 0.0)\n",
    "\n",
    "        N1 = a + b\n",
    "        N0 = c + d\n",
    "\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            odds_k1 = np.where(\n",
    "                N1 == 0, 1.0,\n",
    "                np.where((a > 0) & (b > 0), a / b,\n",
    "                         np.where(b == 0, N1 + 1.0, 1.0 / (N1 + 1.0)))\n",
    "            )\n",
    "            odds_k0 = np.where(\n",
    "                N0 == 0, 1.0,\n",
    "                np.where((c > 0) & (d > 0), c / d,\n",
    "                         np.where(d == 0, N0 + 1.0, 1.0 / (N0 + 1.0)))\n",
    "            )\n",
    "            te = odds_k1 / odds_k0\n",
    "\n",
    "        out[\"total_effect\"] = np.where(np.isfinite(te), te, 1.0).astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# TOPK_HI/LO\n",
    "def _select_top_by_te_unique_k(k_to_T: pd.DataFrame, top_k: int = TOP_K,\n",
    "                               hi: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced selection without fallback:\n",
    "      - Take up to top_k//2 strongest risk (TE >= hi)\n",
    "      - Take up to top_k - top_k//2 strongest protective (TE <= 1/hi)\n",
    "      - If either side is short, do NOT fill from elsewhere; total may be < top_k\n",
    "      - Return one row per k (**concept_code**), ranked by extremeness\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    # Ensure total_effect numeric\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with total_effect; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # Extremeness symmetrical around 1\n",
    "    df[\"effect_strength\"] = np.where(df[\"total_effect\"] >= 1.0,\n",
    "                                     df[\"total_effect\"],\n",
    "                                     1.0 / df[\"total_effect\"])\n",
    "\n",
    "    # One row per k: **k is concept_code in (target=Y, code=k)**\n",
    "    best_per_k = (df.sort_values(\"effect_strength\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"concept_code\"], keep=\"first\"))\n",
    "\n",
    "    HI = hi\n",
    "    LO = 1.0 / hi\n",
    "\n",
    "    risk_pool = best_per_k[best_per_k[\"total_effect\"] >= HI].copy()\n",
    "    prot_pool = best_per_k[best_per_k[\"total_effect\"] <= LO].copy()\n",
    "\n",
    "    want_risk = top_k // 2\n",
    "    want_prot = top_k - want_risk\n",
    "\n",
    "    sel_risk = risk_pool.nlargest(min(len(risk_pool), want_risk), \"effect_strength\")\n",
    "    sel_prot = prot_pool.nlargest(min(len(prot_pool), want_prot), \"effect_strength\")\n",
    "\n",
    "    selected = pd.concat([sel_risk, sel_prot], ignore_index=True)\n",
    "\n",
    "    print(f\"[SELECT] total unique k={best_per_k['concept_code'].nunique():,}  \"\n",
    "          f\"risk={len(risk_pool):,}  prot={len(prot_pool):,}  \"\n",
    "          f\"selected(total)={selected['concept_code'].nunique()}  \"\n",
    "          f\"with: risk={len(sel_risk)}, prot={len(sel_prot)}\")\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _fetch_k_to_T(conn, outcome_code: str) -> pd.DataFrame:\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[outcome_code])\n",
    "\n",
    "def _fetch_k_to_T_in(conn, target_codes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch rows whose LEFT (target) is in target_codes.\n",
    "    This is used to include alias outcomes (descendants of T) on the LEFT.\n",
    "    \"\"\"\n",
    "    tmp_name = \"tmp_targets_magi\"\n",
    "    with conn:\n",
    "        conn.execute(f\"DROP TABLE IF EXISTS {tmp_name}\")\n",
    "        conn.execute(f\"CREATE TEMP TABLE {tmp_name}(concept_code TEXT)\")\n",
    "        conn.executemany(f\"INSERT INTO {tmp_name}(concept_code) VALUES (?)\", [(c,) for c in target_codes])\n",
    "\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- LEFT  (Y or Y-alias)\n",
    "             ccn.concept_code AS concept_code           -- RIGHT (k)\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      JOIN {tmp_name} tmp     ON tcn.concept_code         = tmp.concept_code   -- <<< LEFT filter\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn)\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list):\n",
    "    \"\"\"\n",
    "    Induced subgraph where LEFT is in events_list.\n",
    "    \"\"\"\n",
    "    ph = \",\".join([\"?\"] * len(events_list))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- LEFT\n",
    "             ccn.concept_code AS concept_code           -- RIGHT\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=list(events_list))\n",
    "\n",
    "def expand_targets_with_descendants(targets: List[str]) -> Dict[str, Set[str]]:\n",
    "    out: Dict[str, Set[str]] = {}\n",
    "    for T in targets:\n",
    "        aliases, _name_map = snomed_aliases_for_outcome(T, include_parents=True)\n",
    "        out[T] = aliases if aliases is not None else {T}\n",
    "    return out\n",
    "\n",
    "def print_full_targets(targets: List[str], *, save_csv: bool = True, out_dir: str = \"./\"):\n",
    "    \"\"\"\n",
    "    Pretty-print the expanded target definitions and (optionally) save a CSV with all rows:\n",
    "      root_target, alias_code, alias_conceptId, is_root\n",
    "    \"\"\"\n",
    "    expanded = expand_targets_with_descendants(targets)\n",
    "\n",
    "    # pretty print to console\n",
    "    for T, alias_set in expanded.items():\n",
    "        n = len(alias_set)\n",
    "        sample = \", \".join(sorted(list(alias_set))[:10])\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"[TARGET] {T}\")\n",
    "        print(f\"  aliases (parents + descendants + root): {n}\")\n",
    "        print(f\"  sample: {sample}{' ...' if n > 10 else ''}\")\n",
    "\n",
    "    # optional CSV dump\n",
    "    if save_csv:\n",
    "        rows = []\n",
    "        for T, alias_set in expanded.items():\n",
    "            root_id = extract_snomed_id(T)\n",
    "            for alias in sorted(alias_set):\n",
    "                alias_id = extract_snomed_id(alias)\n",
    "                rows.append({\n",
    "                    \"root_target\": T,\n",
    "                    \"alias_code\": alias,\n",
    "                    \"alias_conceptId\": alias_id if alias_id else \"\",\n",
    "                    \"is_root\": (alias == T)\n",
    "                })\n",
    "        df = pd.DataFrame(rows, columns=[\"root_target\",\"alias_code\",\"alias_conceptId\",\"is_root\"])\n",
    "        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        path = os.path.join(out_dir, f\"targets_expanded_{ts}.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"\\n[SAVED] Full target expansions → {path}\")\n",
    "\n",
    "    return expanded\n",
    "\n",
    "# ========= main loop (remove dup) =========\n",
    "if __name__ == '__main__':\n",
    "    # ---- Runtime safety checks ----\n",
    "    uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "    if not os.path.exists(MAGI_DB_PATH):\n",
    "        raise FileNotFoundError(f\"MAGI_DB_PATH not found: {MAGI_DB_PATH}. Set MAGI_DB_PATH to a valid SQLite DB or run with CSV input.\")\n",
    "    with sqlite3.connect(uri, uri=True) as conn:\n",
    "        # ---- Save expanded targets list ----\n",
    "        expanded_targets = print_full_targets(TARGETS, save_csv=True, out_dir=OUT_DIR)\n",
    "    for T in TARGETS:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "        # --- Identify alias family (parents + descendants + root) for filtering only\n",
    "        #     We will NOT use these as targets anywhere; we just detect & later drop if they appear as predictors.\n",
    "        aliases_codes, _name_map_alias = snomed_aliases_for_outcome(T, include_parents=True)\n",
    "        alias_family = set(aliases_codes) if aliases_codes else set()\n",
    "        if alias_family:\n",
    "            print(f\"[ALIASES] Identified {len(alias_family):,} alias codes for {T} (parents+descendants+root).\")\n",
    "        else:\n",
    "            print(\"[ALIASES] Non-SNOMED or no aliases detected; proceeding without alias filtering.\")\n",
    "\n",
    "        # 1) k→T rows: **ONLY the root target T** on the LEFT (no parents/descendants as target)\n",
    "        k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "        # --- Optional: audit if any alias codes show up as RIGHT-side predictors (concept_code)\n",
    "        if aliases_codes and not k_to_T.empty:\n",
    "            present_mask = k_to_T[\"concept_code\"].isin(alias_family)\n",
    "            k_to_T_alias = k_to_T.loc[present_mask].copy()\n",
    "            if not k_to_T_alias.empty:\n",
    "                alias_break = (\n",
    "                    k_to_T_alias\n",
    "                    .groupby(\"concept_code\", as_index=False)\n",
    "                    .agg(\n",
    "                        rows=(\"concept_code\", \"size\"),\n",
    "                        n_code_target=(\"n_code_target\", \"sum\"),\n",
    "                        n_code_no_target=(\"n_code_no_target\", \"sum\"),\n",
    "                        n_target=(\"n_target\", \"sum\"),\n",
    "                        n_no_target=(\"n_no_target\", \"sum\"),\n",
    "                    )\n",
    "                    .sort_values(\"rows\", ascending=False)\n",
    "                )\n",
    "                alias_csv = os.path.join(OUT_DIR, f\"alias_predictors_present_{T}.csv\")\n",
    "                alias_break.to_csv(alias_csv, index=False)\n",
    "                print(f\"[AUDIT] Alias-family predictors present among k→T rows → {alias_csv}\")\n",
    "            else:\n",
    "                print(\"[AUDIT] No alias-family predictors present among k→T rows.\")\n",
    "\n",
    "        # --- Drop exact duplicate rows (all columns identical)\n",
    "        before = len(k_to_T)\n",
    "        k_to_T = k_to_T.drop_duplicates()\n",
    "        after = len(k_to_T)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] k→T exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        if k_to_T.empty:\n",
    "            print(f\"[WARN] No k→T rows for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects on the k→T list\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) select predictors: top-K by total_effect (one row per k).  (No alias-target usage.)\n",
    "        sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "        if sel_rows.empty:\n",
    "            print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        selected_k = set(sel_rows[\"concept_code\"].astype(str))  # predictors are on RIGHT\n",
    "        print(f\"[SELECT] unique k available={k_to_T['concept_code'].nunique():,}  \"\n",
    "            f\"selected={len(selected_k):,}\")\n",
    "\n",
    "        # 4) build subgraph among {T} ∪ selected_k\n",
    "        #    LEFT is always in events_set (targets); RIGHT limited to events_set as well.\n",
    "        #    (**Do not** expand RIGHT with alias family; we analyze only the chosen predictors and T.)\n",
    "        events_set = selected_k | {T}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].isin(events_set) &\n",
    "            df_trim[\"concept_code\"].isin(events_set)\n",
    "        ].copy()\n",
    "\n",
    "        # --- Drop exact duplicate rows\n",
    "        before = len(df_trim)\n",
    "        df_trim = df_trim.drop_duplicates()\n",
    "        after = len(df_trim)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "        df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "        # Reporting\n",
    "        k_to_T_count = (df_trim[\"concept_code\"] == T).sum()\n",
    "        T_to_j_count = (df_trim[\"target_concept_code\"] == T).sum()\n",
    "        print(f\"[TRIM] rows={len(df_trim):,}  events={len(events_set)}  \"\n",
    "            f\"k→T rows={k_to_T_count}  T→j rows={T_to_j_count}\")\n",
    "\n",
    "        # 5) save subgraph (for audit)\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "        # 6) run MAGI — analyze with **only** the canonical root outcome T\n",
    "        try:\n",
    "            # Do NOT pass name_map; do NOT canonicalize aliases; no alias-target expansion.\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=None, force_outcome=T)\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] MAGI failed:\", e)\n",
    "            continue\n",
    "\n",
    "        # 7) save NON-ZERO coefficients, then REMOVE any parent/descendant predictors\n",
    "        outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "        coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "        # robust: accept 'coef' or 'beta' column name\n",
    "        coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "        if coef_col is None:\n",
    "            raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "        # filter to non-zero (tolerance to avoid tiny numerical noise)\n",
    "        eps = 1e-12\n",
    "        mask_nz = coef_df[coef_col].astype(float).abs() > eps\n",
    "        coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "        # Identify predictor column name (commonly 'predictor')\n",
    "        pred_col = \"predictor\" if \"predictor\" in coef_nz.columns else None\n",
    "        if pred_col is None:\n",
    "            # Fallback: try to infer the predictor column as the first non-coef column not equal to '(intercept)'\n",
    "            maybe_pred_cols = [c for c in coef_nz.columns if c not in {coef_col}]\n",
    "            if \"(intercept)\" in coef_nz.get(maybe_pred_cols[0], \"\").values if maybe_pred_cols else False:\n",
    "                # nothing sensible to infer; keep as-is\n",
    "                pred_col = None\n",
    "            else:\n",
    "                pred_col = maybe_pred_cols[0] if maybe_pred_cols else None\n",
    "\n",
    "        # Drop alias-family predictors (parents/descendants of T), always keep intercept row\n",
    "        dropped_alias = 0\n",
    "        if pred_col and alias_family:\n",
    "            is_intercept = coef_nz[pred_col].astype(str) == \"(intercept)\"\n",
    "            is_alias_pred = coef_nz[pred_col].astype(str).isin(alias_family)\n",
    "            dropped_alias = int(is_alias_pred.sum())\n",
    "            coef_nz = coef_nz[is_intercept | (~is_alias_pred)].copy()\n",
    "\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "        coef_nz.to_csv(coef_csv, index=False)\n",
    "\n",
    "        print(f\"[SAVED] Coefficients (non-zero only, alias-family predictors removed={dropped_alias}) → {coef_csv}  \"\n",
    "            f\"| kept={len(coef_nz):,} of {len(coef_df):,}  \"\n",
    "            f\"| nodes={len(res.get('order_used', [])) - 1}  \"\n",
    "            f\"| used_total_effect={res.get('used_total_effect', True)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
