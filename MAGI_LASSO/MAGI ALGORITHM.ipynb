{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import math\n",
    "from typing import Dict, List, Union, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = \"/projects/klybarge/pcori_ad/magi/magi_db/magi.db\"  # read-only\n",
    "OUT_DIR = \"./\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_top500\"   \n",
    "TARGETS = [\"aa_meas_citalopram_rem\",]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d591da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update 10 23 25\n",
    "# RIGHT= k (predictor) and LEFT = Y/j\n",
    "def analyze_causal_sequence_py(\n",
    "    df_in: Union[str, pd.DataFrame],\n",
    "    *,\n",
    "    name_map: Dict[str, str] = None,     # raw_code -> friendly label (applied to BOTH columns)\n",
    "    events: List[str] = None,            # event names to KEEP (AFTER recoding). If None: auto-detect\n",
    "    force_outcome: str = None,           # if provided and present, force this to be the FINAL node (Y)\n",
    "    lambda_min_count: int = 15           # L-threshold for λ: if n_code < L ⇒ λ_{k,j}=0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    MAGI (Python): Reference routine with explicit comments.\n",
    "\n",
    "    ─────────────────────────────────────────────────────────────────────────────\n",
    "    COLUMN CONVENTION PER ROW:\n",
    "      Left  column: target_concept_code  (call this X for this row)\n",
    "      Right column: concept_code         (call this k for this row)\n",
    "\n",
    "      n_code_target        ≡  k ∧ X\n",
    "      n_code_no_target     ≡  k ∧ ¬X\n",
    "      n_target_no_code     ≡  ¬k ∧ X\n",
    "      n_no_target          ≡  total(¬X)\n",
    "      n_target             ≡  total(X)\n",
    "      n_code               ≡  total(k)\n",
    "      n_code_before_target ≡  count(k before X)\n",
    "      n_target_before_code ≡  count(X before k)\n",
    "\n",
    "    ORIENTATION (locked to your spec):\n",
    "      • Total effect T_{kY}:   read row (target = Y, code = k).\n",
    "      • Lambda     λ_{k,j}:    read row (target = j, code = k), and compute\n",
    "                               λ_{k,j} = n_code_target / n_code  with L-threshold on n_code.\n",
    "\n",
    "    TEMPORAL SCORE for each node Zi:\n",
    "      Score(Z_i) = Σ_{j≠i} [ C(Z_i≺Z_j) - C(Z_j≺Z_i) + C(Z_i∧¬Z_j) - C(Z_j∧¬Z_i) ]\n",
    "      Read from row (target=Z_i, code=Z_j):\n",
    "        - n_code_before_target      → C(Z_j≺Z_i)\n",
    "        - n_target_before_code      → C(Z_i≺Z_j)\n",
    "        - n_code_no_target          → C(Z_j∧¬Z_i)\n",
    "        - n_target_no_code          → C(Z_i∧¬Z_j)\n",
    "\n",
    "    T_{kY} from row (Y, k):\n",
    "      a = n_code_target        (k ∧ Y)\n",
    "      b = n_code_no_target     (k ∧ ¬Y)\n",
    "      c = n_target_no_code     (¬k ∧ Y)\n",
    "      d = n_no_target - b      (¬k ∧ ¬Y)   ← computed on the fly (no extra column needed)\n",
    "      With sample-size–anchored odds:\n",
    "         odds_k1 = a/b with guards; odds_k0 = c/d with guards; T = odds_k1 / odds_k0\n",
    "\n",
    "    DIRECT EFFECTS via backward recursion:\n",
    "      D_{k,Y} = ( T_{k,Y} - Σ_j λ_{k,j} D_{j,Y} ) / ( 1 - Σ_j λ_{k,j} ),\n",
    "      where j are downstream nodes between k and Y in the temporal order.\n",
    "\n",
    "    LOGISTIC LINK:\n",
    "      logit P(Y=1 | Z) = β0 + Σ_k β_k Z_k  with β_k = log D_{k,Y};\n",
    "      invalid/nonpositive D map to β_k=0.\n",
    "\n",
    "    RETURNS a dict with:\n",
    "      sorted_scores, temporal_order, order_used,\n",
    "      T_val, D_val, lambda_l, coef_df, beta_0, beta, logit_predictors, predict_proba\n",
    "    \"\"\"\n",
    "    # ── 0) Ingest & validate ───────────────────────────────────────────────────\n",
    "    df = pd.read_csv(df_in) if isinstance(df_in, str) else df_in.copy()\n",
    "\n",
    "    need_cols = [\n",
    "        \"target_concept_code\", \"concept_code\",\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\",\n",
    "        \"n_target_no_code\",\n",
    "        \"n_code\",\n",
    "        \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # Always treat endpoints as strings to avoid silent filter drop\n",
    "    df[\"target_concept_code\"] = df[\"target_concept_code\"].astype(str)\n",
    "\n",
    "    # Optional recoding\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "        df[\"concept_code\"]        = df[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # Limit to selected events (union if auto)\n",
    "    if events is None:\n",
    "        ev_t = df[\"target_concept_code\"].unique().tolist()\n",
    "        ev_c = df[\"concept_code\"].unique().tolist()\n",
    "        events = sorted(set(ev_t) | set(ev_c))\n",
    "    else:\n",
    "        events = [str(e) for e in events]\n",
    "\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events.\")\n",
    "\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numerics\n",
    "    num_cols = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\", \"n_target_no_code\",\n",
    "        \"n_code\", \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # ── 1) Temporal score (read rows: target=Z_i, code=Z_j) ────────────────────\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zi in events:\n",
    "        s = 0.0\n",
    "        for zj in (x for x in events if x != zi):\n",
    "            # Row oriented as (Zi, Zj)\n",
    "            pair = df[(df[\"target_concept_code\"] == zi) & (df[\"concept_code\"] == zj)]\n",
    "            if pair.empty:\n",
    "                continue\n",
    "            c_i_before_j = float(pair[\"n_target_before_code\"].sum(skipna=True))   # Zi before Zj\n",
    "            c_j_before_i = float(pair[\"n_code_before_target\"].sum(skipna=True))   # Zj before Zi\n",
    "            c_i_and_not_j = float(pair[\"n_target_no_code\"].sum(skipna=True))      # Zi ∧ ¬Zj\n",
    "            c_j_and_not_i = float(pair[\"n_code_no_target\"].sum(skipna=True))      # Zj ∧ ¬Zi\n",
    "            s += (c_i_before_j - c_j_before_i + c_i_and_not_j - c_j_and_not_i)\n",
    "        scores[zi] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Choose outcome Y: either forced or top-scoring node\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "    else:\n",
    "        outcome = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "\n",
    "    events_order = temporal_order              # earliest … → Y\n",
    "    nodes = events_order[:-1]            # everything before Y\n",
    "\n",
    "    # ── 2) T and λ (row orientations locked) ───────────────────────────────────\n",
    "    T_val = pd.Series(0.0, index=nodes, dtype=float)\n",
    "    D_val = pd.Series(np.nan, index=nodes, dtype=float)\n",
    "    lambda_l: Dict[str, pd.Series] = {}\n",
    "\n",
    "    for k in nodes:\n",
    "        # ---- T_{kY} from the single row (target=Y, code=k) ----\n",
    "        row_Yk = df[(df[\"target_concept_code\"] == outcome) & (df[\"concept_code\"] == k)]\n",
    "\n",
    "        # 2×2 cells (a,b,c,d) at (Y,k):\n",
    "        a = float(row_Yk[\"n_code_target\"].sum(skipna=True))        # k ∧ Y\n",
    "        b = float(row_Yk[\"n_code_no_target\"].sum(skipna=True))     # k ∧ ¬Y\n",
    "        c = float(row_Yk[\"n_target_no_code\"].sum(skipna=True))     # ¬k ∧ Y\n",
    "        # d is not in data; compute from the same row: d = total(¬Y) − (k ∧ ¬Y)\n",
    "        n_noY = float(row_Yk[\"n_no_target\"].max(skipna=True)) if not row_Yk.empty else 0.0\n",
    "        d = max(n_noY - b, 0.0)                                    # ¬k ∧ ¬Y\n",
    "\n",
    "        # Stratum sizes\n",
    "        N1 = a + b                   # k = 1\n",
    "        N0 = c + d                   # k = 0\n",
    "\n",
    "        # Sample-size–anchored odds for k=1 and k=0\n",
    "        if N1 == 0:\n",
    "            odds_k1 = 1.0\n",
    "        else:\n",
    "            if a == 0:\n",
    "                odds_k1 = 1.0 / (N1 + 1.0)\n",
    "            elif b == 0:\n",
    "                odds_k1 = (N1 + 1.0)\n",
    "            else:\n",
    "                odds_k1 = a / b\n",
    "\n",
    "        if N0 == 0:\n",
    "            odds_k0 = 1.0\n",
    "        else:\n",
    "            if c == 0:\n",
    "                odds_k0 = 1.0 / (N0 + 1.0)\n",
    "            elif d == 0:\n",
    "                odds_k0 = (N0 + 1.0)\n",
    "            else:\n",
    "                odds_k0 = c / d\n",
    "\n",
    "        T_val.loc[k] = float(odds_k1 / odds_k0) if odds_k0 > 0 else (N1 + 1.0)\n",
    "\n",
    "        # ---- λ_{k,j} from rows (target=j, code=k): λ = n_code_target / n_code ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_jk = df[(df[\"target_concept_code\"] == j) & (df[\"concept_code\"] == k)]\n",
    "            if row_jk.empty:\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            num = float(pd.to_numeric(row_jk[\"n_code_target\"], errors=\"coerce\").fillna(0.0).sum())\n",
    "            den = float(pd.to_numeric(row_jk[\"n_code\"],        errors=\"coerce\").fillna(0.0).sum())\n",
    "\n",
    "            # L-threshold on the conditioning size n_code (count of k)\n",
    "            if (den <= 0) or (den < lambda_min_count):\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            lam = num / den\n",
    "            lam = 0.0 if not np.isfinite(lam) else float(min(max(lam, 0.0), 1.0))\n",
    "            lam_pairs.append((j, lam))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ── 3) Backward recursion for direct effects D ─────────────────────────────\n",
    "    # Last antecedent (just before Y): no downstream → D = T\n",
    "    if len(nodes) >= 1:\n",
    "        last_anc = nodes[-1]\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "\n",
    "    # Walk backward for the rest\n",
    "    if len(nodes) > 1:\n",
    "        for k in list(reversed(nodes[:-1])):\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            downstream = list(lam_vec.index)  # j nodes after k (already resolved)\n",
    "            lam_vals = lam_vec.reindex(downstream).fillna(0.0).to_numpy()\n",
    "            D_down  = pd.to_numeric(D_val.reindex(downstream), errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vals * D_down))\n",
    "            den = 1.0 - float(np.nansum(lam_vals))\n",
    "\n",
    "            if (not np.isfinite(den)) or den == 0.0:\n",
    "                D_val.loc[k] = T_val.loc[k]            # neutralize if pathological\n",
    "            else:\n",
    "                tmp = num / den\n",
    "                D_val.loc[k] = tmp if np.isfinite(tmp) else T_val.loc[k]\n",
    "\n",
    "    # ── 4) Logistic link (β) and predict_proba ─────────────────────────────────\n",
    "    # Intercept β0 from marginal prevalence of Y (rows with target == Y)\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    n_t = float(resp_rows[\"n_target\"].max()) if not resp_rows.empty else np.nan\n",
    "    n_n = float(resp_rows[\"n_no_target\"].max()) if not resp_rows.empty else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    p_y = min(max(p_y, 1e-12), 1 - 1e-12)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))\n",
    "\n",
    "    # β_k = log D_{k,Y}; invalid/nonpositive → 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    beta_vals = np.log(D_clean.where(D_clean > 0.0)).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_vals.index) + [\"(intercept)\"],\n",
    "        \"beta\":      list(beta_vals.values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # Vectorized predict_proba\n",
    "    predictors = list(beta_vals.index)\n",
    "    beta_vec = beta_vals.values\n",
    "\n",
    "    def predict_proba(Z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Compute P(Y=1|Z) using: logit P = β0 + Σ_k β_k Z_k.\n",
    "        Z can be:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame containing any/all of `predictors` (others ignored)\n",
    "        \"\"\"\n",
    "        def sigmoid(x):\n",
    "            x = np.clip(x, -700, 700)  # numerical stability for large |η|\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "        if isinstance(Z, pd.DataFrame):\n",
    "            M = Z.reindex(columns=predictors, fill_value=0.0).astype(float).to_numpy()\n",
    "            return sigmoid(beta_0 + M @ beta_vec)\n",
    "\n",
    "        if isinstance(Z, (dict, pd.Series)):\n",
    "            v = np.array([float(Z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            return float(sigmoid(beta_0 + float(v @ beta_vec)))\n",
    "\n",
    "        arr = np.asarray(Z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            return float(sigmoid(beta_0 + float(arr @ beta_vec)))\n",
    "        if arr.ndim == 2:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*,{len(predictors)}), got {arr.shape}\")\n",
    "            return sigmoid(beta_0 + arr @ beta_vec)\n",
    "\n",
    "        raise ValueError(\"Unsupported input for predict_proba\")\n",
    "\n",
    "    # ── 5) Package results ─────────────────────────────────────────────────────\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d9807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
