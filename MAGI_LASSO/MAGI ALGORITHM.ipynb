{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import math\n",
    "from typing import Dict, List, Union, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = \"/projects/klybarge/pcori_ad/magi/magi_db/magi.db\"  # read-only\n",
    "OUT_DIR = \"./\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_top500\"   \n",
    "TARGETS = [\"aa_meas_citalopram_rem\",]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d591da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### MAGI FUNCTION ###############\n",
    "\"\"\"\n",
    "MAGI: Dependent Bayes with Temporal Ordering — Reference Implementation\n",
    "-------------------------------------------------------------------------------\n",
    "This file adds **step-by-step comments** to the function `analyze_causal_sequence_py`. The comments mirror the proposed\n",
    "algorithm sections:\n",
    "\n",
    "1) Determination of Temporal Order\n",
    "2) Estimation of Dependent Bayes (T-values, λ-links, and D-values)\n",
    "3) Logistic link to produce P(Y=1 | Z)\n",
    "\n",
    "INPUT TABLE EXPECTATIONS (long format, one row per directed pair):\n",
    "- target_concept_code : str (# left node in the edge; in many places this\n",
    "denotes the earlier event)\n",
    "- concept_code : str (# right node in the edge; the later event)\n",
    "- n_code_target : float/int (# co-occurrence count for concept & target\n",
    "in the *target=Y* stratum; used as 'a')\n",
    "- n_code_no_target : float/int (# co-occurrence count for concept & target\n",
    "in the *target=¬Y* stratum; used as 'b')\n",
    "- n_target : float/int (# total Y count for this target concept, per edge row; we take max within a block)\n",
    "- n_no_target : float/int (# total ¬Y count; max within a block)\n",
    "- no_code_no_target : float/int (# optional, computed if missing as n_no_target - n_code_no_target; clipped ≥0)\n",
    "- n_target_before_code: float/int (# count where target occurred before code)\n",
    "- n_code_before_target: float/int (# count where code occurred before target)\n",
    "\n",
    "NOTES ON TEMPORAL COUNTS:\n",
    "For a row with target_concept_code = A and concept_code = B, the columns\n",
    "`n_code_before_target` and `n_target_before_code` are interpreted as:\n",
    "- n_code_before_target: # of persons where **B happened before A**\n",
    "- n_target_before_code: # of persons where **A happened before B**\n",
    "We aggregate these across j≠i to compute *temporal scores* for each event.\n",
    "\n",
    "PIECEWISE, SAMPLE-SIZE–ANCHORED ADJUSTMENTS:\n",
    "- For odds terms that would be 0 or ∞ due to zero cells, we replace the\n",
    "offending odds with 1/(N+1) or (N+1)/1, where N is the size of the\n",
    "appropriate stratum, so all ratios remain finite and interpretable.\n",
    "\n",
    "RETURN VALUE:\n",
    "A dict with temporal ordering, T-values, λ-vectors, D-values, coefficients\n",
    "for a logistic link, a `predict_proba` callable, and trace tables.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_causal_sequence_py(\n",
    "    data: Union[str, pd.DataFrame],\n",
    "    name_map: Dict[str, str],\n",
    "    events: List[str],\n",
    "    force_outcome=None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Compute temporal order, dependent-Bayes direct effects (D), and\n",
    "    a logistic-link probability for outcome Y from pairwise counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str | DataFrame\n",
    "        CSV path or in-memory DataFrame with the columns described above.\n",
    "    name_map : Dict[str, str]\n",
    "        Optional mapping raw code -> friendly label. If provided, both\n",
    "        `target_concept_code` and `concept_code` are replaced.\n",
    "    events : List[str]\n",
    "        List of event names/codes to restrict the analysis to. If `None`,\n",
    "        events are auto-detected from the table and intersected.\n",
    "    force_outcome : str | None\n",
    "        If provided and found among events, this event is forced to be the\n",
    "        **final** node (i.e., the outcome) in the temporal order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        - sorted_scores : pd.Series of temporal scores (desc)\n",
    "        - temporal_order: list of events (outcome at the end)\n",
    "        - order_used    : same as temporal_order\n",
    "        - T_val         : pd.Series of total effects T_{k,Y}\n",
    "        - D_val         : pd.Series of direct effects D_{k,Y}\n",
    "        - coef_df       : pd.DataFrame of coefficients (β_k and intercept)\n",
    "        - lambda_l      : dict[str -> pd.Series] of λ_{k,j} vectors\n",
    "        - trace_df      : pd.DataFrame detailing the backward recursion steps\n",
    "        - invalid_predictors: list of predictors whose log(D) was invalid\n",
    "        - beta_0, beta, logit_predictors, predict_proba: logistic elements\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 0) INGEST & BASIC VALIDATION\n",
    "    # ---------------------------------------------------------------------\n",
    "    if isinstance(data, str):\n",
    "        # Read from CSV path\n",
    "        df = pd.read_csv(data)\n",
    "    else:\n",
    "        # Work on a copy to avoid mutating caller's object\n",
    "        df = data.copy()\n",
    "\n",
    "    # Ensure required identifier columns are present\n",
    "    for col in [\"target_concept_code\", \"concept_code\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Optional recoding to human-friendly labels\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "        df[\"concept_code\"] = df[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # Numeric columns the algorithm expects\n",
    "    need = [\n",
    "        \"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\",\n",
    "        \"n_target_before_code\", \"n_code_before_target\",\n",
    "    ]\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # If precomputed total_effect exists (for λ or sanity checks), coerce to numeric\n",
    "    has_total = \"total_effect\" in df.columns\n",
    "    if has_total:\n",
    "        df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) EVENT SET (optional auto-detect) & TYPE COERCION\n",
    "    # ---------------------------------------------------------------------\n",
    "    if events is None:\n",
    "        # Auto-detect: intersect events appearing on both sides of edges\n",
    "        ev_targets = df[\"target_concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        ev_code = df[\"concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        events = sorted(set(ev_targets).intersection(ev_code))\n",
    "        if len(events) == 0:\n",
    "            # Fall back to union if intersection is empty\n",
    "            events = sorted(set(ev_targets) | set(ev_code))\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events after auto-detection.\")\n",
    "\n",
    "    # Keep only rows whose endpoints are both in the chosen event set\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numeric columns robustly (invalid -> NaN); subsequent ops handle NaNs\n",
    "    for c in need:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # If `no_code_no_target` missing, derive as (n_no_target - n_code_no_target) ≥ 0\n",
    "    if \"no_code_no_target\" not in df.columns:\n",
    "        df[\"no_code_no_target\"] = (df[\"n_no_target\"] - df[\"n_code_no_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        df[\"no_code_no_target\"] = pd.to_numeric(df[\"no_code_no_target\"], errors=\"coerce\").clip(lower=0)\n",
    "\n",
    "    # Helper: total count for an event (max n_target where that event is target)\n",
    "    # This mirrors your original choice; change to .sum() if warranted.\n",
    "    def C_of(ev: str) -> float:\n",
    "        sub = df[df[\"target_concept_code\"] == ev]\n",
    "        if sub.empty:\n",
    "            return float(\"nan\")\n",
    "        C = pd.to_numeric(sub[\"n_target\"], errors=\"coerce\").max()\n",
    "        return float(C) if pd.notna(C) and np.isfinite(C) else float(\"nan\")\n",
    "\n",
    "    # Helper: numerically safe log (log(1)=0 for invalid/≤0)\n",
    "    def safe_log(x: float) -> float:\n",
    "        try:\n",
    "            xv = float(x)\n",
    "        except (TypeError, ValueError):\n",
    "            return 0.0\n",
    "        if not np.isfinite(xv) or xv <= 0.0:\n",
    "            return 0.0\n",
    "        return math.log(xv)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) TEMPORAL ORDER — pairwise-before counts → per-node score\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Score(Z_i) = Σ_{j≠i} [ C(Z_i≪Z_j) - C(Z_j≪Z_i) + C(Z_i ∩ ¬Z_j) - C(Z_j ∩ ¬Z_i) ]\n",
    "    # Here we implement the *before/after* portion using the provided columns.\n",
    "    # If only presence/absence terms are available, you may approximate using\n",
    "    # the last two terms.\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zk in events:\n",
    "        s = 0.0\n",
    "        for zj in [x for x in events if x != zk]:\n",
    "            # For the pair (zj, zk), we interpret:\n",
    "            #   n_code_before_target   — # where zk (code) before zj (target)\n",
    "            #   n_target_before_code   — # where zj (target) before zk (code)\n",
    "            rowr = df[(df[\"target_concept_code\"] == zj) & (df[\"concept_code\"] == zk)]\n",
    "            if not rowr.empty:\n",
    "                s += float(rowr[\"n_code_before_target\"].sum(skipna=True) -\n",
    "                           rowr[\"n_target_before_code\"].sum(skipna=True))\n",
    "        scores[zk] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Outcome selection:\n",
    "    #  - If `force_outcome` is provided and present, put it at the end.\n",
    "    #  - Else, default to the top-scoring node as outcome.\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome_event = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "    else:\n",
    "        outcome_event = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "\n",
    "    # Propagation order is the temporal order; last is outcome Y\n",
    "    events_order = temporal_order\n",
    "    outcome = events_order[-1]\n",
    "    antecedents = events_order[:-1]\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) T-VALUES (TOTAL EFFECTS) & λ-LINKS BETWEEN ANTECEDENTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # For each antecedent k, we compute T_{k,Y} as an odds ratio between\n",
    "    # strata Z_k=1 and Z_k=0 with sample-size–anchored fixes for zero cells.\n",
    "    # For λ_{k,j} (dependence of j given k), we first use precomputed\n",
    "    # `total_effect` if present.\n",
    "    # If λ is missing (no recorded dependence), assume independence\n",
    "    # and treat the conditional contribution as 0 in the adjustment sum.\n",
    "\n",
    "    T_val = pd.Series(0.0, index=antecedents, dtype=float)  # T_{k,Y}\n",
    "    D_val = pd.Series(np.nan, index=antecedents, dtype=float)  # D_{k,Y}\n",
    "    lambda_l: Dict[str, pd.Series] = {}  # per-k vector of λ_{k,j} for j after k and before Y\n",
    "\n",
    "    for k in antecedents:\n",
    "        # ---- Contingency for (k, outcome) ----\n",
    "        row_ko = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == outcome)]\n",
    "\n",
    "        # a: Y∩k    b: ¬Y∩k    c: Y∩¬k    d: ¬Y∩¬k\n",
    "        a = float(row_ko[\"n_code_target\"].sum(skipna=True))            # co-occurrence in Y\n",
    "        b = float(row_ko[\"n_code_no_target\"].sum(skipna=True))         # co-occurrence in ¬Y\n",
    "\n",
    "        # If n_target_no_code absent, approximate c by (max n_target - a)\n",
    "        if \"n_target_no_code\" in row_ko.columns:\n",
    "            c = float(row_ko[\"n_target_no_code\"].sum(skipna=True))\n",
    "        else:\n",
    "            c = float(pd.to_numeric(row_ko[\"n_target\"], errors=\"coerce\").max() - a)\n",
    "\n",
    "        # If no_code_no_target absent, approximate d by (max n_no_target - b)\n",
    "        if \"no_code_no_target\" in row_ko.columns:\n",
    "            d = float(row_ko[\"no_code_no_target\"].sum(skipna=True))\n",
    "        else:\n",
    "            d = float(pd.to_numeric(row_ko[\"n_no_target\"], errors=\"coerce\").max() - b)\n",
    "\n",
    "        N1, N0 = a + b, c + d  # stratum sizes for Z_k=1 and Z_k=0\n",
    "\n",
    "        # ---- Sample-size–anchored odds for Z_k=1 ----\n",
    "        if a == 0:\n",
    "            odds_pos_adj = 1.0 / (N1 + 1.0)  # 0 → tiny positive odds\n",
    "        elif b == 0:\n",
    "            odds_pos_adj = (N1 + 1.0) / 1.0  # ∞ → capped by (N1+1)\n",
    "        else:\n",
    "            odds_pos_adj = a / b\n",
    "\n",
    "        # ---- Sample-size–anchored odds for Z_k=0 ----\n",
    "        if c == 0:\n",
    "            odds_neg_adj = 1.0 / (N0 + 1.0)\n",
    "        elif d == 0:\n",
    "            odds_neg_adj = (N0 + 1.0) / 1.0\n",
    "        else:\n",
    "            odds_neg_adj = c / d\n",
    "\n",
    "        # Total effect T_{k,Y}\n",
    "        T_val.loc[k] = float(odds_pos_adj / odds_neg_adj)\n",
    "\n",
    "        # ---- λ_{k,j}: dependence of j on k for nodes j after k (and before Y) ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_kj = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == j)]\n",
    "            if row_kj.empty:\n",
    "                lam_pairs.append((j, 0.0))  # no evidence of dependence\n",
    "                continue\n",
    "\n",
    "            # Prefer precomputed total_effect if present for this edge\n",
    "            te = float(pd.to_numeric(row_kj[\"total_effect\"], errors=\"coerce\").max()) if has_total else float(\"nan\")\n",
    "            if np.isfinite(te):\n",
    "                lam_pairs.append((j, te))\n",
    "                continue\n",
    "\n",
    "            # Otherwise approximate λ with piecewise, size-anchored logic\n",
    "            # C11 = C(j∩k); Cj_not_k = C(j∩¬k); Ck = C(k)\n",
    "            C11 = float(row_kj[\"n_code_target\"].sum(skipna=True))  # re-using the same column name for co-occurrence\n",
    "            if \"n_code_no_target\" in row_kj.columns:\n",
    "                Cj_not_k = float(row_kj[\"n_code_no_target\"].sum(skipna=True))\n",
    "            else:\n",
    "                Cj = C_of(j)\n",
    "                Cj_not_k = 0.0 if (not np.isfinite(Cj)) else max(Cj - C11, 0.0)\n",
    "            Ck = C_of(k)\n",
    "\n",
    "            if Cj_not_k == 0:\n",
    "                L = 1.0 + C11          # always-with-k → boost\n",
    "            elif C11 == 0:\n",
    "                L = 1.0 / (1.0 + Cj_not_k)  # never-with-k → downweight\n",
    "            elif np.isfinite(Ck) and Ck > 0:\n",
    "                L = C11 / Ck           # normalized co-occurrence\n",
    "            else:\n",
    "                L = 0.0\n",
    "\n",
    "            lam_pairs.append((j, float(L)))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4) BACKWARD RECURSION — resolve D_{k,Y} from T and λ\n",
    "    # ---------------------------------------------------------------------\n",
    "    # D_{k,Y} = ( T_{k,Y} - Σ_i λ_{k,k+i} * D_{k+i,Y} ) / ( 1 - Σ_i λ_{k,k+i} )\n",
    "    # Start at the last antecedent (just before Y): D := T, since there are no\n",
    "    # downstream nodes to adjust for.\n",
    "\n",
    "    trace_rows = []  # for human-auditable tracing of the recursion\n",
    "\n",
    "    last_anc = antecedents[-1] if antecedents else None\n",
    "    if last_anc is not None:\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "        trace_rows.append({\n",
    "            \"stage\": \"Last 2 Nodes\",\n",
    "            \"nodes\": f\"{last_anc} - {outcome}\",\n",
    "            \"k\": last_anc,\n",
    "            \"T_kY\": T_val.loc[last_anc],\n",
    "            \"lambda_terms\": None,\n",
    "            \"sum_lambda\": 0.0,\n",
    "            \"D_kY\": D_val.loc[last_anc],\n",
    "            \"log_D\": safe_log(D_val.loc[last_anc]),\n",
    "        })\n",
    "\n",
    "    if len(antecedents) > 1:\n",
    "        # Walk backwards through the remaining antecedents\n",
    "        for k in list(reversed(antecedents))[1:]:\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            # When computing the adjustment, use **only** D-values for nodes that\n",
    "            # are after k and already resolved (present in lam_vec index).\n",
    "            code = list(lam_vec.index)\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vec.reindex(code).values * D_val.reindex(code).values))\n",
    "            den = 1.0 - float(np.nansum(lam_vec.values))  # may approach 0 if λ's are large\n",
    "\n",
    "            # If den is pathological (≤0 or NaN), fall back to T (neutralization).\n",
    "            D_val.loc[k] = (num / den) if np.isfinite(num / den) else T_val.loc[k]\n",
    "\n",
    "            span = len(events_order) - events_order.index(k) + 1\n",
    "            lam_str = \", \".join(\n",
    "                f\"λ_{events_order.index(k)+1}{events_order.index(c)+1}={lam_vec[c]:.6f}\"\n",
    "                for c in code\n",
    "            ) if len(lam_vec) else None\n",
    "\n",
    "            trace_rows.append({\n",
    "                \"stage\": f\"Last {span} Nodes\",\n",
    "                \"nodes\": \" - \".join([k] + events_order[events_order.index(k)+1:]),\n",
    "                \"k\": k,\n",
    "                \"T_kY\": T_val.loc[k],\n",
    "                \"lambda_terms\": lam_str,\n",
    "                \"sum_lambda\": float(np.nansum(lam_vec.values)),\n",
    "                \"D_kY\": D_val.loc[k],\n",
    "                \"log_D\": safe_log(D_val.loc[k]),\n",
    "            })\n",
    "\n",
    "    trace_df = pd.DataFrame(trace_rows)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 5) COEFFICIENTS — map D's onto a logistic link\n",
    "    # ---------------------------------------------------------------------\n",
    "    # We model:  logit P(Y=1 | Z) = β0 + Σ_k β_k * Z_k,  with β_k = log D_{k,Y}\n",
    "    # Intercept β0 is set by the marginal prevalence of Y (from `n_target` &\n",
    "    # `n_no_target`) for the outcome rows.\n",
    "\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    if resp_rows.empty:\n",
    "        raise ValueError(f\"No rows for outcome '{outcome}'.\")\n",
    "\n",
    "    n_t = resp_rows[\"n_target\"].dropna().iloc[0] if resp_rows[\"n_target\"].dropna().size else np.nan\n",
    "    n_n = resp_rows[\"n_no_target\"].dropna().iloc[0] if resp_rows[\"n_no_target\"].dropna().size else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))  # stable enough for p∈(0,1)\n",
    "\n",
    "    # β_k = log(D_{k,Y}); protect against non-positive D by mapping to 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    D_pos = D_clean.where(D_clean > 0)\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        beta_vals = np.log(D_pos.to_numpy())\n",
    "    beta_k_raw = pd.Series(beta_vals, index=D_val.index)\n",
    "    invalid_predictors = list(beta_k_raw[~np.isfinite(beta_k_raw)].index)\n",
    "\n",
    "    beta_k = beta_k_raw.copy()\n",
    "    beta_k[~np.isfinite(beta_k)] = 0.0  # neutralize invalid predictors\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_k.index) + [\"(intercept)\"],\n",
    "        \"beta\": list(beta_k.astype(float).values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6) PREDICT_PROBA — vectorized logistic link\n",
    "    # ---------------------------------------------------------------------\n",
    "    predictors = list(beta_k.index)\n",
    "    beta_vec = beta_k.astype(float).values\n",
    "\n",
    "    def predict_proba(z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]) -> Union[float, np.ndarray, pd.Series]:\n",
    "        \"\"\"Compute P(Y=1 | Z) using the logistic link.\n",
    "\n",
    "        Accepts:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame with columns containing any/all of `predictors` (others ignored)\n",
    "\n",
    "        Returns:\n",
    "          - float for 1D inputs; np.ndarray or pd.Series for vectorized inputs\n",
    "        \"\"\"\n",
    "        if isinstance(z, pd.DataFrame):\n",
    "            Z = z.reindex(columns=predictors, fill_value=0).astype(float).to_numpy()\n",
    "            eta = beta_0 + Z @ beta_vec\n",
    "            # Stable sigmoid via clipping; avoids overflow for extreme η\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "        if isinstance(z, (dict, pd.Series)):\n",
    "            v = np.array([float(z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            eta = beta_0 + float(v @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "\n",
    "        arr = np.asarray(z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            eta = beta_0 + float(arr @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "        else:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*, {len(predictors)}), got {arr.shape}\")\n",
    "            eta = beta_0 + arr @ beta_vec\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7) PACKAGE RESULTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"trace_df\": trace_df,\n",
    "        \"invalid_predictors\": invalid_predictors,\n",
    "        # Logistic link outputs:\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d9807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
