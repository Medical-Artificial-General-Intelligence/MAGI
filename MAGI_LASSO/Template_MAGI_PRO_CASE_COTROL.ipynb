{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d083353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Dict, List, Union, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e5047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = \"/projects/klybarge/pcori_ad/magi/magi_db/magi.db\" \n",
    "OUT_DIR = \"./Test\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_top500\"  \n",
    "TARGETS = [ \"aa_meas_citalopram_rem\",]\n",
    "TOP_K = 500\n",
    "\n",
    "HI = 1.5\n",
    "LO = 1.0 / HI  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19edb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### MAGI FUNCTION ###############\n",
    "def analyze_causal_sequence_py(\n",
    "    data: Union[str, pd.DataFrame],\n",
    "    name_map: Dict[str, str],\n",
    "    events: List[str],\n",
    "    force_outcome=None,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    # -- 0) Ingest\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_csv(data)\n",
    "    else:\n",
    "        df = data.copy()\n",
    "\n",
    "    # -- 1) Recode event names\n",
    "    for col in [\"target_concept_code\", \"concept_code\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "        df[\"concept_code\"]        = df[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # -- 2) Filter & coerce numerics\n",
    "    need = [\n",
    "        \"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\",\n",
    "        \"n_target_before_code\", \"n_code_before_target\",\n",
    "    ]\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    has_total = \"total_effect\" in df.columns\n",
    "    if has_total:\n",
    "        df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "\n",
    "    # -- Auto-detect events if not provided\n",
    "    if events is None:\n",
    "        ev_targets  = df[\"target_concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        ev_children = df[\"concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        events = sorted(set(ev_targets).intersection(ev_children))\n",
    "        if len(events) == 0:\n",
    "            events = sorted(set(ev_targets) | set(ev_children))\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events after auto-detection.\")\n",
    "\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "    for c in need:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    if \"no_code_no_target\" not in df.columns:\n",
    "        df[\"no_code_no_target\"] = (df[\"n_no_target\"] - df[\"n_code_no_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        df[\"no_code_no_target\"] = pd.to_numeric(df[\"no_code_no_target\"], errors=\"coerce\").clip(lower=0)\n",
    "\n",
    "    # Helper: total count for a parent\n",
    "    def C_of(ev: str) -> float:\n",
    "        sub = df[df[\"target_concept_code\"] == ev]\n",
    "        if sub.empty:\n",
    "            return float(\"nan\")\n",
    "        C = pd.to_numeric(sub[\"n_target\"], errors=\"coerce\").max()\n",
    "        return float(C) if pd.notna(C) and np.isfinite(C) else float(\"nan\")\n",
    "\n",
    "    # ---- SAFE log for trace (treat invalid/≤0 as log(1)=0) ----\n",
    "    def safe_log(x: float) -> float:\n",
    "        try:\n",
    "            xv = float(x)\n",
    "        except (TypeError, ValueError):\n",
    "            return 0.0\n",
    "        if not np.isfinite(xv) or xv <= 0.0:\n",
    "            return 0.0\n",
    "        return math.log(xv)\n",
    "\n",
    "    # -- 3) Temporal order from before/after counts\n",
    "    scores = {}\n",
    "    for zk in events:\n",
    "        s = 0.0\n",
    "        for zj in [x for x in events if x != zk]:\n",
    "            rowr = df[(df[\"target_concept_code\"] == zj) & (df[\"concept_code\"] == zk)]\n",
    "            if not rowr.empty:\n",
    "                s += float(rowr[\"n_code_before_target\"].sum(skipna=True) -\n",
    "                           rowr[\"n_target_before_code\"].sum(skipna=True))\n",
    "        scores[zk] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "    # --- enforce outcome if requested ---\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome_event = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "    else:\n",
    "        outcome_event = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "\n",
    "    # -- 4) Propagation order\n",
    "    events_order = temporal_order\n",
    "    outcome = events_order[-1]\n",
    "    antecedents = events_order[:-1]\n",
    "\n",
    "    # -- 5) T-values and λ’s\n",
    "    T_val = pd.Series(0.0, index=antecedents, dtype=float)\n",
    "    D_val = pd.Series(np.nan, index=antecedents, dtype=float)\n",
    "    lambda_l: Dict[str, pd.Series] = {}\n",
    "\n",
    "    for k in antecedents:\n",
    "        row_ko = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == outcome)]\n",
    "        a = float(row_ko[\"n_code_target\"].sum(skipna=True))            # Y∩k\n",
    "        b = float(row_ko[\"n_code_no_target\"].sum(skipna=True))         # ¬Y∩k\n",
    "\n",
    "        if \"n_target_no_code\" in row_ko.columns:\n",
    "            c = float(row_ko[\"n_target_no_code\"].sum(skipna=True))     # Y∩¬k\n",
    "        else:\n",
    "            c = float(pd.to_numeric(row_ko[\"n_target\"], errors=\"coerce\").max() - a)\n",
    "\n",
    "        if \"no_code_no_target\" in row_ko.columns:\n",
    "            d = float(row_ko[\"no_code_no_target\"].sum(skipna=True))    # ¬Y∩¬k\n",
    "        else:\n",
    "            d = float(pd.to_numeric(row_ko[\"n_no_target\"], errors=\"coerce\").max() - b)\n",
    "\n",
    "        N1, N0 = a + b, c + d\n",
    "\n",
    "        # adjusted odds (add-1 when a/b/c/d is 0)\n",
    "        if a == 0:\n",
    "            odds_pos_adj = 1.0 / (N1 + 1.0)\n",
    "        elif b == 0:\n",
    "            odds_pos_adj = (N1 + 1.0) / 1.0\n",
    "        else:\n",
    "            odds_pos_adj = a / b\n",
    "\n",
    "        if c == 0:\n",
    "            odds_neg_adj = 1.0 / (N0 + 1.0)\n",
    "        elif d == 0:\n",
    "            odds_neg_adj = (N0 + 1.0) / 1.0\n",
    "        else:\n",
    "            odds_neg_adj = c / d\n",
    "\n",
    "        T_val.loc[k] = float(odds_pos_adj / odds_neg_adj)\n",
    "\n",
    "        # λ_{k,j} = L(X_j | X_k) with j after k and before outcome\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_vals = []\n",
    "        for j in js:\n",
    "            row_kj = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == j)]\n",
    "            if row_kj.empty:\n",
    "                lam_vals.append((j, 0.0)); continue\n",
    "\n",
    "            # Prefer precomputed total_effect for lambda if available\n",
    "            te = float(pd.to_numeric(row_kj[\"total_effect\"], errors=\"coerce\").max()) if has_total else float(\"nan\")\n",
    "            if np.isfinite(te):\n",
    "                lam_vals.append((j, te))\n",
    "                continue\n",
    "\n",
    "            # else compute via piecewise rule\n",
    "            C11 = float(row_kj[\"n_code_target\"].sum(skipna=True))      # C(j∩k)\n",
    "            if \"n_code_no_target\" in row_kj.columns:                   # C(j∩¬k)\n",
    "                Cj_not_k = float(row_kj[\"n_code_no_target\"].sum(skipna=True))\n",
    "            else:\n",
    "                Cj = C_of(j)\n",
    "                Cj_not_k = 0.0 if (not np.isfinite(Cj)) else max(Cj - C11, 0.0)\n",
    "            Ck = C_of(k)\n",
    "\n",
    "            if Cj_not_k == 0:\n",
    "                L = 1.0 + C11\n",
    "            elif C11 == 0:\n",
    "                L = 1.0 / (1.0 + Cj_not_k)\n",
    "            elif np.isfinite(Ck) and Ck > 0:\n",
    "                L = C11 / Ck\n",
    "            else:\n",
    "                L = 0.0\n",
    "\n",
    "            lam_vals.append((j, float(L)))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_vals}, dtype=float)\n",
    "\n",
    "    # -- 6) Excel-style recursion trace\n",
    "    trace_rows = []\n",
    "    last_anc = antecedents[-1] if antecedents else None\n",
    "    if last_anc is not None:\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "        trace_rows.append({\n",
    "            \"stage\": \"Last 2 Nodes\",\n",
    "            \"nodes\": f\"{last_anc} - {outcome}\",\n",
    "            \"k\": last_anc,\n",
    "            \"T_kY\": T_val.loc[last_anc],\n",
    "            \"lambda_terms\": None,\n",
    "            \"sum_lambda\": 0.0,\n",
    "            \"D_kY\": D_val.loc[last_anc],\n",
    "            \"log_D\": safe_log(D_val.loc[last_anc]),\n",
    "        })\n",
    "\n",
    "    if len(antecedents) > 1:\n",
    "        for k in list(reversed(antecedents))[1:]:\n",
    "            lam = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            children = list(lam.index)\n",
    "            num = T_val.loc[k] - float(np.nansum(lam.reindex(children).values * D_val.reindex(children).values))\n",
    "            den = 1.0 - float(np.nansum(lam.values))\n",
    "            D_val.loc[k] = (num / den) if np.isfinite(num / den) else T_val.loc[k]\n",
    "\n",
    "            span = len(events_order) - events_order.index(k) + 1\n",
    "            lam_str = \", \".join(\n",
    "                f\"λ_{events_order.index(k)+1}{events_order.index(ch)+1}={lam[ch]:.6f}\"\n",
    "                for ch in children\n",
    "            ) if len(lam) else None\n",
    "\n",
    "            trace_rows.append({\n",
    "                \"stage\": f\"Last {span} Nodes\",\n",
    "                \"nodes\": \" - \".join([k] + events_order[events_order.index(k)+1:]),\n",
    "                \"k\": k,\n",
    "                \"T_kY\": T_val.loc[k],\n",
    "                \"lambda_terms\": lam_str,\n",
    "                \"sum_lambda\": float(np.nansum(lam.values)),\n",
    "                \"D_kY\": D_val.loc[k],\n",
    "                \"log_D\": safe_log(D_val.loc[k]),\n",
    "            })\n",
    "\n",
    "    trace_df = pd.DataFrame(trace_rows)\n",
    "\n",
    "    # -- 7) Coefficients\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    if resp_rows.empty:\n",
    "        raise ValueError(f\"No rows for outcome '{outcome}'.\")\n",
    "\n",
    "    n_t = resp_rows[\"n_target\"].dropna().iloc[0] if resp_rows[\"n_target\"].dropna().size else np.nan\n",
    "    n_n = resp_rows[\"n_no_target\"].dropna().iloc[0] if resp_rows[\"n_no_target\"].dropna().size else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))\n",
    "\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "# only keep positive values for log; others become NaN\n",
    "    D_pos = D_clean.where(D_clean > 0)\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        beta_vals = np.log(D_pos.to_numpy())  # no warnings thrown\n",
    "    beta_k_raw = pd.Series(beta_vals, index=D_val.index)\n",
    "    invalid_predictors = list(beta_k_raw[~np.isfinite(beta_k_raw)].index)\n",
    "    beta_k = beta_k_raw.copy()\n",
    "    beta_k[~np.isfinite(beta_k)] = 0.0\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_k.index) + [\"(intercept)\"],\n",
    "        \"beta\": list(beta_k.astype(float).values) + [beta_0],\n",
    "    })\n",
    "\n",
    "\n",
    "    # -- 8) Logistic link: P(Y=1|Z) = 1 / (1 + exp(-(β0 + Σ β_i Z_i)))\n",
    "    predictors = list(beta_k.index)\n",
    "    beta_vec = beta_k.astype(float).values\n",
    "\n",
    "    def predict_proba(z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]) -> Union[float, np.ndarray, pd.Series]:\n",
    "        \"\"\"\n",
    "        Compute probability using the logistic link.\n",
    "\n",
    "        Accepts:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame with columns containing any/all of `predictors` (others ignored)\n",
    "\n",
    "        Returns:\n",
    "          - float for 1D inputs; np.ndarray or pd.Series for vectorized inputs\n",
    "        \"\"\"\n",
    "        if isinstance(z, pd.DataFrame):\n",
    "            Z = z.reindex(columns=predictors, fill_value=0).astype(float).to_numpy()\n",
    "            eta = beta_0 + Z @ beta_vec\n",
    "            # stable sigmoid\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "        if isinstance(z, (dict, pd.Series)):\n",
    "            v = np.array([float(z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            eta = beta_0 + float(v @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "\n",
    "        arr = np.asarray(z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            eta = beta_0 + float(arr @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "        else:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*, {len(predictors)}), got {arr.shape}\")\n",
    "            eta = beta_0 + arr @ beta_vec\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"trace_df\": trace_df,\n",
    "        \"invalid_predictors\": invalid_predictors,\n",
    "        # Logistic link outputs:\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "\n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fill no_code_no_target & total_effect in RAM if missing.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"no_code_no_target\" not in df.columns:\n",
    "        df[\"n_no_target\"] = pd.to_numeric(df[\"n_no_target\"], errors=\"coerce\")\n",
    "        df[\"n_code_no_target\"] = pd.to_numeric(df[\"n_code_no_target\"], errors=\"coerce\")\n",
    "        df[\"no_code_no_target\"] = (df[\"n_no_target\"] - df[\"n_code_no_target\"]).clip(lower=0)\n",
    "\n",
    "    if \"total_effect\" not in df.columns:\n",
    "        for c in [\"n_code_target\",\"n_code_no_target\",\"n_target\",\"n_no_target\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
    "        a = df[\"n_code_target\"].astype(float)\n",
    "        b = df[\"n_code_no_target\"].astype(float)\n",
    "        c_ = (df[\"n_target\"].astype(float) - a).clip(lower=0)\n",
    "        d = (df[\"n_no_target\"].astype(float) - b).clip(lower=0)\n",
    "        N1, N0 = (a + b), (c_ + d)\n",
    "        odds_pos = np.where((a > 0) & (b > 0), a / b,\n",
    "                            np.where(b == 0, N1 + 1.0, 1.0 / (N1 + 1.0)))\n",
    "        odds_neg = np.where((c_ > 0) & (d > 0), c_ / d,\n",
    "                            np.where(d == 0, N0 + 1.0, 1.0 / (N0 + 1.0)))\n",
    "        df[\"total_effect\"] = odds_pos / odds_neg\n",
    "    return df\n",
    "\n",
    "def _select_k_balanced_no_fallback(k_to_T: pd.DataFrame, top_k: int = 500,\n",
    "                                   hi: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced selection without fallback:\n",
    "      - Take up to top_k//2 strongest risk (TE >= hi)\n",
    "      - Take up to top_k - top_k//2 strongest protective (TE <= 1/hi)\n",
    "      - If either side is short, do NOT fill from elsewhere; total may be < top_k\n",
    "      - Return one row per k (target_concept_code), ranked by extremeness\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    # Ensure total_effect numeric\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with total_effect; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # Extremeness symmetrical around 1\n",
    "    df[\"effect_strength\"] = np.where(df[\"total_effect\"] >= 1.0,\n",
    "                                     df[\"total_effect\"],\n",
    "                                     1.0 / df[\"total_effect\"])\n",
    "\n",
    "    # One row per k (keep most extreme)\n",
    "    best_per_k = (df.sort_values(\"effect_strength\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"target_concept_code\"], keep=\"first\"))\n",
    "\n",
    "    HI = hi\n",
    "    LO = 1.0 / hi\n",
    "\n",
    "    risk_pool = best_per_k[best_per_k[\"total_effect\"] >= HI].copy()\n",
    "    prot_pool = best_per_k[best_per_k[\"total_effect\"] <= LO].copy()\n",
    "\n",
    "    # Target split\n",
    "    want_risk = top_k // 2\n",
    "    want_prot = top_k - want_risk\n",
    "\n",
    "    # Take strongest from each side, capped by availability\n",
    "    take_risk = min(len(risk_pool), want_risk)\n",
    "    take_prot = min(len(prot_pool), want_prot)\n",
    "\n",
    "    sel_risk = risk_pool.nlargest(take_risk, \"effect_strength\")\n",
    "    sel_prot = prot_pool.nlargest(take_prot, \"effect_strength\")\n",
    "\n",
    "    selected = pd.concat([sel_risk, sel_prot], ignore_index=True)\n",
    "\n",
    "    print(f\"[SELECT] total unique k={len(best_per_k):,}  \"\n",
    "          f\"risk={len(risk_pool):,}  prot={len(prot_pool):,}  \"\n",
    "          f\"selected(total)={selected['target_concept_code'].nunique()}  \"\n",
    "          f\"with: risk={take_risk}, prot={take_prot}\")\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "\n",
    "def _fetch_k_to_T(conn, target_code: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch only k→T rows (concept_code == target).\"\"\"\n",
    "    q = \"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE ccn.concept_code = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[target_code])\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list):\n",
    "    \"\"\"Fetch edges with target_concept_code IN events_set (single IN to avoid 999 param issues).\"\"\"\n",
    "    ph = \",\".join([\"?\"] * len(events_list))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=list(events_list))\n",
    "# ===== Main loop over targets =====\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "\n",
    "    for T in TARGETS:\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "        # 1) k→T only\n",
    "        k_to_T = _fetch_k_to_T(conn, T)\n",
    "        if k_to_T.empty:\n",
    "            print(f\"[WARN] No k→T rows for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) select k (balanced + fallback)\n",
    "        sel_rows = _select_k_balanced_no_fallback(k_to_T, top_k=TOP_K)\n",
    "        selected_k = set(sel_rows[\"target_concept_code\"].astype(str))\n",
    "        if len(selected_k) == 0:\n",
    "            print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 4) build subgraph events and fetch edges\n",
    "        events_set = selected_k | {T}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "        # Keep only edges fully inside the set (both ends)\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].isin(events_set) &\n",
    "            df_trim[\"concept_code\"].isin(events_set)\n",
    "        ].copy()\n",
    "\n",
    "        print(f\"[TRIM] rows={len(df_trim):,}  events={len(events_set)}  \"\n",
    "              f\"k→T rows={int((df_trim['concept_code']==T).sum())}  \"\n",
    "              f\"T→j rows={int((df_trim['target_concept_code']==T).sum())}\")\n",
    "\n",
    "        # 5) save subgraph for audit\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "        # 6) run MAGI (force outcome if your function supports it; else warn if differs)\n",
    "        try:\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=None, force_outcome=T)  # if patched\n",
    "        except TypeError:\n",
    "            res = analyze_causal_sequence_py(df_trim)  # original signature\n",
    "            outcome_used = res[\"order_used\"][-1]\n",
    "            if outcome_used != T:\n",
    "                print(f\"[NOTE] outcome auto-inferred as {outcome_used}, not {T}\")\n",
    "\n",
    "        # 7) save coefficients\n",
    "        outcome_used = res[\"order_used\"][-1]\n",
    "        coef_df = res[\"coef_df\"]\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}.csv\")\n",
    "        coef_df.to_csv(coef_csv, index=False)\n",
    "        print(f\"[SAVED] Coefficients → {coef_csv}  | antecedents={len(res['order_used'])-1}  \"\n",
    "              f\"used_total_effect={res.get('used_total_effect', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9aff190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(OUT_DIR, f\"magi_subgraph_aa_meas_citalopram_rem.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0015e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (4687, 19)\n",
      "df columns: ['target_concept_code_int', 'concept_code_int', 'n_code_target', 'n_code_no_target', 'n_target', 'n_no_target', 'n_code', 'n_target_before_code', 'n_code_before_target', 'n_target_no_code', 'n_no_code_no_target', 'lr', 'norm_lr', 'lr_rank', 'total_effects', 'total_effects_norm', 'total_effects_rank', 'target_concept_code', 'concept_code']\n"
     ]
    }
   ],
   "source": [
    "print(\"df shape:\", df.shape)\n",
    "print(\"df columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "134f08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) prune events to what's present in df ---\n",
    "def prune_events_to_data(df: pd.DataFrame, name_map: dict, events: list, outcome: str = None):\n",
    "    \"\"\"\n",
    "    Recode df with name_map (no-op if empty), then keep only events present in either column.\n",
    "    If outcome is provided, verify it exists and has at least one row with any remaining event.\n",
    "    Returns (present_events, missing_events).\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    if \"target_concept_code\" not in df2.columns or \"concept_code\" not in df2.columns:\n",
    "        raise KeyError(\"df must contain 'target_concept_code' and 'concept_code'\")\n",
    "\n",
    "    if name_map:\n",
    "        df2[\"target_concept_code\"] = df2[\"target_concept_code\"].replace(name_map)\n",
    "        df2[\"concept_code\"]        = df2[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    syms = set(df2[\"target_concept_code\"]).union(df2[\"concept_code\"])\n",
    "    present = [e for e in events if e in syms]\n",
    "    missing = sorted(set(events) - syms)\n",
    "\n",
    "    if outcome is not None:\n",
    "        if outcome not in syms:\n",
    "            raise ValueError(f\"Outcome '{outcome}' not in data after recode. Missing: {missing}\")\n",
    "        has_rows = ((df2[\"target_concept_code\"] == outcome) & (df2[\"concept_code\"].isin(present))).any()\n",
    "        if not has_rows:\n",
    "            raise ValueError(f\"No rows for outcome '{outcome}' with remaining events {present}.\")\n",
    "\n",
    "    if len(present) < 2:\n",
    "        raise ValueError(f\"Need at least 2 events present to run the analysis. Present={present}, Missing={missing}\")\n",
    "\n",
    "    return present, missing\n",
    "\n",
    "\n",
    "# --- 2) name_map (raw -> friendly); keep empty to use raw codes ---\n",
    "name_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "909df200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) events to analyze (raw codes since name_map is empty) ---\n",
    "events = [\n",
    "    \"aa_meas_citalopram_rem\",  # intended outcome\n",
    "    \"rx_RxNorm_1236136\",\n",
    "    \"px_CPT4_33226\",\n",
    "    \"dx_SNOMED_33339001\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0341e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping missing events: ['dx_SNOMED_33339001']\n",
      "events (present): ['aa_meas_citalopram_rem', 'rx_RxNorm_1236136', 'px_CPT4_33226']\n"
     ]
    }
   ],
   "source": [
    "# --- 4) prune & run MAGI analysis with intended outcome ---\n",
    "desired_outcome = \"aa_meas_citalopram_rem\"\n",
    "present, missing = prune_events_to_data(df, name_map, events, outcome=desired_outcome)\n",
    "if missing:\n",
    "    print(\"Dropping missing events:\", missing)\n",
    "print(\"events (present):\", present)\n",
    "\n",
    "res = analyze_causal_sequence_py(\n",
    "    df, name_map=name_map, events=present, force_outcome=desired_outcome\n",
    ")\n",
    "assert res[\"order_used\"][-1] == desired_outcome, (\n",
    "    f\"Outcome mismatch: expected {desired_outcome}, got {res['order_used'][-1]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c014107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome (model): aa_meas_citalopram_rem\n",
      "Temporal order: px_CPT4_33226 -> rx_RxNorm_1236136 -> aa_meas_citalopram_rem\n",
      "Predictors used by the logit: ['px_CPT4_33226', 'rx_RxNorm_1236136']\n",
      "\n",
      "Coefficients (predictors + intercept):\n",
      "           predictor      beta\n",
      "0      px_CPT4_33226  1.540802\n",
      "1  rx_RxNorm_1236136 -1.262856\n",
      "2        (intercept) -3.843496\n",
      "Baseline probability: 0.020969454388289125\n",
      "Predicted probability (desired on, others 0): 0.02750359341046971\n"
     ]
    }
   ],
   "source": [
    "# --- 5) predict probabilities  ---\n",
    "\n",
    "# Outcome chosen by the model\n",
    "model_outcome = res[\"order_used\"][-1]\n",
    "print(\"Outcome (model):\", model_outcome)\n",
    "print(\"Temporal order:\", \" -> \".join(res[\"order_used\"]))\n",
    "\n",
    "# Predictors actually used by the logit (exclude the outcome just in case)\n",
    "preds = [p for p in res[\"logit_predictors\"] if p != model_outcome]\n",
    "print(\"Predictors used by the logit:\", preds)\n",
    "\n",
    "# Compare with your pruned events list (present minus outcome)\n",
    "desired_source = [e for e in present if e != model_outcome]\n",
    "ignored = sorted(set(desired_source) - set(preds))\n",
    "if ignored:\n",
    "    print(\"Ignored (present but not in model):\", ignored)\n",
    "\n",
    "# show coefficients for the predictors + intercept\n",
    "coef = res[\"coef_df\"]\n",
    "print(\"\\nCoefficients (predictors + intercept):\")\n",
    "print(coef[coef[\"predictor\"].isin(preds + [\"(intercept)\"])].reset_index(drop=True))\n",
    "\n",
    "# Baseline (all 0)\n",
    "z0 = {p: 0 for p in preds}\n",
    "p0 = res[\"predict_proba\"](z0)\n",
    "print(\"Baseline probability:\", p0)\n",
    "\n",
    "desired_in_model = [e for e in desired_source if e in preds]\n",
    "z = {p: (1 if p in desired_in_model else 0) for p in preds}\n",
    "p = res[\"predict_proba\"](z)\n",
    "print(\"Predicted probability (desired on, others 0):\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e560ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------- 6. helper: infer binary labels (Female/Male) ----------\n",
    "def infer_binary_labels(code: str):\n",
    "    \"\"\"Return ('label for =1', 'label for =0') based on the code name.\"\"\"\n",
    "    c = (code or \"\").lower()\n",
    "    # Common sex/gender patterns\n",
    "    if c.endswith(\"_f\") or \"female\" in c or \"gender_f\" in c or \"sex_f\" in c:\n",
    "        return (\"Female\", \"Male\")\n",
    "    if c.endswith(\"_m\") or \"male\" in c or \"gender_m\" in c or \"sex_m\" in c:\n",
    "        return (\"Male\", \"Female\")\n",
    "    # Default\n",
    "    return (f\"{code}=1\", f\"{code}=0\")\n",
    "\n",
    "\n",
    "# ---------- A) predictor vs outcome 2×2 (with Female/Male row labels if k is sex) ----------\n",
    "def generate_contingency_tables(df: pd.DataFrame, name_map: dict, res: dict):\n",
    "    \"\"\"\n",
    "    For each predictor k in res['logit_predictors'] vs outcome Y, produce a 2×2:\n",
    "      rows:  Z_k=1, Z_k=0  (labeled Female/Male if k is a sex flag)\n",
    "      cols:  Y=1,  Y=0\n",
    "    a = Y∩k  (n_code_target)\n",
    "    b = Y∩¬k (n_code_no_target)\n",
    "    c = ¬Y∩k (derived: n_target - a)\n",
    "    d = ¬Y∩¬k (derived: n_no_target - b)\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2[\"target_concept_code\"] = df2[\"target_concept_code\"].replace(name_map)\n",
    "    df2[\"concept_code\"]        = df2[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # ensure numeric\n",
    "    need = [\"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\"]\n",
    "    for col in need:\n",
    "        if col not in df2.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "        df2[col] = pd.to_numeric(df2[col], errors=\"coerce\")\n",
    "\n",
    "    Y = res[\"order_used\"][-1]\n",
    "    preds = list(res[\"logit_predictors\"])\n",
    "    out = {}\n",
    "\n",
    "    for k in preds:\n",
    "        row = df2[(df2[\"target_concept_code\"] == k) & (df2[\"concept_code\"] == Y)]\n",
    "\n",
    "        a = float(row[\"n_code_target\"].sum(skipna=True)) if not row.empty else 0.0\n",
    "        b = float(row[\"n_code_no_target\"].sum(skipna=True)) if not row.empty else 0.0\n",
    "\n",
    "        nt  = float(pd.to_numeric(row[\"n_target\"], errors=\"coerce\").max()) if not row.empty else 0.0\n",
    "        nnt = float(pd.to_numeric(row[\"n_no_target\"], errors=\"coerce\").max()) if not row.empty else 0.0\n",
    "        c = max(nt  - a, 0.0)\n",
    "        d = max(nnt - b, 0.0)\n",
    "\n",
    "        # Margins\n",
    "        z1, z0 = a + c, b + d\n",
    "        y1, y0 = a + b, c + d\n",
    "        N      = y1 + y0\n",
    "\n",
    "        # Label rows: Female/Male if k is a sex flag\n",
    "        row1, row0 = infer_binary_labels(k)\n",
    "\n",
    "        tbl = pd.DataFrame(\n",
    "            [[a, c, z1],\n",
    "             [b, d, z0],\n",
    "             [y1, y0, N]],\n",
    "            index=[row1, row0, \"Sum\"],\n",
    "            columns=[f\"{Y}=1\", f\"{Y}=0\", \"Sum\"]\n",
    "        )\n",
    "        out[k] = tbl\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------- B) pairwise variable–variable 2×2 (labels Female/Male on any sex axis) ----------\n",
    "def generate_pairwise_contingency_tables(df: pd.DataFrame, name_map: dict,\n",
    "                                         items=None, use_predictors=True, res=None):\n",
    "    \"\"\"\n",
    "    For each unordered pair (k, j), build a 2×2:\n",
    "      rows = k=1/0 (Female/Male if k is a sex flag)\n",
    "      cols = j=1/0 (Female/Male if j is a sex flag)\n",
    "\n",
    "    a = k∩j            (max from either orientation’s n_code_target)\n",
    "    b = k∩¬j           (prefer row target=j, concept=k: n_code_no_target; else C(k)-a)\n",
    "    c = j∩¬k           (prefer row target=k, concept=j: n_code_no_target; else C(j)-a)\n",
    "    d = N - (a+b+c)    (N from totals if available; else observed sum)\n",
    "    \"\"\"\n",
    "    from itertools import combinations\n",
    "    if use_predictors:\n",
    "        if res is None:\n",
    "            raise ValueError(\"res is required when use_predictors=True.\")\n",
    "        items = list(res[\"logit_predictors\"]) if items is None else items\n",
    "    elif items is None:\n",
    "        raise ValueError(\"Provide `items` when use_predictors=False.\")\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2[\"target_concept_code\"] = df2[\"target_concept_code\"].replace(name_map)\n",
    "    df2[\"concept_code\"]        = df2[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    for col in [\"n_code_target\",\"n_code_no_target\",\"n_target\",\"n_no_target\"]:\n",
    "        if col not in df2.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "        df2[col] = pd.to_numeric(df2[col], errors=\"coerce\")\n",
    "\n",
    "    # Totals helper\n",
    "    def C_of(ev: str) -> float:\n",
    "        sub = df2[df2[\"target_concept_code\"] == ev]\n",
    "        if sub.empty: return float(\"nan\")\n",
    "        C = pd.to_numeric(sub[\"n_target\"], errors=\"coerce\").max()\n",
    "        return float(C) if pd.notna(C) and np.isfinite(C) else float(\"nan\")\n",
    "\n",
    "    # N helper\n",
    "    def N_for_pair(k: str, j: str) -> float:\n",
    "        cands = []\n",
    "        for (t, cpt) in [(k, j), (j, k)]:\n",
    "            row = df2[(df2[\"target_concept_code\"] == t) & (df2[\"concept_code\"] == cpt)]\n",
    "            if not row.empty:\n",
    "                nt = pd.to_numeric(row[\"n_target\"], errors=\"coerce\")\n",
    "                nn = pd.to_numeric(row[\"n_no_target\"], errors=\"coerce\")\n",
    "                val = (nt + nn).max()\n",
    "                if pd.notna(val) and np.isfinite(val): cands.append(float(val))\n",
    "        if cands: return max(cands)\n",
    "        for t in (k, j):\n",
    "            sub = df2[df2[\"target_concept_code\"] == t]\n",
    "            if not sub.empty:\n",
    "                nt = pd.to_numeric(sub[\"n_target\"], errors=\"coerce\").max()\n",
    "                nn = pd.to_numeric(sub[\"n_no_target\"], errors=\"coerce\").max()\n",
    "                if pd.notna(nt) and pd.notna(nn): return float(nt + nn)\n",
    "        return float(\"nan\")\n",
    "\n",
    "    syms = set(df2[\"target_concept_code\"]).union(df2[\"concept_code\"])\n",
    "    items = [x for x in items if x in syms]\n",
    "\n",
    "    out = {}\n",
    "    for k, j in combinations(sorted(set(items)), 2):\n",
    "        row_kj = df2[(df2[\"target_concept_code\"] == k) & (df2[\"concept_code\"] == j)]\n",
    "        row_jk = df2[(df2[\"target_concept_code\"] == j) & (df2[\"concept_code\"] == k)]\n",
    "\n",
    "        a1 = float(pd.to_numeric(row_kj[\"n_code_target\"], errors=\"coerce\").sum(skipna=True)) if not row_kj.empty else np.nan\n",
    "        a2 = float(pd.to_numeric(row_jk[\"n_code_target\"], errors=\"coerce\").sum(skipna=True)) if not row_jk.empty else np.nan\n",
    "        a = np.nanmax([a1, a2]) if np.any(np.isfinite([a1, a2])) else 0.0\n",
    "\n",
    "        if not row_jk.empty:\n",
    "            b = float(pd.to_numeric(row_jk[\"n_code_no_target\"], errors=\"coerce\").sum(skipna=True))\n",
    "        else:\n",
    "            Ck = C_of(k); b = 0.0 if not np.isfinite(Ck) else max(Ck - a, 0.0)\n",
    "\n",
    "        if not row_kj.empty:\n",
    "            c = float(pd.to_numeric(row_kj[\"n_code_no_target\"], errors=\"coerce\").sum(skipna=True))\n",
    "        else:\n",
    "            Cj = C_of(j); c = 0.0 if not np.isfinite(Cj) else max(Cj - a, 0.0)\n",
    "\n",
    "        N = N_for_pair(k, j)\n",
    "        if not np.isfinite(N):\n",
    "            d = 0.0; N = a + b + c\n",
    "        else:\n",
    "            d = max(N - (a + b + c), 0.0)\n",
    "\n",
    "        # Human-friendly labels\n",
    "        row1, row0 = infer_binary_labels(k)\n",
    "        col1, col0 = infer_binary_labels(j)\n",
    "\n",
    "        # Build table with sex-aware labels\n",
    "        k1, k0 = a + b, c + d\n",
    "        j1, j0 = a + c, b + d\n",
    "        tbl = pd.DataFrame(\n",
    "            [[a, b, k1],\n",
    "             [c, d, k0],\n",
    "             [j1, j0, N]],\n",
    "            index=[row1, row0, \"Sum\"],\n",
    "            columns=[col1, col0, \"Sum\"]\n",
    "        )\n",
    "        out[(k, j)] = tbl\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01f451e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== px_CPT4_33226 vs aa_meas_citalopram_rem ===\n",
      "                 aa_meas_citalopram_rem=1  aa_meas_citalopram_rem=0       Sum\n",
      "px_CPT4_33226=1                       0.0                      10.0      10.0\n",
      "px_CPT4_33226=0                   11159.0                  520986.0  532145.0\n",
      "Sum                               11159.0                  520996.0  532155.0\n",
      "\n",
      "=== rx_RxNorm_1236136 vs aa_meas_citalopram_rem ===\n",
      "                     aa_meas_citalopram_rem=1  aa_meas_citalopram_rem=0  \\\n",
      "rx_RxNorm_1236136=1                       0.0                     165.0   \n",
      "rx_RxNorm_1236136=0                   11159.0                  520831.0   \n",
      "Sum                                   11159.0                  520996.0   \n",
      "\n",
      "                          Sum  \n",
      "rx_RxNorm_1236136=1     165.0  \n",
      "rx_RxNorm_1236136=0  531990.0  \n",
      "Sum                  532155.0  \n",
      "\n",
      "=== Pairwise: px_CPT4_33226 vs rx_RxNorm_1236136 ===\n",
      "                 rx_RxNorm_1236136=1  rx_RxNorm_1236136=0       Sum\n",
      "px_CPT4_33226=1                  0.0                 10.0      10.0\n",
      "px_CPT4_33226=0                165.0             531980.0  532145.0\n",
      "Sum                            165.0             531990.0  532155.0\n"
     ]
    }
   ],
   "source": [
    "# --- 7) print the contingency tables ---\n",
    "pred_vs_outcome = generate_contingency_tables(df, name_map, res)\n",
    "for k, t in pred_vs_outcome.items():\n",
    "    print(f\"\\n=== {k} vs {res['order_used'][-1]} ===\")\n",
    "    print(t)\n",
    "\n",
    "pairwise = generate_pairwise_contingency_tables(df, name_map, use_predictors=True, res=res)\n",
    "for (k, j), t in pairwise.items():\n",
    "    print(f\"\\n=== Pairwise: {k} vs {j} ===\")\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### All of Us Case-Control Organization ################ \n",
    "# ---------- UTILS ----------\n",
    "def banner(txt):\n",
    "    bar = \"=\" * max(12, len(txt) + 4)\n",
    "    print(f\"\\n{bar}\\n{txt}\\n{bar}\")\n",
    "\n",
    "def subhead(txt):\n",
    "    print(f\"\\n--- {txt} ---\")\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in (\"-\", \"_\") else \"_\" for ch in s)\n",
    "\n",
    "def qtiles(x):\n",
    "    x = np.asarray(x)\n",
    "    return np.quantile(x, [0, 0.01, 0.25, 0.5, 0.75, 0.99, 1.0])\n",
    "\n",
    "def preview_active_codes(X_csr, feature_codes, row_indices, k=8):\n",
    "    \"\"\"Print up to k active codes for a few rows.\"\"\"\n",
    "    for i in row_indices:\n",
    "        start, end = X_csr.indptr[i], X_csr.indptr[i+1]\n",
    "        cols = X_csr.indices[start:end]\n",
    "        codes_list = [feature_codes[j] for j in cols[:k]]\n",
    "        print(f\"   row {i}: n_active={len(cols)}  sample_active={codes_list}\")\n",
    "\n",
    "def load_magi_betas(coef_csv):\n",
    "    df = pd.read_csv(coef_csv)\n",
    "    def pick(df, opts):\n",
    "        for c in opts:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        raise KeyError(f\"Missing any of {opts} in {coef_csv}. Found: {list(df.columns)}\")\n",
    "    code_col = pick(df, [\"concept_code\",\"standard_concept_code\",\"predictor\",\"feature\",\"term\",\"name\"])\n",
    "    beta_col = pick(df, [\"coef\",\"coefficient\",\"beta\",\"estimate\",\"b\",\"value\"])\n",
    "    df[code_col] = df[code_col].astype(str).str.strip()\n",
    "    is_int = df[code_col].str.lower().isin([\"(intercept)\",\"intercept\",\"const\",\"(const)\",\"bias\"])\n",
    "    intercept = float(df.loc[is_int, beta_col].iloc[0]) if is_int.any() else 0.0\n",
    "    coef_map  = dict(zip(df.loc[~is_int, code_col], df.loc[~is_int, beta_col].astype(float)))\n",
    "    return intercept, coef_map\n",
    "\n",
    "def sample_all_pos_kx_neg(y, k=4, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos = np.where(y == 1)[0]\n",
    "    neg = np.where(y == 0)[0]\n",
    "    if pos.size == 0: raise ValueError(\"No positives for this target.\")\n",
    "    want = min(k * pos.size, neg.size)\n",
    "    sel_neg = rng.choice(neg, size=want, replace=False)\n",
    "    sel = np.concatenate([pos, sel_neg]); rng.shuffle(sel)\n",
    "    return sel\n",
    "\n",
    "def score_from_betas(X_sub, feature_codes, betas_map, intercept):\n",
    "    feat = np.array(feature_codes, dtype=str)\n",
    "    mask = np.isin(feat, list(betas_map.keys()))\n",
    "    idx  = np.where(mask)[0]\n",
    "    if idx.size == 0: raise ValueError(\"No overlap between features and MAGI coefficients.\")\n",
    "    betas = np.array([betas_map[c] for c in feat[idx]], dtype=float)\n",
    "    lp = intercept + X_sub[:, idx].dot(betas)      # (n,)\n",
    "    p  = expit(np.asarray(lp).ravel())\n",
    "    return np.asarray(lp).ravel(), p, idx, betas\n",
    "\n",
    "def plot_roc(y_true, p_hat, title, out_png, out_svg):\n",
    "    fpr, tpr, _ = roc_curve(y_true, p_hat)\n",
    "    auc = roc_auc_score(y_true, p_hat)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\", linewidth=1)\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title); plt.legend(loc=\"lower right\"); plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(out_svg, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return auc\n",
    "\n",
    "# ---------- LOAD DESIGN ONCE ----------\n",
    "banner(\"LOAD DESIGN\")\n",
    "X_full  = load_npz(f\"{BASE}/Lasso_X.npz\").tocsr().astype(np.float32)\n",
    "persons = pd.read_csv(f\"{BASE}/person_index.csv\")[\"person_id\"].astype(str).to_numpy()\n",
    "codes   = pd.read_csv(f\"{BASE}/code_index.csv\")[\"concept_code\"].astype(str).to_numpy()\n",
    "print(f\"[INFO] Matrix: persons={X_full.shape[0]:,}  codes={X_full.shape[1]:,}\")\n",
    "if len(persons) != X_full.shape[0] or len(codes) != X_full.shape[1]:\n",
    "    raise ValueError(\"[ERROR] person/code indices do not match matrix shape.\")\n",
    "\n",
    "# ---------- RUN PER TARGET ----------\n",
    "summary = []\n",
    "for tcode in TARGETS:\n",
    "    pretty = TARGET_NAME.get(tcode, tcode)\n",
    "    banner(f\"TARGET {tcode} — {pretty}\")\n",
    "\n",
    "    # SECTION A: labels\n",
    "    subhead(\"A) Label vector from full design\")\n",
    "    idx_y = np.where(codes == tcode)[0]\n",
    "    if idx_y.size == 0:\n",
    "        print(f\"[SKIP] Target not found in code_index.csv → {tcode}\")\n",
    "        continue\n",
    "    y_full = X_full[:, idx_y[0]].toarray().ravel().astype(np.int8)\n",
    "    print(f\"[INFO] y_full: n={y_full.size:,}  pos={int(y_full.sum()):,}  \"\n",
    "          f\"prev={y_full.mean():.4f}\")\n",
    "\n",
    "    # SECTION B: predictors (keep all except DV)\n",
    "    subhead(\"B) Predictor matrix (keep all columns except DV)\")\n",
    "    mask_pred = (codes != tcode)\n",
    "    X = X_full[:, mask_pred]\n",
    "    feature_codes = codes[mask_pred]\n",
    "    print(f\"[INFO] Predictors: persons={X.shape[0]:,}  features={X.shape[1]:,}\")\n",
    "    print(f\"[CHECK] DV in features? {tcode in feature_codes} (should be False)\")\n",
    "\n",
    "    # SECTION C: sampling (all pos + 4x neg)\n",
    "    subhead(\"C) Sampling (keep ALL positives + 4× negatives)\")\n",
    "    sel = sample_all_pos_kx_neg(y_full, k=NEG_MULT, seed=RNG_SEED)\n",
    "    X_sub       = X[sel, :]\n",
    "    y_sub       = y_full[sel].astype(np.int8)\n",
    "    persons_sub = persons[sel]\n",
    "    n_rows      = X_sub.shape[0]\n",
    "    n_pos_sub   = int(y_sub.sum())\n",
    "    n_neg_sub   = n_rows - n_pos_sub\n",
    "    print(f\"[INFO] subset: n={n_rows:,}  pos={n_pos_sub:,}  neg={n_neg_sub:,}  \"\n",
    "          f\"ratio≈{(n_neg_sub/max(n_pos_sub,1)):.2f}:1  PR-baseline={y_sub.mean():.4f}\")\n",
    "    # a tiny peek at first 3 rows' active codes\n",
    "    try:\n",
    "        preview_active_codes(X_sub, feature_codes, row_indices=range(min(3, n_rows)), k=8)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] preview_active_codes failed: {e}\")\n",
    "\n",
    "    # SECTION D: coefficients\n",
    "    subhead(\"D) Load MAGI coefficients\")\n",
    "    coef_csv = COEF_PATTERN.format(target=tcode)\n",
    "    if not os.path.exists(coef_csv):\n",
    "        print(f\"[SKIP] Coef file missing: {coef_csv}\")\n",
    "        continue\n",
    "    intercept, coef_map = load_magi_betas(coef_csv)\n",
    "    print(f\"[INFO] Coefs: intercept={intercept:.6f}  n_features={len(coef_map):,}\")\n",
    "    # show a few coef samples\n",
    "    for k,(cc,bb) in enumerate(list(coef_map.items())[:5]):\n",
    "        print(f\"   beta[{cc}] = {bb:.6f}\")\n",
    "    if \"(intercept)\" not in open(coef_csv, 'r', encoding=\"utf-8\", errors=\"ignore\").read():\n",
    "        print(\"[NOTE] No explicit '(intercept)' row in CSV; using 0.0 if not found.\")\n",
    "\n",
    "    # SECTION E: alignment & scoring\n",
    "    subhead(\"E) Align & score\")\n",
    "    lp, p_hat, idx_overlap, betas_vec = None, None, None, None\n",
    "    try:\n",
    "        lp, p_hat, idx_cols, betas_vec = score_from_betas(X_sub, feature_codes, coef_map, intercept)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] Scoring failed (no overlap or other issue): {e}\")\n",
    "        continue\n",
    "\n",
    "    n_overlap = idx_cols.size\n",
    "    print(f\"[INFO] overlap with predictors = {n_overlap:,} columns\")\n",
    "    print(f\"[INFO] first 5 aligned columns: {[feature_codes[i] for i in idx_cols[:5]]}\")\n",
    "    print(f\"[INFO] first 5 aligned betas:   {[float(b) for b in betas_vec[:5]]}\")\n",
    "\n",
    "    # SECTION F: metrics & distributions\n",
    "    subhead(\"F) Metrics & probability distribution\")\n",
    "    auc    = roc_auc_score(y_sub, p_hat)\n",
    "    pr_auc = average_precision_score(y_sub, p_hat)\n",
    "    q = qtiles(p_hat)\n",
    "    print(f\"[RESULT] AUC={auc:.4f}  |  PR-AUC={pr_auc:.4f}  (baseline={y_sub.mean():.4f})\")\n",
    "    print(f\"[DIST] prob quantiles: min={q[0]:.4g}, p1={q[1]:.4g}, p25={q[2]:.4g}, \"\n",
    "          f\"median={q[3]:.4g}, p75={q[4]:.4g}, p99={q[5]:.4g}, max={q[6]:.4g}\")\n",
    "    print(f\"[COUNT] prob>=0.999: {(p_hat>=0.999).sum()}  |  prob<=0.001: {(p_hat<=0.001).sum()}\")\n",
    "\n",
    "    # SECTION G: save predictions\n",
    "    subhead(\"G) Save per-person predictions\")\n",
    "    safe = safe_name(pretty)\n",
    "    pred_csv = os.path.join(CSV_DIR, f\"pred_{safe}.csv\")\n",
    "    pd.DataFrame({\n",
    "        \"person_id\": persons_sub,\n",
    "        \"y_true\": y_sub.astype(int),\n",
    "        \"score_logit\": lp,\n",
    "        \"prob\": p_hat\n",
    "    }).to_csv(pred_csv, index=False)\n",
    "    print(f\"[SAVE] predictions → {pred_csv}\")\n",
    "    print(pd.read_csv(pred_csv).head(10))\n",
    "\n",
    "    # SECTION H: plots\n",
    "    subhead(\"H) ROC plots (PNG/SVG)\")\n",
    "    png_path = os.path.join(PNG_DIR, f\"ROC_{safe}.png\")\n",
    "    svg_path = os.path.join(PNG_DIR, f\"ROC_{safe}.svg\")\n",
    "    _auc = plot_roc(y_sub, p_hat, pretty, png_path, svg_path)\n",
    "    print(f\"[SAVE] ROC → {png_path}\")\n",
    "    print(f\"[SAVE] ROC → {svg_path}\")\n",
    "\n",
    "    # accumulate summary\n",
    "    summary.append({\n",
    "        \"target_code\": tcode,\n",
    "        \"target_name\": pretty,\n",
    "        \"n_cases\": n_rows,\n",
    "        \"n_pos\": n_pos_sub,\n",
    "        \"n_neg\": n_neg_sub,\n",
    "        \"feature_overlap\": n_overlap,\n",
    "        \"AUC\": auc,\n",
    "        \"PR_AUC\": pr_auc,\n",
    "        \"PR_baseline\": y_sub.mean(),\n",
    "        \"coef_csv\": coef_csv,\n",
    "        \"pred_csv\": pred_csv,\n",
    "        \"roc_png\": png_path\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
