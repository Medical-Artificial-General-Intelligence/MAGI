{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import math\n",
    "from typing import Dict, List, Union, Any, Optional, Set, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = os.getenv(\"MAGI_DB_PATH\", \"/projects/klybarge/pcori_ad/magi/magi_db/magi.db\")  # read-only; override via env\n",
    "OUT_DIR = \"./MAGI_LASSO\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_top500\"   \n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "\n",
    "TOP_K = 500  # 500 for others\n",
    "\n",
    "TARGETS = [\n",
    "    \"aa_meas_amitriptyline_rem\",\n",
    "    \"aa_meas_fluoxetine_rem\", \n",
    "    \"aa_meas_citalopram_rem\",\n",
    "    \"aa_meas_venlafaxine_rem\",\n",
    "    \"aa_meas_mirtazapine_rem\",\n",
    "    \"aa_meas_sertraline_rem\",\n",
    "    \"aa_meas_bupropion_rem\",\n",
    "    \"aa_meas_trazodone_rem\",\n",
    "    \"aa_meas_duloxetine_rem\",\n",
    "    \"aa_meas_escitalopram_rem\",\n",
    "    \"aa_meas_paroxetine_rem\",\n",
    "    \"aa_meas_nortriptyline_rem\",\n",
    "    \"aa_meas_other_rem\",\n",
    "    \"aa_meas_doxepin_rem\",\n",
    "    \"aa_meas_desvenlafaxine_rem\",\n",
    "]\n",
    "\n",
    "# RIGHT= k (predictor) and LEFT = Y/j\n",
    "def analyze_causal_sequence_py(\n",
    "    df_in: Union[str, pd.DataFrame],\n",
    "    *,\n",
    "    name_map: Dict[str, str] = None,     # raw_code -> friendly label (applied to BOTH columns)\n",
    "    events: List[str] = None,            # event names to KEEP (AFTER recoding). If None: auto-detect\n",
    "    force_outcome: str = None,           # if provided and present, force this to be the FINAL node (Y)\n",
    "    lambda_min_count: int = 15           # L-threshold for λ: if n_code < L ⇒ λ_{k,j}=0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    MAGI (Python): Reference routine with explicit comments.\n",
    "\n",
    "    ─────────────────────────────────────────────────────────────────────────────\n",
    "    COLUMN CONVENTION PER ROW:\n",
    "      Left  column: target_concept_code  (call this X for this row)\n",
    "      Right column: concept_code         (call this k for this row)\n",
    "\n",
    "      n_code_target        ≡  k ∧ X\n",
    "      n_code_no_target     ≡  k ∧ ¬X\n",
    "      n_target_no_code     ≡  ¬k ∧ X\n",
    "      n_no_target          ≡  total(¬X)\n",
    "      n_target             ≡  total(X)\n",
    "      n_code               ≡  total(k)\n",
    "      n_code_before_target ≡  count(k before X)\n",
    "      n_target_before_code ≡  count(X before k)\n",
    "\n",
    "    ORIENTATION (locked to your spec):\n",
    "      • Total effect T_{kY}:   read row (target = Y, code = k).\n",
    "      • Lambda     λ_{k,j}:    read row (target = j, code = k), and compute\n",
    "                               λ_{k,j} = n_code_target / n_code  with L-threshold on n_code.\n",
    "\n",
    "    TEMPORAL SCORE for each node Zi:\n",
    "      Score(Z_i) = Σ_{j≠i} [ C(Z_i≺Z_j) - C(Z_j≺Z_i) + C(Z_i∧¬Z_j) - C(Z_j∧¬Z_i) ]\n",
    "      Read from row (target=Z_i, code=Z_j):\n",
    "        - n_code_before_target      → C(Z_j≺Z_i)\n",
    "        - n_target_before_code      → C(Z_i≺Z_j)\n",
    "        - n_code_no_target          → C(Z_j∧¬Z_i)\n",
    "        - n_target_no_code          → C(Z_i∧¬Z_j)\n",
    "\n",
    "    T_{kY} from row (Y, k):\n",
    "      a = n_code_target        (k ∧ Y)\n",
    "      b = n_code_no_target     (k ∧ ¬Y)\n",
    "      c = n_target_no_code     (¬k ∧ Y)\n",
    "      d = n_no_target - b      (¬k ∧ ¬Y)   ← computed on the fly (no extra column needed)\n",
    "      With sample-size–anchored odds:\n",
    "         odds_k1 = a/b with guards; odds_k0 = c/d with guards; T = odds_k1 / odds_k0\n",
    "\n",
    "    DIRECT EFFECTS via backward recursion:\n",
    "      D_{k,Y} = ( T_{k,Y} - Σ_j λ_{k,j} D_{j,Y} ) / ( 1 - Σ_j λ_{k,j} ),\n",
    "      where j are downstream nodes between k and Y in the temporal order.\n",
    "\n",
    "    LOGISTIC LINK:\n",
    "      logit P(Y=1 | Z) = β0 + Σ_k β_k Z_k  with β_k = log D_{k,Y};\n",
    "      invalid/nonpositive D map to β_k=0.\n",
    "\n",
    "    RETURNS a dict with:\n",
    "      sorted_scores, temporal_order, order_used,\n",
    "      T_val, D_val, lambda_l, coef_df, beta_0, beta, logit_predictors, predict_proba\n",
    "    \"\"\"\n",
    "    # ── 0) Ingest & validate ───────────────────────────────────────────────────\n",
    "    df = pd.read_csv(df_in) if isinstance(df_in, str) else df_in.copy()\n",
    "\n",
    "    need_cols = [\n",
    "        \"target_concept_code\", \"concept_code\",\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\",\n",
    "        \"n_target_no_code\",\n",
    "        \"n_code\",\n",
    "        \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # Always treat endpoints as strings to avoid silent filter drop\n",
    "    df[\"target_concept_code\"] = df[\"target_concept_code\"].astype(str)\n",
    "\n",
    "    # Optional recoding\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "        df[\"concept_code\"]        = df[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # Limit to selected events (union if auto)\n",
    "    if events is None:\n",
    "        ev_t = df[\"target_concept_code\"].unique().tolist()\n",
    "        ev_c = df[\"concept_code\"].unique().tolist()\n",
    "        events = sorted(set(ev_t) | set(ev_c))\n",
    "    else:\n",
    "        events = [str(e) for e in events]\n",
    "\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events.\")\n",
    "\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numerics\n",
    "    num_cols = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\", \"n_target_no_code\",\n",
    "        \"n_code\", \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # ── 1) Temporal score (read rows: target=Z_i, code=Z_j) ────────────────────\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zi in events:\n",
    "        s = 0.0\n",
    "        for zj in (x for x in events if x != zi):\n",
    "            # Row oriented as (Zi, Zj)\n",
    "            pair = df[(df[\"target_concept_code\"] == zi) & (df[\"concept_code\"] == zj)]\n",
    "            if pair.empty:\n",
    "                continue\n",
    "            c_i_before_j = float(pair[\"n_target_before_code\"].sum(skipna=True))   # Zi before Zj\n",
    "            c_j_before_i = float(pair[\"n_code_before_target\"].sum(skipna=True))   # Zj before Zi\n",
    "            c_i_and_not_j = float(pair[\"n_target_no_code\"].sum(skipna=True))      # Zi ∧ ¬Zj\n",
    "            c_j_and_not_i = float(pair[\"n_code_no_target\"].sum(skipna=True))      # Zj ∧ ¬Zi\n",
    "            s += (c_i_before_j - c_j_before_i + c_i_and_not_j - c_j_and_not_i)\n",
    "        scores[zi] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Choose outcome Y: either forced or top-scoring node\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "    else:\n",
    "        outcome = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "\n",
    "    events_order = temporal_order              # earliest … → Y\n",
    "    nodes = events_order[:-1]            # everything before Y\n",
    "\n",
    "    # ── 2) T and λ (row orientations locked) ───────────────────────────────────\n",
    "    T_val = pd.Series(0.0, index=nodes, dtype=float)\n",
    "    D_val = pd.Series(np.nan, index=nodes, dtype=float)\n",
    "    lambda_l: Dict[str, pd.Series] = {}\n",
    "\n",
    "    for k in nodes:\n",
    "        # ---- T_{kY} from the single row (target=Y, code=k) ----\n",
    "        row_Yk = df[(df[\"target_concept_code\"] == outcome) & (df[\"concept_code\"] == k)]\n",
    "\n",
    "        # 2×2 cells (a,b,c,d) at (Y,k):\n",
    "        a = float(row_Yk[\"n_code_target\"].sum(skipna=True))        # k ∧ Y\n",
    "        b = float(row_Yk[\"n_code_no_target\"].sum(skipna=True))     # k ∧ ¬Y\n",
    "        c = float(row_Yk[\"n_target_no_code\"].sum(skipna=True))     # ¬k ∧ Y\n",
    "        # d is not in data; compute from the same row: d = total(¬Y) − (k ∧ ¬Y)\n",
    "        n_noY = float(row_Yk[\"n_no_target\"].max(skipna=True)) if not row_Yk.empty else 0.0\n",
    "        d = max(n_noY - b, 0.0)                                    # ¬k ∧ ¬Y\n",
    "\n",
    "        # Stratum sizes\n",
    "        N1 = a + b                   # k = 1\n",
    "        N0 = c + d                   # k = 0\n",
    "\n",
    "        # Sample-size–anchored odds for k=1 and k=0\n",
    "        if N1 == 0:\n",
    "            odds_k1 = 1.0\n",
    "        else:\n",
    "            if a == 0:\n",
    "                odds_k1 = 1.0 / (N1 + 1.0)\n",
    "            elif b == 0:\n",
    "                odds_k1 = (N1 + 1.0)\n",
    "            else:\n",
    "                odds_k1 = a / b\n",
    "\n",
    "        if N0 == 0:\n",
    "            odds_k0 = 1.0\n",
    "        else:\n",
    "            if c == 0:\n",
    "                odds_k0 = 1.0 / (N0 + 1.0)\n",
    "            elif d == 0:\n",
    "                odds_k0 = (N0 + 1.0)\n",
    "            else:\n",
    "                odds_k0 = c / d\n",
    "\n",
    "        T_val.loc[k] = float(odds_k1 / odds_k0) if odds_k0 > 0 else (N1 + 1.0)\n",
    "\n",
    "        # ---- λ_{k,j} from rows (target=j, code=k): λ = n_code_target / n_code ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_jk = df[(df[\"target_concept_code\"] == j) & (df[\"concept_code\"] == k)]\n",
    "            if row_jk.empty:\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            num = float(pd.to_numeric(row_jk[\"n_code_target\"], errors=\"coerce\").fillna(0.0).sum())\n",
    "            den = float(pd.to_numeric(row_jk[\"n_code\"],        errors=\"coerce\").fillna(0.0).sum())\n",
    "\n",
    "            # L-threshold on the conditioning size n_code (count of k)\n",
    "            if (den <= 0) or (den < lambda_min_count):\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            lam = num / den\n",
    "            lam = 0.0 if not np.isfinite(lam) else float(min(max(lam, 0.0), 1.0))\n",
    "            lam_pairs.append((j, lam))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ── 3) Backward recursion for direct effects D ─────────────────────────────\n",
    "    # Last antecedent (just before Y): no downstream → D = T\n",
    "    if len(nodes) >= 1:\n",
    "        last_anc = nodes[-1]\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "\n",
    "    # Walk backward for the rest\n",
    "    if len(nodes) > 1:\n",
    "        for k in list(reversed(nodes[:-1])):\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            downstream = list(lam_vec.index)  # j nodes after k (already resolved)\n",
    "            lam_vals = lam_vec.reindex(downstream).fillna(0.0).to_numpy()\n",
    "            D_down  = pd.to_numeric(D_val.reindex(downstream), errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vals * D_down))\n",
    "            den = 1.0 - float(np.nansum(lam_vals))\n",
    "\n",
    "            if (not np.isfinite(den)) or den == 0.0:\n",
    "                D_val.loc[k] = T_val.loc[k]            # neutralize if pathological\n",
    "            else:\n",
    "                tmp = num / den\n",
    "                D_val.loc[k] = tmp if np.isfinite(tmp) else T_val.loc[k]\n",
    "\n",
    "    # ── 4) Logistic link (β) and predict_proba ─────────────────────────────────\n",
    "    # Intercept β0 from marginal prevalence of Y (rows with target == Y)\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    n_t = float(resp_rows[\"n_target\"].max()) if not resp_rows.empty else np.nan\n",
    "    n_n = float(resp_rows[\"n_no_target\"].max()) if not resp_rows.empty else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    p_y = min(max(p_y, 1e-12), 1 - 1e-12)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))\n",
    "\n",
    "    # β_k = log D_{k,Y}; invalid/nonpositive → 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    beta_vals = np.log(D_clean.where(D_clean > 0.0)).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_vals.index) + [\"(intercept)\"],\n",
    "        \"beta\":      list(beta_vals.values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # Vectorized predict_proba\n",
    "    predictors = list(beta_vals.index)\n",
    "    beta_vec = beta_vals.values\n",
    "\n",
    "    def predict_proba(Z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Compute P(Y=1|Z) using: logit P = β0 + Σ_k β_k Z_k.\n",
    "        Z can be:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame containing any/all of `predictors` (others ignored)\n",
    "        \"\"\"\n",
    "        def sigmoid(x):\n",
    "            x = np.clip(x, -700, 700)  # numerical stability for large |η|\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "        if isinstance(Z, pd.DataFrame):\n",
    "            M = Z.reindex(columns=predictors, fill_value=0.0).astype(float).to_numpy()\n",
    "            return sigmoid(beta_0 + M @ beta_vec)\n",
    "\n",
    "        if isinstance(Z, (dict, pd.Series)):\n",
    "            v = np.array([float(Z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            return float(sigmoid(beta_0 + float(v @ beta_vec)))\n",
    "\n",
    "        arr = np.asarray(Z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            return float(sigmoid(beta_0 + float(arr @ beta_vec)))\n",
    "        if arr.ndim == 2:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*,{len(predictors)}), got {arr.shape}\")\n",
    "            return sigmoid(beta_0 + arr @ beta_vec)\n",
    "\n",
    "        raise ValueError(\"Unsupported input for predict_proba\")\n",
    "\n",
    "    # ── 5) Package results ─────────────────────────────────────────────────────\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "\n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only coerce numerics and compute total_effect if absent, using provided cells and\n",
    "    computing d = n_no_target - n_code_no_target on the fly (since d isn't in data).\n",
    "    \"\"\"\n",
    "    required = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target_no_code\", \"n_target\", \"n_no_target\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns for TE: {', '.join(missing)}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    for c in required:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    if \"total_effect\" not in out.columns:\n",
    "        a = out[\"n_code_target\"].astype(float)\n",
    "        b = out[\"n_code_no_target\"].astype(float)\n",
    "        c = out[\"n_target_no_code\"].astype(float)\n",
    "        d = (out[\"n_no_target\"] - out[\"n_code_no_target\"]).astype(float)\n",
    "        d = np.maximum(d, 0.0)\n",
    "\n",
    "        N1 = a + b\n",
    "        N0 = c + d\n",
    "\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            odds_k1 = np.where(\n",
    "                N1 == 0, 1.0,\n",
    "                np.where((a > 0) & (b > 0), a / b,\n",
    "                         np.where(b == 0, N1 + 1.0, 1.0 / (N1 + 1.0)))\n",
    "            )\n",
    "            odds_k0 = np.where(\n",
    "                N0 == 0, 1.0,\n",
    "                np.where((c > 0) & (d > 0), c / d,\n",
    "                         np.where(d == 0, N0 + 1.0, 1.0 / (N0 + 1.0)))\n",
    "            )\n",
    "            te = odds_k1 / odds_k0\n",
    "\n",
    "        out[\"total_effect\"] = np.where(np.isfinite(te), te, 1.0).astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# TOPK_HI/LO\n",
    "def _select_top_by_te_unique_k(k_to_T: pd.DataFrame, top_k: int = None, hi: float = 1.5) -> pd.DataFrame:\n",
    "    if top_k is None:\n",
    "        top_k = TOP_K\n",
    "    \"\"\"\n",
    "    Balanced selection without fallback:\n",
    "      - Take up to top_k//2 strongest risk (TE >= hi)\n",
    "      - Take up to top_k - top_k//2 strongest protective (TE <= 1/hi)\n",
    "      - If either side is short, do NOT fill from elsewhere; total may be < top_k\n",
    "      - Return one row per k (**concept_code**), ranked by extremeness\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    # Ensure total_effect numeric\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with total_effect; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # Extremeness symmetrical around 1\n",
    "    df[\"effect_strength\"] = np.where(df[\"total_effect\"] >= 1.0,\n",
    "                                     df[\"total_effect\"],\n",
    "                                     1.0 / df[\"total_effect\"])\n",
    "\n",
    "    # One row per k: **k is concept_code in (target=Y, code=k)**\n",
    "    best_per_k = (df.sort_values(\"effect_strength\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"concept_code\"], keep=\"first\"))\n",
    "\n",
    "    HI = hi\n",
    "    LO = 1.0 / hi\n",
    "\n",
    "    risk_pool = best_per_k[best_per_k[\"total_effect\"] >= HI].copy()\n",
    "    prot_pool = best_per_k[best_per_k[\"total_effect\"] <= LO].copy()\n",
    "\n",
    "    want_risk = top_k // 2\n",
    "    want_prot = top_k - want_risk\n",
    "\n",
    "    sel_risk = risk_pool.nlargest(min(len(risk_pool), want_risk), \"effect_strength\")\n",
    "    sel_prot = prot_pool.nlargest(min(len(prot_pool), want_prot), \"effect_strength\")\n",
    "\n",
    "    selected = pd.concat([sel_risk, sel_prot], ignore_index=True)\n",
    "\n",
    "    print(f\"[SELECT] total unique k={best_per_k['concept_code'].nunique():,}  \"\n",
    "          f\"risk={len(risk_pool):,}  prot={len(prot_pool):,}  \"\n",
    "          f\"selected(total)={selected['concept_code'].nunique()}  \"\n",
    "          f\"with: risk={len(sel_risk)}, prot={len(sel_prot)}\")\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _fetch_k_to_T(conn, outcome_code: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch all rows whose LEFT (target) == outcome Y.\n",
    "    These rows provide the single (Y, k) lines needed to compute T_{kY}.\n",
    "    \"\"\"\n",
    "    q = \"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- LEFT  (Y)\n",
    "             ccn.concept_code AS concept_code           -- RIGHT (k)\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code = ?                         -- <<< filter on LEFT == Y\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[outcome_code])\n",
    "\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list):\n",
    "    \"\"\"\n",
    "    Fetch the induced subgraph for the event set on the LEFT side.\n",
    "    This captures all rows (X, k) where X ∈ events_set and k ∈ events_set,\n",
    "    which includes:\n",
    "      • (Y, k) rows (needed for T_{kY})\n",
    "      • (j, k) rows (needed for λ_{k,j})\n",
    "    \"\"\"\n",
    "    ph = \",\".join([\"?\"] * len(events_list))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- LEFT\n",
    "             ccn.concept_code AS concept_code           -- RIGHT\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=list(events_list))\n",
    "\n",
    "# ========= main loop =========\n",
    "if __name__ == '__main__':\n",
    "    # Runtime safety checks\n",
    "    uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "    if not os.path.exists(MAGI_DB_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"MAGI_DB_PATH not found: {MAGI_DB_PATH}. \"\n",
    "            f\"Set MAGI_DB_PATH to a valid SQLite DB or run with CSV input.\"\n",
    "        )\n",
    "\n",
    "    with sqlite3.connect(uri, uri=True) as conn:\n",
    "        for T in TARGETS:\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "            # 1) Y→k rows to compute T_{kY}\n",
    "            k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "            # DEDUP\n",
    "            before = len(k_to_T)\n",
    "            k_to_T = k_to_T.drop_duplicates()\n",
    "            after = len(k_to_T)\n",
    "            if after < before:\n",
    "                print(f\"[DEDUP] k_to_T exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "            if k_to_T.empty:\n",
    "                print(f\"[WARN] No predictor→target rows for {T}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 2) derive totals/effects on the Y→k list\n",
    "            k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "            # 3) pick predictors by TE (RIGHT side = concept_code)\n",
    "            sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "            if sel_rows.empty:\n",
    "                print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Save selected factors for audit\n",
    "            sub_csv = os.path.join(OUT_DIR, f\"risk_prot_{T}.csv\")\n",
    "            os.makedirs(OUT_DIR, exist_ok=True)\n",
    "            sel_rows.to_csv(sub_csv, index=False)\n",
    "            print(f\"[SAVED] Factors → {sub_csv}\")\n",
    "\n",
    "            # selected_k := chosen predictors; never include T itself\n",
    "            selected_k = set(sel_rows[\"concept_code\"].astype(str))\n",
    "            if T in selected_k:\n",
    "                selected_k.remove(T)\n",
    "\n",
    "            print(f\"[SELECT] unique k available={k_to_T['concept_code'].nunique():,}  \"\n",
    "                  f\"selected={len(selected_k):,}\")\n",
    "\n",
    "            if not selected_k:\n",
    "                print(f\"[WARN] No predictors after removing target {T}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 4) subgraph for λ: LEFT in {T} ∪ selected_k; RIGHT in selected_k only\n",
    "            events_set    = selected_k | {T}\n",
    "            right_allowed = selected_k\n",
    "\n",
    "            df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "            df_trim = df_trim[df_trim[\"concept_code\"].isin(right_allowed)].copy()\n",
    "\n",
    "            # DEDUP\n",
    "            before = len(df_trim)\n",
    "            df_trim = df_trim.drop_duplicates()\n",
    "            after = len(df_trim)\n",
    "            if after < before:\n",
    "                print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "            # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "            df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "            # Sanity: both Y→k and j→k present\n",
    "            y_to_k = int((df_trim[\"target_concept_code\"] == T).sum())\n",
    "            j_to_k = int(((df_trim[\"target_concept_code\"] != T) &\n",
    "                          (df_trim[\"concept_code\"].isin(right_allowed))).sum())\n",
    "            print(f\"[TRIM] rows={len(df_trim):,} events={len(events_set)}  Y→k={y_to_k}  j→k={j_to_k}\")\n",
    "\n",
    "            # 5) save subgraph (audit)\n",
    "            sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "            df_trim.to_csv(sub_csv, index=False)\n",
    "            print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "            # 6) run MAGI with outcome forced to T; no alias/name_map in this version\n",
    "            try:\n",
    "                res = analyze_causal_sequence_py(\n",
    "                    df_trim, events=None, name_map=None, force_outcome=T\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"[ERROR] MAGI failed:\", e)\n",
    "                continue\n",
    "\n",
    "            # 7) save NON-ZERO coefficients, whitelisting to selected_k (+ intercept)\n",
    "            outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "            coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "            # robust: accept 'coef' or 'beta' column name\n",
    "            coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "            if coef_col is None:\n",
    "                raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "            eps = 1e-12\n",
    "            mask_nz = coef_df[coef_col].astype(float).abs() > eps\n",
    "            coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "            # Keep only intercept or predictors in selected_k\n",
    "            is_intercept = coef_nz[\"predictor\"].eq(\"(intercept)\")\n",
    "            in_whitelist = coef_nz[\"predictor\"].isin(selected_k)\n",
    "            keep_mask = is_intercept | in_whitelist\n",
    "            dropped = int((~keep_mask).sum())\n",
    "            if dropped:\n",
    "                print(f\"[FILTER] Dropping {dropped} predictor(s) not in whitelist.\")\n",
    "\n",
    "            coef_nz = coef_nz.loc[keep_mask]\n",
    "\n",
    "            coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "            coef_nz.to_csv(coef_csv, index=False)\n",
    "            print(f\"[SAVED] Coefficients (non-zero only) → {coef_csv}  \"\n",
    "                  f\"| kept={len(coef_nz):,} of {len(coef_df):,}  \"\n",
    "                  f\"| nodes={len(res.get('order_used', [])) - 1}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
