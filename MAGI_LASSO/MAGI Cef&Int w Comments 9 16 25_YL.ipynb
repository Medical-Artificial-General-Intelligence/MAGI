{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# typing: static type hints for clarity & IDE/tooling support\n",
    "from typing import Union, Dict, List, Any\n",
    "# pandas: tabular data I/O (CSV) and DataFrame transforms/joins/group-bys\n",
    "import pandas as pd\n",
    "# numpy: fast vectorized numerics (arrays, nansum, clipping) for counts & logits\n",
    "import numpy as np\n",
    "# math: scalar math utilities; used for log in safe_log (per-element, non-vector)\n",
    "import math\n",
    "# build file paths, make folders, read env vars, list files, etc.\n",
    "import os\n",
    "# open a connection to a DB file, run SQL queries, fetch results\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d591da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### MAGI FUNCTION ###############\n",
    "\"\"\"\n",
    "MAGI: Dependent Bayes with Temporal Ordering — Reference Implementation\n",
    "-------------------------------------------------------------------------------\n",
    "This file adds **step-by-step comments** to the function `analyze_causal_sequence_py`. The comments mirror the proposed\n",
    "algorithm sections:\n",
    "\n",
    "1) Determination of Temporal Order\n",
    "2) Estimation of Dependent Bayes (T-values, λ-links, and D-values)\n",
    "3) Logistic link to produce P(Y=1 | Z)\n",
    "\n",
    "INPUT TABLE EXPECTATIONS (long format, one row per directed pair):\n",
    "- target_concept_code : str (# left node in the edge; in many places this\n",
    "denotes the earlier event)\n",
    "- concept_code : str (# right node in the edge; the later event)\n",
    "- n_code_target : float/int (# co-occurrence count for concept & target\n",
    "in the *target=Y* stratum; used as 'a')\n",
    "- n_code_no_target : float/int (# co-occurrence count for concept & target\n",
    "in the *target=¬Y* stratum; used as 'b')\n",
    "- n_target : float/int (# total Y count for this target concept, per edge row; we take max within a block)\n",
    "- n_no_target : float/int (# total ¬Y count; max within a block)\n",
    "- no_code_no_target : float/int (# optional, computed if missing as n_no_target - n_code_no_target; clipped ≥0)\n",
    "- n_target_before_code: float/int (# count where target occurred before code)\n",
    "- n_code_before_target: float/int (# count where code occurred before target)\n",
    "\n",
    "NOTES ON TEMPORAL COUNTS:\n",
    "For a row with target_concept_code = A and concept_code = B, the columns\n",
    "`n_code_before_target` and `n_target_before_code` are interpreted as:\n",
    "- n_code_before_target: # of persons where **B happened before A**\n",
    "- n_target_before_code: # of persons where **A happened before B**\n",
    "We aggregate these across j≠i to compute *temporal scores* for each event.\n",
    "\n",
    "PIECEWISE, SAMPLE-SIZE–ANCHORED ADJUSTMENTS:\n",
    "- For odds terms that would be 0 or ∞ due to zero cells, we replace the\n",
    "offending odds with 1/(N+1) or (N+1)/1, where N is the size of the\n",
    "appropriate stratum, so all ratios remain finite and interpretable.\n",
    "\n",
    "RETURN VALUE:\n",
    "A dict with temporal ordering, T-values, λ-vectors, D-values, coefficients\n",
    "for a logistic link, a `predict_proba` callable, and trace tables.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def analyze_causal_sequence_py(\n",
    "    data: Union[str, pd.DataFrame],\n",
    "    name_map: Dict[str, str],\n",
    "    events: List[str],\n",
    "    force_outcome=None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Compute temporal order, dependent-Bayes direct effects (D), and\n",
    "    a logistic-link probability for outcome Y from pairwise counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str | DataFrame\n",
    "        CSV path or in-memory DataFrame with the columns described above.\n",
    "    name_map : Dict[str, str]\n",
    "        Optional mapping raw code -> friendly label. If provided, both\n",
    "        `target_concept_code` and `concept_code` are replaced.\n",
    "    events : List[str]\n",
    "        List of event names/codes to restrict the analysis to. If `None`,\n",
    "        events are auto-detected from the table and intersected.\n",
    "    force_outcome : str | None\n",
    "        If provided and found among events, this event is forced to be the\n",
    "        **final** node (i.e., the outcome) in the temporal order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        - sorted_scores : pd.Series of temporal scores (desc)\n",
    "        - temporal_order: list of events (outcome at the end)\n",
    "        - order_used    : same as temporal_order\n",
    "        - T_val         : pd.Series of total effects T_{k,Y}\n",
    "        - D_val         : pd.Series of direct effects D_{k,Y}\n",
    "        - coef_df       : pd.DataFrame of coefficients (β_k and intercept)\n",
    "        - lambda_l      : dict[str -> pd.Series] of λ_{k,j} vectors\n",
    "        - trace_df      : pd.DataFrame detailing the backward recursion steps\n",
    "        - invalid_predictors: list of predictors whose log(D) was invalid\n",
    "        - beta_0, beta, logit_predictors, predict_proba: logistic elements\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 0) INGEST & BASIC VALIDATION\n",
    "    # ---------------------------------------------------------------------\n",
    "    if isinstance(data, str):\n",
    "        # Read from CSV path\n",
    "        df = pd.read_csv(data)\n",
    "    else:\n",
    "        # Work on a copy to avoid mutating caller's object\n",
    "        df = data.copy()\n",
    "\n",
    "    # Ensure required identifier columns are present\n",
    "    for col in [\"target_concept_code\", \"concept_code\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Optional recoding to human-friendly labels\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "        df[\"concept_code\"] = df[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # Numeric columns the algorithm expects\n",
    "    need = [\n",
    "        \"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\",\n",
    "        \"n_target_before_code\", \"n_code_before_target\",\n",
    "    ]\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # If precomputed total_effect exists (for λ or sanity checks), coerce to numeric\n",
    "    has_total = \"total_effect\" in df.columns\n",
    "    if has_total:\n",
    "        df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) EVENT SET (optional auto-detect) & TYPE COERCION\n",
    "    # ---------------------------------------------------------------------\n",
    "    if events is None:\n",
    "        # Auto-detect: intersect events appearing on both sides of edges\n",
    "        ev_targets = df[\"target_concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        ev_code = df[\"concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        events = sorted(set(ev_targets).intersection(ev_code))\n",
    "        if len(events) == 0:\n",
    "            # Fall back to union if intersection is empty\n",
    "            events = sorted(set(ev_targets) | set(ev_code))\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events after auto-detection.\")\n",
    "\n",
    "    # Keep only rows whose endpoints are both in the chosen event set\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numeric columns robustly (invalid -> NaN); subsequent ops handle NaNs\n",
    "    for c in need:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # If `no_code_no_target` missing, derive as (n_no_target - n_code_no_target) ≥ 0\n",
    "    if \"no_code_no_target\" not in df.columns:\n",
    "        df[\"no_code_no_target\"] = (df[\"n_no_target\"] - df[\"n_code_no_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        df[\"no_code_no_target\"] = pd.to_numeric(df[\"no_code_no_target\"], errors=\"coerce\").clip(lower=0)\n",
    "\n",
    "    # Helper: total count for an event (max n_target where that event is target)\n",
    "    # This mirrors your original choice; change to .sum() if warranted.\n",
    "    def C_of(ev: str) -> float:\n",
    "        sub = df[df[\"target_concept_code\"] == ev]\n",
    "        if sub.empty:\n",
    "            return float(\"nan\")\n",
    "        C = pd.to_numeric(sub[\"n_target\"], errors=\"coerce\").max()\n",
    "        return float(C) if pd.notna(C) and np.isfinite(C) else float(\"nan\")\n",
    "\n",
    "    # Helper: numerically safe log (log(1)=0 for invalid/≤0)\n",
    "    def safe_log(x: float) -> float:\n",
    "        try:\n",
    "            xv = float(x)\n",
    "        except (TypeError, ValueError):\n",
    "            return 0.0\n",
    "        if not np.isfinite(xv) or xv <= 0.0:\n",
    "            return 0.0\n",
    "        return math.log(xv)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) TEMPORAL ORDER — pairwise-before counts → per-node score\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Score(Z_i) = Σ_{j≠i} [ C(Z_i≪Z_j) - C(Z_j≪Z_i) + C(Z_i ∩ ¬Z_j) - C(Z_j ∩ ¬Z_i) ]\n",
    "    # Here we implement the *before/after* portion using the provided columns.\n",
    "    # If only presence/absence terms are available, you may approximate using\n",
    "    # the last two terms.\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zk in events:\n",
    "        s = 0.0\n",
    "        for zj in [x for x in events if x != zk]:\n",
    "            # For the pair (zj, zk), we interpret:\n",
    "            #   n_code_before_target   — # where zk (code) before zj (target)\n",
    "            #   n_target_before_code   — # where zj (target) before zk (code)\n",
    "            rowr = df[(df[\"target_concept_code\"] == zj) & (df[\"concept_code\"] == zk)]\n",
    "            if not rowr.empty:\n",
    "                s += float(rowr[\"n_code_before_target\"].sum(skipna=True) -\n",
    "                           rowr[\"n_target_before_code\"].sum(skipna=True))\n",
    "        scores[zk] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Outcome selection:\n",
    "    #  - If `force_outcome` is provided and present, put it at the end.\n",
    "    #  - Else, default to the top-scoring node as outcome.\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome_event = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "    else:\n",
    "        outcome_event = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "\n",
    "    # Propagation order is the temporal order; last is outcome Y\n",
    "    events_order = temporal_order\n",
    "    outcome = events_order[-1]\n",
    "    antecedents = events_order[:-1]\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) T-VALUES (TOTAL EFFECTS) & λ-LINKS BETWEEN ANTECEDENTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # For each antecedent k, we compute T_{k,Y} as an odds ratio between\n",
    "    # strata Z_k=1 and Z_k=0 with sample-size–anchored fixes for zero cells.\n",
    "    # For λ_{k,j} (dependence of j given k), we first use precomputed\n",
    "    # `total_effect` if present; otherwise, compute from counts with similar\n",
    "    # corrections and/or normalized co-occurrence.\n",
    "\n",
    "    T_val = pd.Series(0.0, index=antecedents, dtype=float)  # T_{k,Y}\n",
    "    D_val = pd.Series(np.nan, index=antecedents, dtype=float)  # D_{k,Y}\n",
    "    lambda_l: Dict[str, pd.Series] = {}  # per-k vector of λ_{k,j} for j after k and before Y\n",
    "\n",
    "    for k in antecedents:\n",
    "        # ---- Contingency for (k, outcome) ----\n",
    "        row_ko = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == outcome)]\n",
    "\n",
    "        # a: Y∩k    b: ¬Y∩k    c: Y∩¬k    d: ¬Y∩¬k\n",
    "        a = float(row_ko[\"n_code_target\"].sum(skipna=True))            # co-occurrence in Y\n",
    "        b = float(row_ko[\"n_code_no_target\"].sum(skipna=True))         # co-occurrence in ¬Y\n",
    "\n",
    "        # If n_target_no_code absent, approximate c by (max n_target - a)\n",
    "        if \"n_target_no_code\" in row_ko.columns:\n",
    "            c = float(row_ko[\"n_target_no_code\"].sum(skipna=True))\n",
    "        else:\n",
    "            c = float(pd.to_numeric(row_ko[\"n_target\"], errors=\"coerce\").max() - a)\n",
    "\n",
    "        # If no_code_no_target absent, approximate d by (max n_no_target - b)\n",
    "        if \"no_code_no_target\" in row_ko.columns:\n",
    "            d = float(row_ko[\"no_code_no_target\"].sum(skipna=True))\n",
    "        else:\n",
    "            d = float(pd.to_numeric(row_ko[\"n_no_target\"], errors=\"coerce\").max() - b)\n",
    "\n",
    "        N1, N0 = a + b, c + d  # stratum sizes for Z_k=1 and Z_k=0\n",
    "\n",
    "        # ---- Sample-size–anchored odds for Z_k=1 ----\n",
    "        if a == 0:\n",
    "            odds_pos_adj = 1.0 / (N1 + 1.0)  # 0 → tiny positive odds\n",
    "        elif b == 0:\n",
    "            odds_pos_adj = (N1 + 1.0) / 1.0  # ∞ → capped by (N1+1)\n",
    "        else:\n",
    "            odds_pos_adj = a / b\n",
    "\n",
    "        # ---- Sample-size–anchored odds for Z_k=0 ----\n",
    "        if c == 0:\n",
    "            odds_neg_adj = 1.0 / (N0 + 1.0)\n",
    "        elif d == 0:\n",
    "            odds_neg_adj = (N0 + 1.0) / 1.0\n",
    "        else:\n",
    "            odds_neg_adj = c / d\n",
    "\n",
    "        # Total effect T_{k,Y}\n",
    "        T_val.loc[k] = float(odds_pos_adj / odds_neg_adj)\n",
    "\n",
    "        # ---- λ_{k,j}: dependence of j on k for nodes j after k (and before Y) ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_kj = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == j)]\n",
    "            if row_kj.empty:\n",
    "                lam_pairs.append((j, 0.0))  # no evidence of dependence\n",
    "                continue\n",
    "\n",
    "            # Prefer precomputed total_effect if present for this edge\n",
    "            #te = float(pd.to_numeric(row_kj[\"total_effect\"], errors=\"coerce\").max()) if has_total else float(\"nan\")\n",
    "            #if np.isfinite(te):\n",
    "             #   lam_pairs.append((j, te))\n",
    "              #  continue\n",
    "\n",
    "            # Otherwise approximate λ with piecewise, size-anchored logic\n",
    "            # C11 = C(j∩k); Cj_not_k = C(j∩¬k); Ck = C(k)\n",
    "            C11 = float(row_kj[\"n_code_target\"].sum(skipna=True))  # re-using the same column name for co-occurrence\n",
    "            if \"n_code_no_target\" in row_kj.columns:\n",
    "                Cj_not_k = float(row_kj[\"n_code_no_target\"].sum(skipna=True))\n",
    "            else:\n",
    "                Cj = C_of(j)\n",
    "                Cj_not_k = 0.0 if (not np.isfinite(Cj)) else max(Cj - C11, 0.0)\n",
    "            Ck = C_of(k)\n",
    "\n",
    "            if Cj_not_k == 0:\n",
    "                L = 1.0 + C11          # always-with-k → boost\n",
    "            elif C11 == 0:\n",
    "                L = 1.0 / (1.0 + Cj_not_k)  # never-with-k → downweight\n",
    "            elif np.isfinite(Ck) and Ck > 0:\n",
    "                L = C11 / Ck           # normalized co-occurrence\n",
    "            else:\n",
    "                L = 0.0\n",
    "\n",
    "            lam_pairs.append((j, float(L)))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4) BACKWARD RECURSION — resolve D_{k,Y} from T and λ\n",
    "    # ---------------------------------------------------------------------\n",
    "    # D_{k,Y} = ( T_{k,Y} - Σ_i λ_{k,k+i} * D_{k+i,Y} ) / ( 1 - Σ_i λ_{k,k+i} )\n",
    "    # Start at the last antecedent (just before Y): D := T, since there are no\n",
    "    # downstream nodes to adjust for.\n",
    "\n",
    "    trace_rows = []  # for human-auditable tracing of the recursion\n",
    "\n",
    "    last_anc = antecedents[-1] if antecedents else None\n",
    "    if last_anc is not None:\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "        trace_rows.append({\n",
    "            \"stage\": \"Last 2 Nodes\",\n",
    "            \"nodes\": f\"{last_anc} - {outcome}\",\n",
    "            \"k\": last_anc,\n",
    "            \"T_kY\": T_val.loc[last_anc],\n",
    "            \"lambda_terms\": None,\n",
    "            \"sum_lambda\": 0.0,\n",
    "            \"D_kY\": D_val.loc[last_anc],\n",
    "            \"log_D\": safe_log(D_val.loc[last_anc]),\n",
    "        })\n",
    "\n",
    "    if len(antecedents) > 1:\n",
    "        # Walk backwards through the remaining antecedents\n",
    "        for k in list(reversed(antecedents))[1:]:\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            # When computing the adjustment, use **only** D-values for nodes that\n",
    "            # are after k and already resolved (present in lam_vec index).\n",
    "            code = list(lam_vec.index)\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vec.reindex(code).values * D_val.reindex(code).values))\n",
    "            den = 1.0 - float(np.nansum(lam_vec.values))  # may approach 0 if λ's are large\n",
    "\n",
    "            # If den is pathological (≤0 or NaN), fall back to T (neutralization).\n",
    "            D_val.loc[k] = (num / den) if np.isfinite(num / den) else T_val.loc[k]\n",
    "\n",
    "            span = len(events_order) - events_order.index(k) + 1\n",
    "            lam_str = \", \".join(\n",
    "                f\"λ_{events_order.index(k)+1}{events_order.index(c)+1}={lam_vec[c]:.6f}\"\n",
    "                for c in code\n",
    "            ) if len(lam_vec) else None\n",
    "\n",
    "            trace_rows.append({\n",
    "                \"stage\": f\"Last {span} Nodes\",\n",
    "                \"nodes\": \" - \".join([k] + events_order[events_order.index(k)+1:]),\n",
    "                \"k\": k,\n",
    "                \"T_kY\": T_val.loc[k],\n",
    "                \"lambda_terms\": lam_str,\n",
    "                \"sum_lambda\": float(np.nansum(lam_vec.values)),\n",
    "                \"D_kY\": D_val.loc[k],\n",
    "                \"log_D\": safe_log(D_val.loc[k]),\n",
    "            })\n",
    "\n",
    "    trace_df = pd.DataFrame(trace_rows)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 5) COEFFICIENTS — map D's onto a logistic link\n",
    "    # ---------------------------------------------------------------------\n",
    "    # We model:  logit P(Y=1 | Z) = β0 + Σ_k β_k * Z_k,  with β_k = log D_{k,Y}\n",
    "    # Intercept β0 is set by the marginal prevalence of Y (from `n_target` &\n",
    "    # `n_no_target`) for the outcome rows.\n",
    "\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    if resp_rows.empty:\n",
    "        raise ValueError(f\"No rows for outcome '{outcome}'.\")\n",
    "\n",
    "    n_t = resp_rows[\"n_target\"].dropna().iloc[0] if resp_rows[\"n_target\"].dropna().size else np.nan\n",
    "    n_n = resp_rows[\"n_no_target\"].dropna().iloc[0] if resp_rows[\"n_no_target\"].dropna().size else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))  # stable enough for p∈(0,1)\n",
    "\n",
    "    # β_k = log(D_{k,Y}); protect against non-positive D by mapping to 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    D_pos = D_clean.where(D_clean > 0)\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        beta_vals = np.log(D_pos.to_numpy())\n",
    "    beta_k_raw = pd.Series(beta_vals, index=D_val.index)\n",
    "    invalid_predictors = list(beta_k_raw[~np.isfinite(beta_k_raw)].index)\n",
    "\n",
    "    beta_k = beta_k_raw.copy()\n",
    "    beta_k[~np.isfinite(beta_k)] = 0.0  # neutralize invalid predictors\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_k.index) + [\"(intercept)\"],\n",
    "        \"beta\": list(beta_k.astype(float).values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6) PREDICT_PROBA — vectorized logistic link\n",
    "    # ---------------------------------------------------------------------\n",
    "    predictors = list(beta_k.index)\n",
    "    beta_vec = beta_k.astype(float).values\n",
    "\n",
    "    def predict_proba(z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]) -> Union[float, np.ndarray, pd.Series]:\n",
    "        \"\"\"Compute P(Y=1 | Z) using the logistic link.\n",
    "\n",
    "        Accepts:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame with columns containing any/all of `predictors` (others ignored)\n",
    "\n",
    "        Returns:\n",
    "          - float for 1D inputs; np.ndarray or pd.Series for vectorized inputs\n",
    "        \"\"\"\n",
    "        if isinstance(z, pd.DataFrame):\n",
    "            Z = z.reindex(columns=predictors, fill_value=0).astype(float).to_numpy()\n",
    "            eta = beta_0 + Z @ beta_vec\n",
    "            # Stable sigmoid via clipping; avoids overflow for extreme η\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "        if isinstance(z, (dict, pd.Series)):\n",
    "            v = np.array([float(z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            eta = beta_0 + float(v @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "\n",
    "        arr = np.asarray(z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            eta = beta_0 + float(arr @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "        else:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*, {len(predictors)}), got {arr.shape}\")\n",
    "            eta = beta_0 + arr @ beta_vec\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7) PACKAGE RESULTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"trace_df\": trace_df,\n",
    "        \"invalid_predictors\": invalid_predictors,\n",
    "        # Logistic link outputs:\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "\n",
    "# ========= helpers =========\n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure derived columns exist and are numeric; compute `total_effect` if missing.\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    This normalizes the input long-format edge table so downstream MAGI steps\n",
    "    can rely on consistent columns and numeric types.\n",
    "\n",
    "    What it does\n",
    "    ------------\n",
    "    1) Coerces relevant count columns to numeric (invalids -> NaN).\n",
    "    2) Derives `no_code_no_target` if missing, as max(n_no_target - n_code_no_target, 0).\n",
    "    3) Computes `total_effect` (TE) per row **if not already present**:\n",
    "       TE ≈ odds ratio proxy for (target_concept_code -> concept_code) based on\n",
    "       available counts. This is used later as a λ candidate when building\n",
    "       dependence links between antecedents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Long-format table with at least:\n",
    "        ['target_concept_code','concept_code','n_code_target','n_code_no_target',\n",
    "         'n_target','n_no_target','n_target_before_code','n_code_before_target'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A copy of `df` with coerced numeric columns, ensured/derived counts,\n",
    "        and a `total_effect` column present.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # -- 1) Coerce to numeric (robust to garbage / missing strings)\n",
    "    base_cols = [\n",
    "        \"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\",\n",
    "        \"n_target_before_code\", \"n_code_before_target\",\n",
    "        \"no_code_no_target\", \"n_target_no_code\", \"total_effect\",\n",
    "    ]\n",
    "    for c in base_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # -- 2) Derive no_code_no_target if absent (the \"d\" cell in a 2x2)\n",
    "    if \"no_code_no_target\" not in df.columns:\n",
    "        # clip at 0 to avoid negative residuals due to data noise\n",
    "        df[\"no_code_no_target\"] = (df[\"n_no_target\"] - df[\"n_code_no_target\"]).clip(lower=0.0)\n",
    "\n",
    "    # -- 3) Total_effect (OR with sample-size–anchored smoothing)\n",
    "    if \"total_effect\" not in df.columns:\n",
    "        a  = df[\"n_code_target\"].astype(float)                      # k=1, T=1\n",
    "        b  = df[\"n_code_no_target\"].astype(float)                   # k=1, T=0\n",
    "        c_ = (df[\"n_target\"].astype(float) - a).clip(lower=0.0)     # k=0, T=1\n",
    "        d  = (df[\"n_no_target\"].astype(float) - b).clip(lower=0.0)  # k=0, T=0\n",
    "        N1, N0 = (a+b), (c_+d)\n",
    "\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            odds_pos = np.where((a>0)&(b>0), a/b, np.where(b==0, N1+1.0, 1.0/(N1+1.0)))\n",
    "            odds_neg = np.where((c_>0)&(d>0), c_/d, np.where(d==0, N0+1.0, 1.0/(N0+1.0)))\n",
    "            df[\"total_effect\"] = odds_pos / odds_neg\n",
    "\n",
    "    return df\n",
    "\n",
    "# TOPK_HI/LO\n",
    "def _select_by_risk_prot(k_to_T: pd.DataFrame, top_k: int = 500,\n",
    "                                   hi: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced selection without fallback:\n",
    "      - Take up to top_k//2 strongest risk (TE >= hi)\n",
    "      - Take up to top_k - top_k//2 strongest protective (TE <= 1/hi)\n",
    "      - If either side is short, do NOT fill from elsewhere; total may be < top_k\n",
    "      - Return one row per k (target_concept_code), ranked by extremeness\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    # Ensure total_effect numeric\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"]).copy()\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Extremeness symmetrical around 1\n",
    "    df[\"effect_strength\"] = np.where(df[\"total_effect\"] >= 1.0,\n",
    "                                     df[\"total_effect\"],\n",
    "                                     1.0 / df[\"total_effect\"])\n",
    "\n",
    "    # One row per k (keep most extreme)\n",
    "    best_per_k = (df.sort_values(\"effect_strength\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"target_concept_code\"], keep=\"first\"))\n",
    "\n",
    "    HI = hi\n",
    "    LO = 1.0 / hi\n",
    "\n",
    "    risk_pool = best_per_k[best_per_k[\"total_effect\"] >= HI].copy()\n",
    "    prot_pool = best_per_k[best_per_k[\"total_effect\"] <= LO].copy()\n",
    "\n",
    "    # Target split\n",
    "    want_risk = top_k // 2\n",
    "    want_prot = top_k - want_risk\n",
    "\n",
    "    # Take strongest from each side, capped by availability\n",
    "    take_risk = min(len(risk_pool), want_risk)\n",
    "    take_prot = min(len(prot_pool), want_prot)\n",
    "\n",
    "    sel_risk = risk_pool.nlargest(take_risk, \"effect_strength\")\n",
    "    sel_prot = prot_pool.nlargest(take_prot, \"effect_strength\")\n",
    "\n",
    "    selected = pd.concat([sel_risk, sel_prot], ignore_index=True)\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "\n",
    "def _fetch_k_to_T(conn, target_code: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch only k→T rows (concept_code == target).\"\"\"\n",
    "    q = \"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- k (predictor)\n",
    "             ccn.concept_code AS concept_code           -- T (outcome)\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE ccn.concept_code = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[target_code])\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list):\n",
    "    \"\"\"Fetch edges with target_concept_code IN events_set (single IN to avoid 999 param issues).\"\"\"\n",
    "    ph = \",\".join([\"?\"] * len(events_list))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=list(events_list))\n",
    "\n",
    "# ========= main loop =========\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    for T in TARGETS:\n",
    "\n",
    "        # 1) k→T rows (concept_code == T)\n",
    "        k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "        # --- drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(k_to_T)\n",
    "        k_to_T = k_to_T.drop_duplicates()\n",
    "        after = len(k_to_T)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] k_to_T exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        if k_to_T.empty:\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects on the k→T list\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) select predictors\n",
    "        sel_rows = _select_by_risk_prot(k_to_T, top_k=500)\n",
    "        if sel_rows.empty:\n",
    "            continue\n",
    "\n",
    "        # IMPORTANT: predictors are the LEFT side (target_concept_code)\n",
    "        selected_k = set(sel_rows[\"target_concept_code\"].astype(str))\n",
    "\n",
    "        # 4) build subgraph among {T} ∪ selected_k\n",
    "        events_set = selected_k | {T}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].isin(events_set) &\n",
    "            df_trim[\"concept_code\"].isin(events_set)\n",
    "        ].copy()\n",
    "\n",
    "        # --- drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(df_trim)\n",
    "        df_trim = df_trim.drop_duplicates()\n",
    "        after = len(df_trim)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "        df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "        #   k→T rows: concept_code == T\n",
    "        #   T→j rows: target_concept_code == T\n",
    "\n",
    "        # 5) save subgraph (for audit)\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "\n",
    "        # 6) run MAGI\n",
    "        try:\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=None, force_outcome=T)\n",
    "        except TypeError:\n",
    "            res = analyze_causal_sequence_py(df_trim)\n",
    "            outcome_used = res.get(\"order_used\", [None])[-1]\n",
    "            if outcome_used != T:\n",
    "                print(f\"[NOTE] outcome auto-inferred as {outcome_used}, not {T}\")\n",
    "\n",
    "        # 7) save NON-ZERO coefficients only\n",
    "        outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "        coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "        # robust: accept 'coef' or 'beta' column name\n",
    "        coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "        if coef_col is None:\n",
    "            raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "        # filter to non-zero (tolerance to avoid tiny numerical noise)\n",
    "        eps = 1e-12\n",
    "        mask_nz = coef_df[coef_col].astype(float).abs() > eps\n",
    "        coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "        coef_nz.to_csv(coef_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
