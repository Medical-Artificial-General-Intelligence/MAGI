{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.special import expit\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score, accuracy_score,\n",
    ")\n",
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "BASE = \"./testing_YL\"\n",
    "\n",
    "TARGET_NAME = {\n",
    "    \"aa_meas_sertraline_rem\": \"Sertraline\",\n",
    "    \"aa_meas_bupropion_rem\": \"Bupropion\",\n",
    "    \"aa_meas_trazodone_rem\": \"Trazodone\",\n",
    "    \"aa_meas_duloxetine_rem\": \"Duloxetine\",\n",
    "    \"aa_meas_escitalopram_rem\": \"Escitalopram\",\n",
    "    \"aa_meas_fluoxetine_rem\": \"Fluoxetine\",\n",
    "    \"aa_meas_citalopram_rem\": \"Citalopram\",\n",
    "    \"aa_meas_venlafaxine_rem\": \"Venlafaxine\",\n",
    "    \"aa_meas_amitriptyline_rem\": \"Amitriptyline\",\n",
    "    \"aa_meas_mirtazapine_rem\": \"Mirtazapine\",\n",
    "    \"aa_meas_paroxetine_rem\": \"Paroxetine\",\n",
    "    \"aa_meas_nortriptyline_rem\": \"Nortriptyline\",\n",
    "    \"aa_meas_other_rem\": \"Others\",\n",
    "    \"aa_meas_doxepin_rem\": \"Doxepin\",\n",
    "    \"aa_meas_desvenlafaxine_rem\": \"Desvenlafaxine\",\n",
    "}\n",
    "TARGETS = list(TARGET_NAME.keys())\n",
    "\n",
    "# Coefficient CSVs live here, one per target\n",
    "# Example filename: ./testing_YL/NIDA/redo/magi_coef_dx_SNOMED_15167005_byTE.csv\n",
    "#COEF_PATTERN = os.path.join(BASE, \"MAGI_LASSO/magi_batch_coef\", \"magi_coef_{target}.csv\")\n",
    "COEF_PATTERN = os.path.join(BASE, \"MAGI_LASSO/magi_batch_coef\", \"magi_coef_{target}_nonzero.csv\")\n",
    "\n",
    "# Output folders\n",
    "OUT_DIR      = os.path.join(BASE, \"MAGI_LASSO/magi_coef/prevalence\")\n",
    "PNG_DIR      = os.path.join(OUT_DIR, \"png\")\n",
    "CSV_DIR      = os.path.join(OUT_DIR, \"preds\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(PNG_DIR, exist_ok=True)\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "SUMMARY_CSV = os.path.join(OUT_DIR, \"summary_per_target_and_bootstrap.csv\")\n",
    "\n",
    "RNG_SEED = 42\n",
    "NEG_MULT = 4\n",
    "N_BOOT   = 100\n",
    "\n",
    "# selection ranking mode\n",
    "RANK_MODE = \"prevalence\"  # or \"beta\"\n",
    "\n",
    "# SAFE presence thresholds (mild/strict)\n",
    "SAFE_MIN_TOTAL_COUNT_EPV_5_9  = 2\n",
    "SAFE_MIN_POS_CARRIERS_EPV_5_9 = 1\n",
    "SAFE_MIN_TOTAL_COUNT_EPV_LT5  = 3\n",
    "SAFE_MIN_POS_CARRIERS_EPV_LT5 = 2\n",
    "# -------------------- EPV CONFIG --------------------\n",
    "EPV_TARGET = 9          # set 5 (current ask) or 9 (old behavior)\n",
    "EPV_MIN    = 5          # lower bound in SAFE band (typically 5)\n",
    "\n",
    "# ---- Bootstrap policy: run ONLY when EPV < 9 ----\n",
    "RUN_BOOTSTRAP_EPV_THRESHOLD = 9.0  # keep as a named constant (no magic numbers)\n",
    "\n",
    "# Choose SAFE presence thresholds by EPV regime\n",
    "SAFE_POLICY = \"mild\"  # (\"mild\" here just uses the table below; keep as-is unless you want a switch)\n",
    "def _safe_thresholds(epv_target: int):\n",
    "    # You already defined these constants above\n",
    "    if epv_target >= 5:\n",
    "        return SAFE_MIN_TOTAL_COUNT_EPV_5_9, SAFE_MIN_POS_CARRIERS_EPV_5_9\n",
    "    else:\n",
    "        return SAFE_MIN_TOTAL_COUNT_EPV_LT5,  SAFE_MIN_POS_CARRIERS_EPV_LT5\n",
    "\n",
    "MIN_TOTAL_COUNT, MIN_POS_CARRIERS = _safe_thresholds(EPV_TARGET)\n",
    "\n",
    "# -------------------- UTILS --------------------\n",
    "def banner(txt):\n",
    "    bar = \"=\" * max(12, len(txt) + 4)\n",
    "    print(f\"\\n{bar}\\n{txt}\\n{bar}\")\n",
    "\n",
    "def subhead(txt):\n",
    "    print(f\"\\n--- {txt} ---\")\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in (\"-\", \"_\") else \"_\" for ch in s)\n",
    "\n",
    "def plot_roc(y_true, p_hat, title, out_png, out_svg):\n",
    "    fpr, tpr, _ = roc_curve(y_true, p_hat)\n",
    "    auc_val = roc_auc_score(y_true, p_hat)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc_val:.3f}\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\", linewidth=1)\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.ylabel(\"True Positive Rate\",  fontsize=16, fontweight=\"bold\")\n",
    "    plt.xticks(fontsize=14, fontweight=\"bold\")\n",
    "    plt.yticks(fontsize=14, fontweight=\"bold\")\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.4)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(out_svg, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return float(auc_val)\n",
    "\n",
    "def load_magi_betas(coef_csv):\n",
    "    df = pd.read_csv(coef_csv)\n",
    "    def pick(df, opts):\n",
    "        for c in opts:\n",
    "            if c in df.columns: return c\n",
    "        raise KeyError(f\"Missing any of {opts} in {coef_csv}. Found: {list(df.columns)}\")\n",
    "    code_col = pick(df, [\"concept_code\",\"standard_concept_code\",\"predictor\",\"feature\",\"term\",\"name\"])\n",
    "    beta_col = pick(df, [\"coef\",\"coefficient\",\"beta\",\"estimate\",\"b\",\"value\"])\n",
    "    df[code_col] = df[code_col].astype(str).str.strip()\n",
    "    is_int = df[code_col].str.lower().isin([\"(intercept)\",\"intercept\",\"const\",\"(const)\",\"bias\"])\n",
    "    intercept = float(df.loc[is_int, beta_col].iloc[0]) if is_int.any() else 0.0\n",
    "    coef_map  = dict(zip(df.loc[~is_int, code_col], df.loc[~is_int, beta_col].astype(float)))\n",
    "    return intercept, coef_map\n",
    "\n",
    "# Fixed-sample policy:\n",
    "# - if #pos < 1000: take all pos, take 4x neg (cap by available)\n",
    "# - else:           take 1000 pos, take 4000 neg (cap by available)\n",
    "N_POS_TARGET = 1000\n",
    "N_NEG_TARGET = 4000\n",
    "\n",
    "def sample_pos_1k_else_all_and_4xneg(y, seed=42, *, n_pos_target=1000, n_neg_target=4000):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    neg_idx = np.where(y == 0)[0]\n",
    "\n",
    "    if pos_idx.size == 0 or neg_idx.size == 0:\n",
    "        raise ValueError(\"Sampling failed: one of the classes has zero members.\")\n",
    "\n",
    "    if pos_idx.size < n_pos_target:\n",
    "        take_pos = pos_idx.size\n",
    "        take_neg = min(4 * take_pos, neg_idx.size)\n",
    "        # sample without replacement\n",
    "        sel_pos = pos_idx  # all positives\n",
    "        sel_neg = rng.choice(neg_idx, size=take_neg, replace=False)\n",
    "        print(f\"[SAMPLE] pos<{n_pos_target}: using ALL pos={take_pos}, neg={take_neg} (4×, capped by available).\")\n",
    "    else:\n",
    "        take_pos = n_pos_target\n",
    "        take_neg = min(n_neg_target, neg_idx.size)\n",
    "        sel_pos = rng.choice(pos_idx, size=take_pos, replace=False)\n",
    "        sel_neg = rng.choice(neg_idx, size=take_neg, replace=False)\n",
    "        print(f\"[SAMPLE] pos≥{n_pos_target}: pos={take_pos}, neg={take_neg} (target 4000, capped by available).\")\n",
    "\n",
    "    sel = np.concatenate([sel_pos, sel_neg])\n",
    "    rng.shuffle(sel)\n",
    "    return sel\n",
    "  \n",
    "\n",
    "def drop_constants_and_duplicates_for_sample(\n",
    "    X_csr, feature_codes, *,\n",
    "    remove_all_zero: bool = True,\n",
    "    remove_duplicates: bool = True,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Optionally drop (a) all-zero columns and (b) duplicate columns (identical index+data).\n",
    "    Returns: (X_reduced, feats_reduced, keep_mask_final)\n",
    "    \"\"\"\n",
    "    n_rows, n_cols = X_csr.shape\n",
    "    if n_cols == 0:\n",
    "        return X_csr, feature_codes, np.zeros(0, dtype=bool)\n",
    "\n",
    "    # Start with 'keep all'\n",
    "    keep_mask = np.ones(n_cols, dtype=bool)\n",
    "\n",
    "    # A) Remove all-zero columns (optional)\n",
    "    if remove_all_zero:\n",
    "        nnz = np.asarray((X_csr != 0).sum(axis=0)).ravel()\n",
    "        keep_mask &= (nnz > 0)\n",
    "\n",
    "    # Short-circuit if nothing to dedup\n",
    "    if not remove_duplicates:\n",
    "        X_red = X_csr[:, keep_mask]\n",
    "        feats_red = feature_codes[keep_mask]\n",
    "        dropped = int(n_cols - keep_mask.sum())\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Cleanup: kept {X_red.shape[1]:,}/{n_cols:,} \"\n",
    "                  f\"(dropped {dropped:,}{' zero' if remove_all_zero else ''}).\")\n",
    "        # Map back to original-column boolean mask\n",
    "        keep_mask_final = keep_mask\n",
    "        return X_red, feats_red, keep_mask_final\n",
    "\n",
    "    # B) Deduplicate among the *kept* columns\n",
    "    X_csc = X_csr[:, keep_mask].tocsc()\n",
    "    sub_keep = np.ones(X_csc.shape[1], dtype=bool)\n",
    "    seen = {}\n",
    "    for j in range(X_csc.shape[1]):\n",
    "        s, e = X_csc.indptr[j], X_csc.indptr[j+1]\n",
    "        key = (X_csc.indices[s:e].tobytes(), X_csc.data[s:e].tobytes())\n",
    "        if key in seen:\n",
    "            sub_keep[j] = False\n",
    "        else:\n",
    "            seen[key] = j\n",
    "\n",
    "    keep_idx = np.where(keep_mask)[0]\n",
    "    keep_mask_final = np.zeros(n_cols, dtype=bool)\n",
    "    keep_mask_final[keep_idx[sub_keep]] = True\n",
    "    X_red = X_csr[:, keep_mask_final]\n",
    "    feats_red = feature_codes[keep_mask_final]\n",
    "    dropped = int(n_cols - keep_mask_final.sum())\n",
    "    if verbose:\n",
    "        dropped_zero = (keep_mask.sum() - sub_keep.sum()) if remove_all_zero else 0\n",
    "        print(f\"[INFO] Cleanup: kept {X_red.shape[1]:,}/{n_cols:,} \"\n",
    "              f\"(dropped {dropped:,}{' incl. zeros/dups' if remove_all_zero else ' (dups)'}).\")\n",
    "    return X_red, feats_red, keep_mask_final\n",
    "\n",
    "def compute_epv(n_pos: int, n_predictors: int) -> float:\n",
    "    if n_predictors <= 0:\n",
    "        return float('inf') if n_pos > 0 else 0.0\n",
    "    return float(n_pos) / float(n_predictors)\n",
    "\n",
    "def brier_disagreement_sum(p1, p2):\n",
    "    \"\"\"Sum_j (MAGI_j - LASSO_j)^2  — matches the attached formula.\"\"\"\n",
    "    p1 = np.asarray(p1, dtype=float).ravel()\n",
    "    p2 = np.asarray(p2, dtype=float).ravel()\n",
    "    return float(np.sum((p1 - p2) ** 2))\n",
    "\n",
    "def brier_disagreement_mean(p1, p2):\n",
    "    \"\"\"Average (1/k) * Sum_j (MAGI_j - LASSO_j)^2.\"\"\"\n",
    "    p1 = np.asarray(p1, dtype=float).ravel()\n",
    "    p2 = np.asarray(p2, dtype=float).ravel()\n",
    "    return float(np.mean((p1 - p2) ** 2))\n",
    "\n",
    "def bootstrap_ci_disagreement_mean(p1, p2, n_splits=1000, alpha=0.05, seed=42):\n",
    "    \"\"\"Percentile CI for the mean disagreement metric.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    p1 = np.asarray(p1, dtype=float).ravel()\n",
    "    p2 = np.asarray(p2, dtype=float).ravel()\n",
    "    n  = p1.size\n",
    "    samples = []\n",
    "    for _ in range(n_splits):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        samples.append(float(np.mean((p1[idx] - p2[idx]) ** 2)))\n",
    "    arr = np.asarray(samples, dtype=float)\n",
    "    mean = float(arr.mean())\n",
    "    std  = float(arr.std(ddof=1))\n",
    "    lo   = float(np.percentile(arr, 100 * (alpha/2)))\n",
    "    hi   = float(np.percentile(arr, 100 * (1 - alpha/2)))\n",
    "    return mean, std, lo, hi\n",
    "\n",
    "# ------------ Selection: EPV-target=9 with SAFE to force EPV∈[5,9] ------------\n",
    "def select_and_trim_to_epv_range(\n",
    "    X_train, y_train, feature_codes, coef_map, *,\n",
    "    dv_code,\n",
    "    rank=RANK_MODE,           # default now: prevalence\n",
    "    prevalence_mode=\"overall\",   # NEW: \"overall\" or \"pos\"\n",
    "    require_present=False,       # keep off for MAGI\n",
    "    min_total_count=1,\n",
    "    min_pos_carriers=0,\n",
    "    epv_target=9,\n",
    "    epv_min=5,\n",
    "    cleanup_zero: bool = False,  # MAGI: keep zeros\n",
    "    cleanup_dups: bool = True,   # we DO dedup first per your spec\n",
    "    enforce_exact_target: bool = True,\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "    y_train = np.asarray(y_train).ravel().astype(int)\n",
    "    n_pos = int(np.sum(y_train == 1))\n",
    "    if n_pos == 0:\n",
    "        if verbose: print(\"[SEL] No positives; empty selection.\")\n",
    "        return X_train[:, :0], feature_codes[:0], np.array([], dtype=int), np.inf\n",
    "\n",
    "    # Target counts\n",
    "    n_min = max(1, int(math.floor(n_pos / float(epv_target))))   # exact target\n",
    "    n_max = max(n_min, int(math.floor(n_pos / float(epv_min))))  # hard cap (EPV ≥ epv_min)\n",
    "\n",
    "    # Map & DV exclusion\n",
    "    mapped_mask = np.array([c in coef_map for c in feature_codes], dtype=bool)\n",
    "    if dv_code is not None:\n",
    "        mapped_mask &= (feature_codes != dv_code)\n",
    "    cand_idx_all = np.where(mapped_mask)[0]\n",
    "    if cand_idx_all.size == 0:\n",
    "        if verbose: print(\"[SEL] No MAGI-mapped features (after DV exclusion).\")\n",
    "        return X_train[:, :0], feature_codes[:0], np.array([], dtype=int), np.inf\n",
    "\n",
    "    # Optional presence filters (keep OFF for MAGI)\n",
    "    if require_present and (min_total_count > 1 or min_pos_carriers > 0):\n",
    "        X_cand = X_train[:, cand_idx_all]\n",
    "        nnz_all = np.asarray((X_cand != 0).sum(axis=0)).ravel()\n",
    "        keep_local = (nnz_all >= max(1, min_total_count))\n",
    "        if min_pos_carriers > 0 and np.any(y_train == 1):\n",
    "            pos_rows = np.where(y_train == 1)[0]\n",
    "            nnz_pos = np.asarray((X_cand[pos_rows, :] != 0).sum(axis=0)).ravel()\n",
    "            keep_local &= (nnz_pos >= min_pos_carriers)\n",
    "        cand_idx_all = cand_idx_all[keep_local]\n",
    "        if cand_idx_all.size == 0:\n",
    "            if verbose: print(\"[SEL] After presence filters, no features remain.\")\n",
    "            return X_train[:, :0], feature_codes[:0], np.array([], dtype=int), np.inf\n",
    "\n",
    "    # === DEDUP FIRST (global on candidates), NO zero-drop for MAGI ===\n",
    "    X_cand_all = X_train[:, cand_idx_all]\n",
    "    feats_cand_all = feature_codes[cand_idx_all]\n",
    "    X_dedup, feats_dedup, mask_dedup = drop_constants_and_duplicates_for_sample(\n",
    "        X_cand_all.tocsr().astype(np.float32),\n",
    "        feats_cand_all,\n",
    "        remove_all_zero=False,     # keep zeros for MAGI\n",
    "        remove_duplicates=False,    # no dedup\n",
    "        verbose=False\n",
    "    )\n",
    "    cand_idx = cand_idx_all[mask_dedup]\n",
    "    dropped_dups = int(cand_idx_all.size - cand_idx.size)\n",
    "\n",
    "    if cand_idx.size == 0:\n",
    "        if verbose: print(\"[SEL] All candidate columns were duplicates; nothing left.\")\n",
    "        return X_train[:, :0], feature_codes[:0], np.array([], dtype=int), np.inf\n",
    "\n",
    "    # === Ranking ===\n",
    "    if rank == \"prevalence\":\n",
    "        if prevalence_mode == \"pos\":\n",
    "            pos_rows = np.where(y_train == 1)[0]\n",
    "            counts = np.asarray((X_train[pos_rows, :][:, cand_idx] != 0).sum(axis=0)).ravel()\n",
    "        else:  # \"overall\"\n",
    "            counts = np.asarray((X_train[:, cand_idx] != 0).sum(axis=0)).ravel()\n",
    "\n",
    "        # Tie-break by |beta| (stable & useful)\n",
    "        betas_for_cand = np.array([coef_map[feature_codes[i]] for i in cand_idx], dtype=float)\n",
    "        # np.lexsort uses last key as primary; we want primary = counts(desc), secondary = |beta|(desc)\n",
    "        order_local = np.lexsort((-np.abs(betas_for_cand), -counts))\n",
    "        ordered_idx = cand_idx[order_local]\n",
    "\n",
    "    elif rank == \"beta\":\n",
    "        betas = np.array([coef_map[feature_codes[i]] for i in cand_idx], dtype=float)\n",
    "        order_local = np.argsort(-np.abs(betas))\n",
    "        ordered_idx = cand_idx[order_local]\n",
    "    else:\n",
    "        raise ValueError(\"rank must be 'prevalence' or 'beta'\")\n",
    "\n",
    "    # === Take top n_min (cap by availability and epv_min) ===\n",
    "    take_cap = min(n_max, ordered_idx.size)\n",
    "    chosen_idx = ordered_idx[:min(n_min, take_cap)]\n",
    "\n",
    "    # === Drop columns identical to y (perfect proxies), then top-up from tail ===\n",
    "    def drop_y_identical(Xm, idxs):\n",
    "        nonlocal y_train\n",
    "        if Xm.shape[1] == 0:\n",
    "            return idxs, 0\n",
    "        yv = y_train.astype(np.int8)\n",
    "        drop = []\n",
    "        for j in range(Xm.shape[1]):\n",
    "            col = Xm.getcol(j).toarray().ravel().astype(np.int8)\n",
    "            if np.array_equal(col, yv):\n",
    "                drop.append(j)\n",
    "        if not drop:\n",
    "            return idxs, 0\n",
    "        keep_mask = np.ones(Xm.shape[1], dtype=bool)\n",
    "        keep_mask[np.array(drop)] = False\n",
    "        return idxs[keep_mask], len(drop)\n",
    "\n",
    "    X_top = X_train[:, chosen_idx]\n",
    "    chosen_idx, n_yident = drop_y_identical(X_top, chosen_idx)\n",
    "\n",
    "    # Top-up from the remainder until we hit n_min (or run out / hit cap)\n",
    "    cursor = len(chosen_idx)\n",
    "    tail = ordered_idx[len(chosen_idx):take_cap]\n",
    "    k_target = min(n_min, take_cap)\n",
    "\n",
    "    tpos = 0\n",
    "    while chosen_idx.size < k_target and tpos < tail.size:\n",
    "        add_idx = tail[tpos]\n",
    "        tpos += 1\n",
    "        # candidate set + add one\n",
    "        tmp = np.concatenate([chosen_idx, [add_idx]])\n",
    "        X_tmp = X_train[:, tmp]\n",
    "        tmp2, dropped = drop_y_identical(X_tmp, tmp)\n",
    "        if tmp2.size > chosen_idx.size:\n",
    "            chosen_idx = tmp2  # accepted; note: if new col equals y it won't increase\n",
    "\n",
    "    # Enforce exact n_min if possible\n",
    "    if enforce_exact_target and chosen_idx.size >= n_min:\n",
    "        chosen_idx = chosen_idx[:n_min]\n",
    "\n",
    "    # Final slice\n",
    "    if chosen_idx.size == 0:\n",
    "        return X_train[:, :0], feature_codes[:0], np.array([], dtype=int), np.inf\n",
    "\n",
    "    X_used = X_train[:, chosen_idx]\n",
    "    feats_used = feature_codes[chosen_idx]\n",
    "    p_final = int(X_used.shape[1])\n",
    "    epv_final = (float(n_pos) / p_final) if p_final > 0 else np.inf\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] n_pos={n_pos}  n_min@{epv_target}={n_min}  n_max@{epv_min}={n_max}  \"\n",
    "              f\"avail={cand_idx_all.size}  kept={p_final}  EPV={epv_final:.3f}\")\n",
    "        if dropped_dups:\n",
    "            print(f\"[SEL] Dedup removed {dropped_dups} columns before ranking.\")\n",
    "        if n_yident:\n",
    "            print(f\"[SEL] Dropped {n_yident} columns identical to y.\")\n",
    "\n",
    "    return X_used, feats_used, chosen_idx.astype(int, copy=False), epv_final\n",
    "\n",
    "# -------------------- Bootstrap helpers --------------------\n",
    "def bootstrap_632plus_or_fallback(estimator, X, y, n_splits, scoring_func,\n",
    "                                  predict_proba, seed, clone_estimator=False):\n",
    "    try:\n",
    "        return bootstrap_point632_score(\n",
    "            estimator=estimator, X=X, y=y,\n",
    "            n_splits=n_splits, method=\".632+\",\n",
    "            scoring_func=scoring_func, predict_proba=predict_proba,\n",
    "            random_seed=seed, clone_estimator=clone_estimator\n",
    "        )\n",
    "    except ZeroDivisionError:\n",
    "        print(\"[WARN] .632+ failed (division by zero). Falling back to .632.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] .632+ failed ({e}). Falling back to .632.\")\n",
    "    return bootstrap_point632_score(\n",
    "        estimator=estimator, X=X, y=y,\n",
    "        n_splits=n_splits, method=\".632\",\n",
    "        scoring_func=scoring_func, predict_proba=predict_proba,\n",
    "        random_seed=seed, clone_estimator=clone_estimator\n",
    "    )\n",
    "\n",
    "# -------------------- Estimators (per-resample selection with EPV rules) --------------------\n",
    "class MagiBootstrapEstimatorCurrent(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, intercept, coef_map, feature_codes_all,\n",
    "                 dv_code,                      # <-- add this\n",
    "                 rank=RANK_MODE, n_frac=0.10, require_present=True,\n",
    "                 min_total_count=1, min_pos_carriers=0, verbose=False):\n",
    "        self.intercept = intercept\n",
    "        self.coef_map = coef_map\n",
    "        self.feature_codes_all = feature_codes_all\n",
    "        self.dv_code = dv_code               # <-- store it\n",
    "        self.rank = rank\n",
    "        self.n_frac = n_frac\n",
    "        self.require_present = require_present\n",
    "        self.min_total_count = min_total_count\n",
    "        self.min_pos_carriers = min_pos_carriers\n",
    "        self.verbose = verbose\n",
    "        self._feats_used = None\n",
    "        self._betas_used = None\n",
    "        self._feature_codes_all = None\n",
    "        self._intercept_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        feature_codes_all = np.asarray(self.feature_codes_all)\n",
    "        coef_map = dict(self.coef_map)\n",
    "        intercept = float(self.intercept)\n",
    "\n",
    "        # selection with DV removed + EPV target 9, SAFE [5,9]\n",
    "        X_used, feats_used, _, _ = select_and_trim_to_epv_range(\n",
    "            X, y, feature_codes_all, coef_map,\n",
    "            dv_code=self.dv_code,                 # <-- pass dv_code\n",
    "            rank=self.rank, prevalence_mode=\"overall\", require_present=self.require_present,\n",
    "            min_total_count=MIN_TOTAL_COUNT, min_pos_carriers=MIN_POS_CARRIERS,\n",
    "            epv_target=EPV_TARGET, epv_min=EPV_MIN,              \n",
    "            verbose=self.verbose,\n",
    "            cleanup_zero=False,       # <<< MAGI: keep all-zero columns\n",
    "            cleanup_dups=False,        # (safe to keep this to avoid double counting)\n",
    "            enforce_exact_target=True,\n",
    "        )\n",
    "        assert self.dv_code not in set(feats_used), \"DV leaked into predictors! (MAGI)\"\n",
    "\n",
    "        if X_used.shape[1] == 0:\n",
    "            self._feats_used = np.array([], dtype=str)\n",
    "            self._betas_used = np.array([], dtype=float)\n",
    "            self._feature_codes_all = feature_codes_all\n",
    "            self._intercept_ = intercept\n",
    "            return self\n",
    "\n",
    "        self._feats_used = feats_used\n",
    "        self._betas_used = np.array([coef_map.get(c, 0.0) for c in feats_used], dtype=float)\n",
    "        self._feature_codes_all = feature_codes_all\n",
    "        self._intercept_ = intercept\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self._feats_used is None or self._feats_used.size == 0:\n",
    "            p = float(expit(self._intercept_))\n",
    "            return np.c_[1.0 - np.full(X.shape[0], p), np.full(X.shape[0], p)]\n",
    "        code_to_idx = {c: i for i, c in enumerate(self._feature_codes_all)}\n",
    "        col_idx = np.array([code_to_idx[c] for c in self._feats_used], dtype=int)\n",
    "        z = self._intercept_ + X[:, col_idx].dot(self._betas_used)\n",
    "        p = expit(np.asarray(z).ravel())\n",
    "        return np.c_[1.0 - p, p]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "class LassoBootstrapEstimatorCurrent(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, feature_codes_all, coef_map,\n",
    "                 dv_code,                      # <-- add this\n",
    "                 rank=RANK_MODE, n_frac=0.10, require_present=True,\n",
    "                 min_total_count=1, min_pos_carriers=0,\n",
    "                 C=0.5, max_iter=2000, n_jobs=-1, random_state=42,\n",
    "                 verbose=False):\n",
    "        self.feature_codes_all = feature_codes_all\n",
    "        self.coef_map = coef_map\n",
    "        self.dv_code = dv_code               # <-- store it\n",
    "        self.rank = rank\n",
    "        self.n_frac = n_frac\n",
    "        self.require_present = require_present\n",
    "        self.min_total_count = min_total_count\n",
    "        self.min_pos_carriers = min_pos_carriers\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self._feats_used = None\n",
    "        self._feature_codes_all = None\n",
    "        self._model = None\n",
    "        self._p_const = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        feature_codes_all = np.asarray(self.feature_codes_all)\n",
    "        coef_map = dict(self.coef_map)\n",
    "\n",
    "        # selection with DV removed + EPV target 9, SAFE [5,9]\n",
    "        X_used, feats_used, _, _ = select_and_trim_to_epv_range(\n",
    "            X, y, feature_codes_all, coef_map,\n",
    "            dv_code=self.dv_code,                 # <-- pass dv_code\n",
    "            rank=self.rank, prevalence_mode=\"overall\", require_present=self.require_present,\n",
    "            epv_target=EPV_TARGET, epv_min=EPV_MIN,\n",
    "            min_total_count=MIN_TOTAL_COUNT, min_pos_carriers=MIN_POS_CARRIERS,             \n",
    "            verbose=self.verbose,\n",
    "            cleanup_zero=True,        # <<< LASSO: drop all-zero\n",
    "            cleanup_dups=False,     \n",
    "            enforce_exact_target=True, # keep feature count aligned with MAGI\n",
    "        )\n",
    "        assert self.dv_code not in set(feats_used), \"DV leaked into predictors! (LASSO)\"\n",
    "\n",
    "        if X_used.shape[1] == 0:\n",
    "            self._feats_used = np.array([], dtype=str)\n",
    "            self._feature_codes_all = feature_codes_all\n",
    "            self._model = None\n",
    "            self._p_const = float(np.mean(y))\n",
    "            return self\n",
    "\n",
    "        self._feats_used = feats_used\n",
    "        self._feature_codes_all = feature_codes_all\n",
    "\n",
    "        self._model = LogisticRegression(\n",
    "            penalty=\"l1\", solver=\"saga\", C=self.C,\n",
    "            max_iter=self.max_iter, n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state\n",
    "        ).fit(X_used, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self._feats_used is None or self._feats_used.size == 0 or self._model is None:\n",
    "            p = np.full(X.shape[0], self._p_const if self._p_const is not None else 0.5, dtype=float)\n",
    "            return np.c_[1.0 - p, p]\n",
    "        code_to_idx = {c: i for i, c in enumerate(self._feature_codes_all)}\n",
    "        col_idx = np.array([code_to_idx[c] for c in self._feats_used], dtype=int)\n",
    "        p = self._model.predict_proba(X[:, col_idx])[:, 1]\n",
    "        return np.c_[1.0 - p, p]\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self._feats_used is None or self._feats_used.size == 0 or self._model is None:\n",
    "            p = self._p_const if self._p_const is not None else 0.5\n",
    "            return (np.full(X.shape[0], p) >= 0.5).astype(int)\n",
    "        code_to_idx = {c: i for i, c in enumerate(self._feature_codes_all)}\n",
    "        col_idx = np.array([code_to_idx[c] for c in self._feats_used], dtype=int)\n",
    "        proba = self._model.predict_proba(X[:, col_idx])[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "# -------------------- LOAD DESIGN --------------------\n",
    "banner(\"LOAD DESIGN\")\n",
    "X_full  = load_npz(f\"{BASE}/Lasso_X.npz\").tocsr().astype(np.float32)\n",
    "persons = pd.read_csv(f\"{BASE}/person_index.csv\")[\"person_id\"].astype(str).to_numpy()\n",
    "codes   = pd.read_csv(f\"{BASE}/code_index.csv\")[\"concept_code\"].astype(str).to_numpy()\n",
    "print(f\"[INFO] Matrix: persons={X_full.shape[0]:,}  codes={X_full.shape[1]:,}\")\n",
    "if len(persons) != X_full.shape[0] or len(codes) != X_full.shape[1]:\n",
    "    raise ValueError(\"[ERROR] person/code indices do not match matrix shape.\")\n",
    "\n",
    "# -------------------- RUN PER TARGET --------------------\n",
    "rows = []\n",
    "for tcode, pretty in TARGET_NAME.items():\n",
    "    safe = safe_name(pretty)\n",
    "    banner(f\"TARGET {tcode} — {pretty}\")\n",
    "\n",
    "    # A) Labels\n",
    "    idx_y = np.where(codes == tcode)[0]\n",
    "    if idx_y.size == 0:\n",
    "        print(f\"[SKIP] Target not found in code_index.csv → {tcode}\")\n",
    "        continue\n",
    "    y_full = X_full[:, idx_y[0]].toarray().ravel().astype(np.int8)\n",
    "\n",
    "    # B) Predictors (exclude DV)\n",
    "    mask_pred = (codes != tcode)\n",
    "    X = X_full[:, mask_pred]\n",
    "    feature_codes = codes[mask_pred]\n",
    "\n",
    "    # C) Sample: include all cases + 4× controls\n",
    "    sel = sample_pos_1k_else_all_and_4xneg(y_full, seed=RNG_SEED,\n",
    "                                           n_pos_target=N_POS_TARGET, n_neg_target=N_NEG_TARGET)\n",
    "    X_sub = X[sel, :].tocsr().astype(np.float32)\n",
    "    y_sub = y_full[sel].astype(int).ravel()\n",
    "    persons_sub = persons[sel]\n",
    "\n",
    "    n_rows = X_sub.shape[0]\n",
    "    n_pos  = int(y_sub.sum())\n",
    "    n_neg  = int(n_rows - n_pos)\n",
    "    #print(f\"[INFO] subset: n={n_rows:,}  pos={n_pos:,}  neg={n_neg:,}  baseline={y_sub.mean():.4f}\")\n",
    "\n",
    "    # D) Load MAGI coefs\n",
    "    coef_csv = COEF_PATTERN.format(target=tcode)\n",
    "    if not os.path.exists(coef_csv):\n",
    "        print(f\"[SKIP] Missing MAGI coef file: {coef_csv}\")\n",
    "        continue\n",
    "    intercept, coef_map = load_magi_betas(coef_csv)\n",
    "    print(f\"[MAGI INFO] MAGI coefs: intercept={intercept:.6f}  n_features={len(coef_map):,}\")\n",
    "\n",
    "    # E) Selection per NEW EPV rules\n",
    "    #    MAGI: keep exactly floor(n_pos/9) ranked features; DO NOT drop always-zero columns.\n",
    "    X_used, feats_used, _, epv = select_and_trim_to_epv_range(\n",
    "        X_sub, y_sub, feature_codes, coef_map, dv_code=tcode,\n",
    "        rank=RANK_MODE, require_present=False,\n",
    "        min_total_count=1, min_pos_carriers=0,\n",
    "        epv_target=EPV_TARGET, epv_min=EPV_MIN,\n",
    "        cleanup_zero=False,    # keep zero-prevalence for MAGI\n",
    "        cleanup_dups=False,    # no dedup to avoid double counting\n",
    "        enforce_exact_target=True,   # <<< force exactly floor(n_pos/9)\n",
    "        verbose=True\n",
    "    )\n",
    "    print(f\"[MAGI INFO] (MAGI selection) kept={X_used.shape[1]}  EPV={epv:.3f}\")\n",
    "    assert tcode not in set(feats_used), \"DV leaked into predictors!\"\n",
    "\n",
    "    # MAGI probabilities on the MAGI-selected columns (zeros allowed)\n",
    "    if X_used.shape[1] == 0:\n",
    "        p_hat_magi = np.full(n_rows, float(expit(intercept)), dtype=float)\n",
    "        magi_nz_feats = np.array([], dtype=str)\n",
    "        magi_nz_betas = np.array([], dtype=float)\n",
    "    else:\n",
    "        betas_vec = np.array([coef_map.get(f, 0.0) for f in feats_used], dtype=float)\n",
    "        lp = intercept + X_used.dot(betas_vec)\n",
    "        p_hat_magi = expit(np.asarray(lp).ravel())\n",
    "        nz_mask = (betas_vec != 0)\n",
    "        magi_nz_feats = np.array(feats_used)[nz_mask]\n",
    "        magi_nz_betas = betas_vec[nz_mask]\n",
    "    print(f\"[MAGI] Selected features: {len(feats_used)}  |  non-zero β: {magi_nz_feats.size}\")\n",
    "    auc_magi = roc_auc_score(y_sub, p_hat_magi)\n",
    "    print(f\"[RESULT] MAGI  AUC={auc_magi:.4f}  baseline={y_sub.mean():.4f}\")\n",
    "\n",
    "    # --- Derive LASSO design from the SAME features, but drop all-zero (and dups) before fitting\n",
    "    if X_used.shape[1] == 0:\n",
    "        X_used_lasso = X_used\n",
    "        feats_used_lasso = feats_used\n",
    "    else:\n",
    "        X_used_lasso, feats_used_lasso, _ = drop_constants_and_duplicates_for_sample(\n",
    "            X_used.tocsr().astype(np.float32), np.asarray(feats_used),\n",
    "            remove_all_zero=True,   # <<< key change: remove zero-columns for LASSO\n",
    "            remove_duplicates=False, # no dedup\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    # Save ALL selected MAGI features (including zeros), plus intercept\n",
    "    magi_all_df = pd.DataFrame({\n",
    "        \"feature\": feats_used,\n",
    "        \"beta\":    [coef_map.get(f, 0.0) for f in feats_used]\n",
    "    }).sort_values(\"beta\", key=np.abs, ascending=False)\n",
    "\n",
    "    magi_all_df.loc[-1] = {\"feature\": \"(intercept)\", \"beta\": float(intercept)}\n",
    "    magi_all_df.index = magi_all_df.index + 1\n",
    "\n",
    "    magi_used_csv = os.path.join(OUT_DIR, f\"magi_coef_used_{safe}.csv\")\n",
    "    magi_all_df.to_csv(magi_used_csv, index=False)\n",
    "\n",
    "\n",
    "    # MAGI preds & ROC\n",
    "    pred_csv_magi = os.path.join(CSV_DIR, f\"pred_{safe}.csv\")\n",
    "    pd.DataFrame({\"person_id\": persons_sub, \"y_true\": y_sub.astype(int), \"prob_magi\": p_hat_magi}).to_csv(pred_csv_magi, index=False)\n",
    "    png_path = os.path.abspath(os.path.join(PNG_DIR, f\"ROC_{safe}.png\"))\n",
    "    svg_path = os.path.abspath(os.path.join(PNG_DIR, f\"ROC_{safe}.svg\"))\n",
    "    plot_roc(y_sub, p_hat_magi, pretty, png_path, svg_path)\n",
    "    \n",
    "    # ---- LASSO on SAME features (cleaned for zeros/dups) ----\n",
    "    try:\n",
    "        if X_used_lasso.shape[1] == 0:\n",
    "            print(\"[SKIP] No usable features; LASSO fallback to constant.\")\n",
    "            lasso_cv_auc = np.nan; lasso_auc = np.nan\n",
    "            p_hat_lasso = np.full(n_rows, float(y_sub.mean()), dtype=float)\n",
    "            lasso_pred_csv = \"\"; lasso_coef_csv = \"\"\n",
    "        else:\n",
    "            clf = LogisticRegressionCV(\n",
    "                Cs=np.logspace(-3, 3, 12),\n",
    "                cv=5,\n",
    "                penalty=\"l1\",\n",
    "                solver=\"saga\",\n",
    "                scoring=\"roc_auc\",\n",
    "                max_iter=2000,\n",
    "                n_jobs=-1,\n",
    "                random_state=RNG_SEED,\n",
    "                refit=True,\n",
    "                fit_intercept=True,\n",
    "            ).fit(X_used_lasso, y_sub)\n",
    "\n",
    "             # Robustly get the CV score matrix and select best C\n",
    "            scores_store = clf.scores_\n",
    "            if isinstance(scores_store, dict):\n",
    "                # LogisticRegressionCV keys are the class labels; pick the positive class if present\n",
    "                # otherwise take the first key consistently\n",
    "                pos_key = 1 if 1 in scores_store else next(iter(scores_store.keys()))\n",
    "                scores_mat = np.asarray(scores_store[pos_key])\n",
    "            else:\n",
    "                scores_mat = np.asarray(scores_store)\n",
    "\n",
    "            # mean AUC across folds for each C candidate\n",
    "            mean_auc_per_C = np.mean(scores_mat, axis=0)\n",
    "\n",
    "            best_idx = int(np.argmax(mean_auc_per_C))\n",
    "            lasso_cv_auc = float(mean_auc_per_C[best_idx])\n",
    "            best_C = float(np.atleast_1d(clf.C_)[0])\n",
    "            print(f\"[MODEL] LASSO best_C={best_C:.6g}  CV-AUC={lasso_cv_auc:.4f}\")\n",
    "\n",
    "            p_hat_lasso = clf.predict_proba(X_used_lasso)[:, 1]\n",
    "            lasso_auc   = roc_auc_score(y_sub, p_hat_lasso)\n",
    "            print(f\"[RESULT] LASSO AUC={lasso_auc:.4f}  baseline={y_sub.mean():.4f}\")\n",
    "\n",
    "            # Save preds / coefs\n",
    "            lasso_pred_csv = os.path.join(CSV_DIR, f\"pred_{safe}_LASSO.csv\")\n",
    "            pd.DataFrame({\"person_id\": persons_sub, \"y_true\": y_sub.astype(int), \"prob_lasso\": p_hat_lasso}).to_csv(lasso_pred_csv, index=False)\n",
    "\n",
    "            coef = clf.coef_.ravel(); intercept_l = float(clf.intercept_.ravel()[0])\n",
    "            nz = np.where(coef != 0)[0]\n",
    "            coef_df = (pd.DataFrame({\"feature\": np.array(feats_used_lasso)[nz], \"coef\": coef[nz]})\n",
    "                       .sort_values(\"coef\", key=np.abs, ascending=False))\n",
    "            coef_df.loc[-1] = {\"feature\": \"(intercept)\", \"coef\": intercept_l}\n",
    "            coef_df.index = coef_df.index + 1\n",
    "            lasso_coef_csv = os.path.join(OUT_DIR, f\"lasso_coef_{safe}.csv\")\n",
    "            coef_df.to_csv(lasso_coef_csv, index=False)\n",
    "            print(f\"[LASSO] # Non-zero features: {nz.size}\")\n",
    "            if nz.size:\n",
    "                rows_print = [(str(feats_used_lasso[i]), float(coef[i])) for i in nz]\n",
    "                rows_print.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "                print(\"[LASSO] Non-zero features (sorted by |coef|):\")\n",
    "                for name, val in rows_print:\n",
    "                    print(f\"   {name:>40}  {val:+.6f}\")\n",
    "            else:\n",
    "                print(\"[LASSO] No non-zero coefficients (all zero)\")\n",
    "            print(f\"[LASSO] Intercept: {intercept_l:+.6f}\")\n",
    "\n",
    "            lasso_roc_png = os.path.abspath(os.path.join(PNG_DIR, f\"ROC_{safe}_LASSO.png\"))\n",
    "            lasso_roc_svg = os.path.abspath(os.path.join(PNG_DIR, f\"ROC_{safe}_LASSO.svg\"))\n",
    "            plot_roc(y_sub, p_hat_lasso, f\"{pretty} (LASSO, CV=5)\", lasso_roc_png, lasso_roc_svg)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] LASSO failed: {e}\")\n",
    "        lasso_cv_auc = np.nan; lasso_auc = np.nan\n",
    "        lasso_pred_csv = \"\"; lasso_coef_csv = \"\"\n",
    "        p_hat_lasso = np.full(n_rows, float(y_sub.mean()), dtype=float)\n",
    "        \n",
    "    # --- Attached formula (model disagreement) ---\n",
    "    brier_dis_sum  = brier_disagreement_sum(p_hat_magi, p_hat_lasso)\n",
    "    brier_dis_mean = brier_disagreement_mean(p_hat_magi, p_hat_lasso)\n",
    "    print(f\"[RESULT] Disagreement (Σ (MAGI-LASSO)^2) = {brier_dis_sum:.6f}\")\n",
    "    print(f\"[RESULT] Disagreement mean (1/k)*Σ (MAGI-LASSO)^2 = {brier_dis_mean:.6f}\")\n",
    "\n",
    "    # 95% bootstrap CI (α=0.05) for the mean disagreement\n",
    "    dis_mean, dis_std, dis_lo, dis_hi = bootstrap_ci_disagreement_mean(\n",
    "        p_hat_magi, p_hat_lasso, n_splits=1000, alpha=0.05, seed=RNG_SEED\n",
    "    )\n",
    "    print(f\"[BOOT] Disagreement mean: {dis_mean:.6f}  95%CI=({dis_lo:.6f}, {dis_hi:.6f})  std={dis_std:.6f}\")\n",
    "    \n",
    "    # ---- Bootstrap policy: run ONLY when EPV < 9 ----\n",
    "    run_bootstrap = (epv < RUN_BOOTSTRAP_EPV_THRESHOLD) and (X_used.shape[1] > 0)\n",
    "    \n",
    "    # Initialize ALL summary metrics so the dict build never NameErrors\n",
    "    AUC632_MAGI_mean = AUC632_MAGI_std = np.nan\n",
    "    AUC632_LASSO_mean = AUC632_LASSO_std = np.nan\n",
    "    ACC632_MAGI_mean = ACC632_MAGI_std = np.nan\n",
    "    ACC632_LASSO_mean = ACC632_LASSO_std = np.nan\n",
    "\n",
    "    if run_bootstrap:\n",
    "        magi_est = MagiBootstrapEstimatorCurrent(\n",
    "            intercept=intercept, coef_map=coef_map, feature_codes_all=feature_codes, dv_code=tcode,  \n",
    "            rank=RANK_MODE, require_present=False, min_total_count=1, min_pos_carriers=0, verbose=False\n",
    "        )\n",
    "        lasso_est = LassoBootstrapEstimatorCurrent(\n",
    "            feature_codes_all=feature_codes, coef_map=coef_map, dv_code=tcode,  \n",
    "            rank=RANK_MODE, require_present=False, min_total_count=1, min_pos_carriers=0,\n",
    "            C=0.5, max_iter=2000, n_jobs=-1, random_state=RNG_SEED, verbose=False\n",
    "        )\n",
    "        try:\n",
    "            auc632_magi  = bootstrap_632plus_or_fallback(\n",
    "                magi_est, X_sub, y_sub, N_BOOT, roc_auc_score, True, RNG_SEED, clone_estimator=False\n",
    "            )\n",
    "            auc632_lasso = bootstrap_632plus_or_fallback(\n",
    "                lasso_est, X_sub, y_sub, N_BOOT, roc_auc_score, True, RNG_SEED, clone_estimator=False\n",
    "            )\n",
    "            acc632_magi  = bootstrap_632plus_or_fallback(\n",
    "                magi_est, X_sub, y_sub, N_BOOT, accuracy_score, False, RNG_SEED, clone_estimator=False\n",
    "            )\n",
    "            acc632_lasso = bootstrap_632plus_or_fallback(\n",
    "                lasso_est, X_sub, y_sub, N_BOOT, accuracy_score, False, RNG_SEED, clone_estimator=False\n",
    "            )\n",
    "            \n",
    "            AUC632_MAGI_mean = float(np.mean(auc632_magi));  AUC632_MAGI_std  = float(np.std(auc632_magi))\n",
    "            AUC632_LASSO_mean = float(np.mean(auc632_lasso));AUC632_LASSO_std = float(np.std(auc632_lasso))\n",
    "            ACC632_MAGI_mean = float(np.mean(acc632_magi));  ACC632_MAGI_std  = float(np.std(acc632_magi))\n",
    "            ACC632_LASSO_mean = float(np.mean(acc632_lasso));ACC632_LASSO_std = float(np.std(acc632_lasso))\n",
    "            print(f\"[BOOT] MAGI  .632+ AUC: mean={AUC632_MAGI_mean:.4f}  std={AUC632_MAGI_std:.4f}\")\n",
    "            print(f\"[BOOT] LASSO .632+ AUC: mean={AUC632_LASSO_mean:.4f} std={AUC632_LASSO_std:.4f}\")\n",
    "            print(f\"[BOOT] MAGI  .632+ ACC: mean={ACC632_MAGI_mean:.4f}  std={ACC632_MAGI_std:.4f}\")\n",
    "            print(f\"[BOOT] LASSO .632+ ACC: mean={ACC632_LASSO_mean:.4f} std={ACC632_LASSO_std:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Bootstrap failed for {tcode}: {e}\")\n",
    "    else:\n",
    "        if X_used.shape[1] == 0:\n",
    "            print(\"[VAL] No predictors kept; skipping bootstrap.\")\n",
    "        else:\n",
    "            print(f\"[VAL] EPV > {RUN_BOOTSTRAP_EPV_THRESHOLD:.1f}: per spec, bootstrap not required; skipping.\")\n",
    "\n",
    "    # ---- Summary row ---------------------------------------------------------\n",
    "    rows.append({\n",
    "        \"target_code\": tcode,\n",
    "        \"target_name\": pretty,\n",
    "        \"n_cases\": int(n_rows),\n",
    "        \"n_pos\": int(n_pos),\n",
    "        \"n_neg\": int(n_neg),\n",
    "        \"rank_mode\": RANK_MODE,\n",
    "        \"features_used\": int(X_used.shape[1]),\n",
    "        \"EPV\": float(epv),\n",
    "        \"magi_nonzero_count\": int(magi_nz_feats.size),\n",
    "        \"lasso_nonzero_count\": int(nz.size) if 'nz' in locals() else 0,\n",
    "        \"AUC_MAGI\": float(auc_magi),\n",
    "        \"AUC_LASSO\": float(lasso_auc) if 'lasso_auc' in locals() and not np.isnan(lasso_auc) else np.nan,       \n",
    "        \"AUC632_MAGI_mean\":  AUC632_MAGI_mean,\n",
    "        \"AUC632_MAGI_std\":   AUC632_MAGI_std,\n",
    "        \"AUC632_LASSO_mean\": AUC632_LASSO_mean,\n",
    "        \"AUC632_LASSO_std\":  AUC632_LASSO_std,\n",
    "        \"ACC632_MAGI_mean\":  ACC632_MAGI_mean,\n",
    "        \"ACC632_MAGI_std\":   ACC632_MAGI_std,\n",
    "        \"ACC632_LASSO_mean\": ACC632_LASSO_mean,\n",
    "        \"ACC632_LASSO_std\":  ACC632_LASSO_std,\n",
    "        \"BrierDis_sum\":  brier_dis_sum,\n",
    "        \"BrierDis_mean\": brier_dis_mean,\n",
    "        \"BrierDis_mean_boot_mean\": dis_mean,\n",
    "        \"BrierDis_mean_boot_std\":  dis_std,\n",
    "        \"BrierDis_mean_boot_lo\":   dis_lo,\n",
    "        \"BrierDis_mean_boot_hi\":   dis_hi,\n",
    "        \"coef_csv\": coef_csv,\n",
    "        \"magi_used_coef_csv\": os.path.join(OUT_DIR, f\"magi_coef_used_{safe}.csv\"),\n",
    "        \"pred_csv_MAGI\": os.path.join(CSV_DIR, f\"pred_{safe}.csv\"),\n",
    "        \"pred_csv_LASSO\": os.path.join(CSV_DIR, f\"pred_{safe}_LASSO.csv\") if 'lasso_pred_csv' in locals() else \"\",\n",
    "        \"lasso_coef_csv\": os.path.join(OUT_DIR, f\"lasso_coef_{safe}.csv\") if 'lasso_coef_csv' in locals() else \"\",\n",
    "        \"roc_png_MAGI\": os.path.abspath(os.path.join(PNG_DIR, f\"ROC_{safe}.png\")),\n",
    "        \"roc_png_LASSO\": os.path.abspath(os.path.join(PNG_DIR, f\"ROC_{safe}_LASSO.png\")) if 'lasso_roc_png' in locals() else \"\",\n",
    "    })\n",
    "\n",
    "# -------------------- SAVE SUMMARY --------------------\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(SUMMARY_CSV, index=False)\n",
    "    # After you build `rows` (or right after selection), do something like:\n",
    "    n_pos_local = int(y_sub.sum())\n",
    "    n_min = max(1, int(np.floor(n_pos_local / float(EPV_TARGET))))\n",
    "    n_max = max(n_min, int(np.floor(n_pos_local / float(EPV_MIN))))\n",
    "    print(f\"[INFO] n_pos={n_pos_local}  \"\n",
    "          f\"n_min(pref@EPV≈{EPV_TARGET})={n_min}  n_max(EPV≥{EPV_MIN})={n_max}  \"\n",
    "          f\"kept={X_used.shape[1]}  EPV={epv:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n[SUMMARY] Nothing to save.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
