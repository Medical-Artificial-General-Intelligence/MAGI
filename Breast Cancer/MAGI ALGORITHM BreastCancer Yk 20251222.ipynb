{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sqlite3, math, sys, time, re\n",
    "from typing import Dict, Any, List, Optional, Iterable, Tuple, Set, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = os.getenv(\"MAGI_DB_PATH\", \"/projects/klybarge/pcori_ad/magi/magi_db/magiv2.db\")\n",
    "OUT_DIR = \"./BreastCancer\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_published\"   \n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "TOP_K = 500  \n",
    "   \n",
    "TARGETS = [\n",
    "    \"dx_SNOMED_254837009\",\n",
    "]\n",
    "\n",
    "def connect_in_read_only_mode(db_path):\n",
    "   \"\"\"\n",
    "   Connects to a SQLite database in read-only mode using a URI.\n",
    "   This is the safest way for multiple users to read from the same DB.\n",
    "   \"\"\"\n",
    "   \n",
    "   if not os.path.exists(db_path):\n",
    "       print(f\"❌ ERROR: Database file not found at: {db_path}\", file=sys.stderr)\n",
    "       return None\n",
    "\n",
    "   try:\n",
    "       # 1. Create a URI for the database file\n",
    "       #    'file:' prefix is necessary\n",
    "       #    '?mode=ro' tells SQLite to open in \"read-only\" mode\n",
    "       db_uri = f\"file:{db_path}?mode=ro\"\n",
    "        \n",
    "       print(f\"Connecting in read-only mode to: {db_uri}\")\n",
    "       \n",
    "       # 2. Connect using the uri=True flag\n",
    "       conn = sqlite3.connect(db_uri, uri=True)\n",
    "       print(\"✅ Connection successful.\")\n",
    "       return conn\n",
    "        \n",
    "   except Exception as e:\n",
    "       print(f\"❌ An unexpected error occurred: {e}\", file=sys.stderr)\n",
    "       return None\n",
    "\n",
    "def load_code_maps(conn):\n",
    "    \"\"\"\n",
    "    Load mapping between concept_code (string, e.g. 'dx_SNOMED_254645002')\n",
    "    and concept_code_int (internal INTEGER used in magi_counts_published).\n",
    "    \"\"\"\n",
    "    df_map = pd.read_sql_query(\n",
    "        \"SELECT concept_code_int, concept_code FROM concept_names\",\n",
    "        conn\n",
    "    )\n",
    "    code2int = dict(zip(df_map[\"concept_code\"], df_map[\"concept_code_int\"]))\n",
    "    int2code = dict(zip(df_map[\"concept_code_int\"], df_map[\"concept_code\"]))\n",
    "    return code2int, int2code\n",
    "\n",
    "# ===== SNOMED parents+descendants support =====\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "IS_A = \"116680003\"\n",
    "CHAR_TYPES = {\n",
    "    \"inferred\": \"900000000000011006\",\n",
    "    \"stated\":   \"900000000000010007\",\n",
    "}\n",
    "\n",
    "SNOMED_REL_FULL_US = \"/projects/klybarge/pcori_ad/magi/Test/Test/RareCancer/sct2_Relationship_Full_US1000124_20250901.txt\"\n",
    "__SNAP_REL__ = None\n",
    "__P2C__, __C2P__ = None, None\n",
    "\n",
    "def build_is_a_snapshot(full_rel_path: str, characteristic: str = \"inferred\") -> pd.DataFrame:\n",
    "    \"\"\"From a Full RF2 file, return the *current* active IS-A rows.\"\"\"\n",
    "    use_cols = [\n",
    "        \"id\",\"effectiveTime\",\"active\",\"moduleId\",\n",
    "        \"sourceId\",\"destinationId\",\"relationshipGroup\",\n",
    "        \"typeId\",\"characteristicTypeId\",\"modifierId\"\n",
    "    ]\n",
    "    df = pd.read_csv(full_rel_path, sep=\"\\t\", dtype=str, usecols=use_cols)\n",
    "    df = df[(df[\"typeId\"] == IS_A) & (df[\"characteristicTypeId\"] == CHAR_TYPES[characteristic])]\n",
    "    df[\"effectiveTime_num\"] = df[\"effectiveTime\"].astype(int)\n",
    "    idx = df.groupby(\"id\")[\"effectiveTime_num\"].idxmax()\n",
    "    snap = df.loc[idx]\n",
    "    snap = snap[snap[\"active\"] == \"1\"][[\"sourceId\",\"destinationId\"]].reset_index(drop=True)\n",
    "    return snap\n",
    "\n",
    "# Build both directions once\n",
    "def _ensure_graph():\n",
    "    global __SNAP_REL__, __P2C__, __C2P__\n",
    "    if __SNAP_REL__ is None:\n",
    "        __SNAP_REL__ = build_is_a_snapshot(SNOMED_REL_FULL_US, characteristic=\"inferred\")\n",
    "    if __P2C__ is None or __C2P__ is None:\n",
    "        __P2C__, __C2P__ = defaultdict(set), defaultdict(set)\n",
    "        for child, parent in zip(__SNAP_REL__[\"sourceId\"], __SNAP_REL__[\"destinationId\"]):\n",
    "            __P2C__[parent].add(child)\n",
    "            __C2P__[child].add(parent)\n",
    "\n",
    "def choose_umbrella_ancestor(root_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a SNOMED conceptId (e.g., '254645002'), choose an 'umbrella' ancestor:\n",
    "\n",
    "      - If there are no direct parents: return root_id.\n",
    "      - If there is 1 direct parent: return that parent.\n",
    "      - If there are multiple direct parents:\n",
    "            * For each direct parent, BFS upward and record distance\n",
    "              (0 = the parent itself).\n",
    "            * Take the intersection of ancestor sets = concepts above ALL parents.\n",
    "            * Among those common ancestors, choose the *lowest* one(s):\n",
    "                  - minimize max(distance_from_parent)\n",
    "                  - break ties by sum of distances\n",
    "                  - break remaining ties by conceptId for determinism.\n",
    "\n",
    "    This prefers *specific* common ancestors close to the parents (true LCA),\n",
    "    instead of very generic concepts that sit high in the hierarchy.\n",
    "    \"\"\"\n",
    "    _ensure_graph()\n",
    "\n",
    "    direct_parents = set(__C2P__.get(root_id, ()))\n",
    "    if not direct_parents:\n",
    "        # No parents at all → just use the original concept\n",
    "        return root_id\n",
    "\n",
    "    if len(direct_parents) == 1:\n",
    "        # Single parent → no conflict, just use it\n",
    "        return next(iter(direct_parents))\n",
    "\n",
    "    # ---- BFS upward from each parent, tracking distance ----\n",
    "    dist_maps = []  # one dict per parent: {conceptId: distance_from_this_parent}\n",
    "    ancestor_sets = []\n",
    "\n",
    "    for p in direct_parents:\n",
    "        dist = {p: 0}\n",
    "        q = deque([p])\n",
    "        while q:\n",
    "            cur = q.popleft()\n",
    "            for par in __C2P__.get(cur, ()):\n",
    "                if par not in dist:\n",
    "                    dist[par] = dist[cur] + 1\n",
    "                    q.append(par)\n",
    "        dist_maps.append(dist)\n",
    "        ancestor_sets.append(set(dist.keys()))\n",
    "\n",
    "    # Common ancestors across all parent branches\n",
    "    common_anc = set.intersection(*ancestor_sets) if ancestor_sets else set()\n",
    "\n",
    "    if not common_anc:\n",
    "        # In the unlikely event there is no common ancestor, fall back\n",
    "        # to a deterministic direct parent.\n",
    "        return sorted(direct_parents)[0]\n",
    "\n",
    "    # ---- Choose the *lowest* common ancestor ----\n",
    "    # Score each candidate by:\n",
    "    #   1) max distance from any parent  (smaller = lower/closer)\n",
    "    #   2) sum of distances              (tie-breaker)\n",
    "    #   3) conceptId (string)            (final deterministic tie-breaker)\n",
    "    best_cid = None\n",
    "    best_score = None\n",
    "\n",
    "    for cid in common_anc:\n",
    "        dists = []\n",
    "        valid = True\n",
    "        for dm in dist_maps:\n",
    "            if cid not in dm:\n",
    "                valid = False\n",
    "                break\n",
    "            dists.append(dm[cid])\n",
    "        if not valid:\n",
    "            continue\n",
    "\n",
    "        score = (max(dists), sum(dists), cid)\n",
    "        if best_score is None or score < best_score:\n",
    "            best_score = score\n",
    "            best_cid = cid\n",
    "\n",
    "    # Safety fallback: if something weird happened, fall back to a direct parent\n",
    "    if best_cid is None:\n",
    "        return sorted(direct_parents)[0]\n",
    "\n",
    "    return best_cid\n",
    "\n",
    "def find_descendants_sct(concept_id: str) -> set:\n",
    "    \"\"\"All is-a descendants for a conceptId.\"\"\"\n",
    "    _ensure_graph()\n",
    "    out, q = set(), deque([concept_id])\n",
    "    while q:\n",
    "        cur = q.popleft()\n",
    "        for kid in __P2C__.get(cur, ()):\n",
    "            if kid not in out:\n",
    "                out.add(kid); q.append(kid)\n",
    "    return out\n",
    "\n",
    "def find_parents_sct(concept_id: str) -> set:\n",
    "    _ensure_graph()\n",
    "    return set(__C2P__.get(concept_id, ()))\n",
    "\n",
    "# ===== MAP PREDEFINED TARGET → PARENT TARGET + DESCENDANT BLOCKLIST =====\n",
    "def extract_snomed_id(dx_code: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given something like 'dx_SNOMED_254645002', return '254645002'.\n",
    "    Return None if this is not a SNOMED dx code.\n",
    "    \"\"\"\n",
    "    prefix = \"dx_SNOMED_\"\n",
    "    if dx_code.startswith(prefix):\n",
    "        return dx_code[len(prefix):]\n",
    "    return None\n",
    "\n",
    "def _make_dx_snomed(concept_id: str) -> str:\n",
    "    \"\"\"Turn '254645002' back into 'dx_SNOMED_254645002'.\"\"\"\n",
    "    return f\"dx_SNOMED_{concept_id}\"\n",
    "\n",
    "def parent_target_and_blocklist_for_T(T: str) -> Tuple[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    For a predefined target T:\n",
    "\n",
    "      1) If T is a SNOMED dx:\n",
    "           - choose an 'umbrella' ancestor using choose_umbrella_ancestor:\n",
    "               • if no parents → use T itself\n",
    "               • if one parent → use that parent\n",
    "               • if multiple parents → pick a common ancestor (grandparent or higher)\n",
    "           - build a predictor blocklist consisting of:\n",
    "               • T itself\n",
    "               • all *siblings* of T (other children of the same parent(s))\n",
    "               • all *descendants of T*\n",
    "           (Parents / umbrella ancestors are NOT blocked and can still be predictors.)\n",
    "      2) If T is not SNOMED, just use T as outcome and an empty blocklist.\n",
    "\n",
    "    Returns (effective_outcome_code, predictor_blocklist_set_of_codes).\n",
    "    \"\"\"\n",
    "    snomed_id = extract_snomed_id(T)\n",
    "    # Non-SNOMED targets: no change\n",
    "    if snomed_id is None:\n",
    "        return T, set()\n",
    "\n",
    "    # --- 1. choose umbrella ancestor (or T itself if no parents) ---\n",
    "    umbrella_id = choose_umbrella_ancestor(snomed_id)\n",
    "    parent_outcome_code = _make_dx_snomed(umbrella_id)\n",
    "\n",
    "    # Ensure graph is built so we can see parents/children\n",
    "    _ensure_graph()\n",
    "\n",
    "    # --- 2. direct parents of T ---\n",
    "    direct_parents = set(__C2P__.get(snomed_id, ()))\n",
    "\n",
    "    # --- 3. siblings of T: all children of each parent, minus T itself ---\n",
    "    sibling_ids: Set[str] = set()\n",
    "    for p in direct_parents:\n",
    "        sibling_ids |= __P2C__.get(p, set())\n",
    "    sibling_ids.discard(snomed_id)\n",
    "\n",
    "    # --- 4. descendants of T ---\n",
    "    t_desc_ids = find_descendants_sct(snomed_id)\n",
    "\n",
    "    # --- 5. blocklist = T + siblings(T) + descendants(T) ---\n",
    "    block_ids = {snomed_id} | sibling_ids | t_desc_ids\n",
    "    blocklist: Set[str] = {_make_dx_snomed(cid) for cid in block_ids}\n",
    "\n",
    "    # Also ensure the exact T code is present (it already is if dx_SNOMED_, but harmless to add)\n",
    "    blocklist.add(T)\n",
    "\n",
    "    return parent_outcome_code, blocklist\n",
    "\n",
    "def snomed_aliases_for_outcome(outcome_code: str, include_parents: bool = True) -> Tuple[Optional[Set[str]], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Given an outcome like 'dx_SNOMED_254645002', return:\n",
    "      aliases_codes: {'dx_SNOMED_<id>', ...} including root and (optionally) parents\n",
    "      name_map:      every alias_code -> outcome_code (canonical), excluding the root itself\n",
    "    \"\"\"\n",
    "    root_id = extract_snomed_id(outcome_code)\n",
    "    if root_id is None:\n",
    "        return None, {}\n",
    "\n",
    "    desc = find_descendants_sct(root_id)\n",
    "    fam_ids = {root_id} | set(desc)\n",
    "    if include_parents:\n",
    "        fam_ids |= find_parents_sct(root_id)\n",
    "\n",
    "    aliases_codes: Set[str] = {f\"dx_SNOMED_{sid}\" for sid in fam_ids}\n",
    "    name_map: Dict[str, str] = {alias: outcome_code for alias in aliases_codes if alias != outcome_code}\n",
    "    return aliases_codes, name_map\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# MAGI core: analyze_causal_sequence_py (INT-BASED)\n",
    "# ======================================================================\n",
    "\n",
    "# RIGHT = k (predictor) and LEFT = Y/j\n",
    "def analyze_causal_sequence_py(\n",
    "    df_in: Union[str, pd.DataFrame],\n",
    "    *,\n",
    "    name_map: Dict[str, str] = None,     # kept for compatibility but IGNORED in _int version\n",
    "    events: List[int] = None,            # event IDs to KEEP; if None: auto-detect from *_int cols\n",
    "    force_outcome: int = None,           # if provided and present, force this to be the FINAL node (Y)\n",
    "    lambda_min_count: int = 15           # L-threshold for λ: if n_code < L ⇒ λ_{k,j}=0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    MAGI (Python, INT-BASED) – uses total_effects from DB for T, no 2×2 fallback.\n",
    "    All computations are done on target_concept_code_int / concept_code_int.\n",
    "\n",
    "    Rules:\n",
    "      • T_{kY}:\n",
    "            If `total_effects` column exists:\n",
    "                T_{kY} = mean(total_effects) from row(s) with (target=Y, code=k).\n",
    "                If those rows are missing or total_effects is NaN/≤0/±inf → T_{kY} = 1.\n",
    "            If `total_effects` column does NOT exist at all → T_{kY} = 1 for all k.\n",
    "      • Temporal score:\n",
    "            For each Zi:\n",
    "                score(Zi) = Σ_{Zj≠Zi} [ n_target_before_code(Zi,Zj) - n_code_before_target(Zi,Zj) ]\n",
    "            This is computed from the same counts as your original code,\n",
    "            just via a MultiIndex instead of repeated scans.\n",
    "      • λ_{k,j}:\n",
    "            λ_{k,j} = n_code_target(j,k) / n_code(j,k),\n",
    "            read from rows with (target=j, code=k), with L-threshold on n_code.\n",
    "    \"\"\"\n",
    "    # ── 0) Ingest & basic checks ───────────────────────────────────────────────\n",
    "    df = pd.read_csv(df_in) if isinstance(df_in, str) else df_in.copy()\n",
    "\n",
    "    need_cols = [\n",
    "        \"target_concept_code_int\", \"concept_code_int\",\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\",\n",
    "        \"n_target_no_code\",\n",
    "        \"n_code\",\n",
    "        \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required count columns for MAGI: {', '.join(missing)}\")\n",
    "\n",
    "    has_total_effects = \"total_effects\" in df.columns\n",
    "\n",
    "    # Ensure *_int are numeric / nullable ints\n",
    "    df[\"target_concept_code_int\"] = pd.to_numeric(df[\"target_concept_code_int\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"concept_code_int\"]        = pd.to_numeric(df[\"concept_code_int\"],        errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # name_map intentionally ignored in _int version\n",
    "\n",
    "    # Limit to selected events\n",
    "    if events is None:\n",
    "        arr_t = df[\"target_concept_code_int\"].dropna().unique()\n",
    "        arr_c = df[\"concept_code_int\"].dropna().unique()\n",
    "        # IntegerArray -> list of Python ints\n",
    "        ev_t = [int(x) for x in arr_t]\n",
    "        ev_c = [int(x) for x in arr_c]\n",
    "        events = sorted(set(ev_t) | set(ev_c))\n",
    "    else:\n",
    "        events = [int(e) for e in events]\n",
    "\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events.\")\n",
    "\n",
    "    df = df[\n",
    "        df[\"target_concept_code_int\"].isin(events)\n",
    "        & df[\"concept_code_int\"].isin(events)\n",
    "    ].copy()\n",
    "\n",
    "    # Coerce numerics\n",
    "    num_cols = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\", \"n_target_no_code\",\n",
    "        \"n_code\",\n",
    "        \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    if has_total_effects:\n",
    "        num_cols.append(\"total_effects\")\n",
    "\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # ── 1) Build an indexed edge table ────────────────────────────────────────\n",
    "    edge = (\n",
    "        df.groupby([\"target_concept_code_int\", \"concept_code_int\"], as_index=True)[\n",
    "            [\"n_target_before_code\", \"n_code_before_target\",\n",
    "             \"n_code_target\", \"n_code\"]\n",
    "        ].sum()\n",
    "    )\n",
    "\n",
    "    edge_targets = edge.index.get_level_values(0)\n",
    "\n",
    "    # ── 2) Temporal scores ────────────────────────────────────────────────────\n",
    "    scores: Dict[int, float] = {}\n",
    "    for zi in events:\n",
    "        if zi not in edge_targets:\n",
    "            scores[zi] = 0.0\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sub = edge.xs(zi, level=\"target_concept_code_int\")  # index = concept_code_int\n",
    "        except KeyError:\n",
    "            scores[zi] = 0.0\n",
    "            continue\n",
    "\n",
    "        s = float(\n",
    "            (sub[\"n_target_before_code\"].fillna(0.0) -\n",
    "             sub[\"n_code_before_target\"].fillna(0.0)).sum()\n",
    "        )\n",
    "        scores[zi] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Choose outcome Y\n",
    "    if (force_outcome is not None) and (force_outcome in sorted_scores.index):\n",
    "        outcome = int(force_outcome)\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "    else:\n",
    "        outcome = int(sorted_scores.index[0])\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "\n",
    "    events_order = temporal_order\n",
    "    nodes = events_order[:-1]\n",
    "\n",
    "    pos_by_event = {ev: i for i, ev in enumerate(events_order)}\n",
    "\n",
    "    # Pre-filter rows where LEFT == outcome for T_{kY}\n",
    "    if has_total_effects:\n",
    "        dfY = df[df[\"target_concept_code_int\"] == outcome].copy()\n",
    "        dfY[\"total_effects\"] = pd.to_numeric(dfY[\"total_effects\"], errors=\"coerce\")\n",
    "    else:\n",
    "        dfY = None\n",
    "\n",
    "    # ── 3) T and λ ─────────────────────────────────────────────────────────────\n",
    "    T_val = pd.Series(0.0, index=nodes, dtype=float)\n",
    "    D_val = pd.Series(np.nan, index=nodes, dtype=float)\n",
    "    lambda_l: Dict[int, pd.Series] = {}\n",
    "\n",
    "    for k in nodes:\n",
    "        # T_{kY}\n",
    "        if has_total_effects:\n",
    "            row_Yk = dfY[dfY[\"concept_code_int\"] == k]\n",
    "            if row_Yk.empty:\n",
    "                T_val.loc[k] = 1.0\n",
    "            else:\n",
    "                T_col = pd.to_numeric(row_Yk[\"total_effects\"], errors=\"coerce\")\n",
    "                T_col = T_col.replace([np.inf, -np.inf], np.nan)\n",
    "                T_clean = T_col.dropna()\n",
    "                if T_clean.empty:\n",
    "                    T_val.loc[k] = 1.0\n",
    "                else:\n",
    "                    T_db = float(T_clean.mean())\n",
    "                    if (not np.isfinite(T_db)) or (T_db <= 0):\n",
    "                        T_db = 1.0\n",
    "                    T_val.loc[k] = T_db\n",
    "        else:\n",
    "            T_val.loc[k] = 1.0\n",
    "\n",
    "        # λ_{k,j}\n",
    "        pos_k = pos_by_event[k]\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = {}\n",
    "        for j in js:\n",
    "            key = (j, k)\n",
    "            if key not in edge.index:\n",
    "                lam_pairs[j] = 0.0\n",
    "                continue\n",
    "\n",
    "            row_jk = edge.loc[key]\n",
    "            num = float(row_jk[\"n_code_target\"])\n",
    "            den = float(row_jk[\"n_code\"])\n",
    "\n",
    "            if (den <= 0) or (den < lambda_min_count):\n",
    "                lam_pairs[j] = 0.0\n",
    "                continue\n",
    "\n",
    "            lam = num / den\n",
    "            if not np.isfinite(lam):\n",
    "                lam = 0.0\n",
    "            lam_pairs[j] = float(min(max(lam, 0.0), 1.0))\n",
    "\n",
    "        lambda_l[k] = pd.Series(lam_pairs, dtype=float)\n",
    "\n",
    "    # ── 4) Backward recursion for D ────────────────────────────────────────────\n",
    "    if len(nodes) >= 1:\n",
    "        last_anc = nodes[-1]\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "\n",
    "    if len(nodes) > 1:\n",
    "        for k in list(reversed(nodes[:-1])):\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            downstream = list(lam_vec.index)\n",
    "            lam_vals = lam_vec.reindex(downstream).fillna(0.0).to_numpy()\n",
    "            D_down  = pd.to_numeric(D_val.reindex(downstream),\n",
    "                                    errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vals * D_down))\n",
    "            den = 1.0 - float(np.nansum(lam_vals))\n",
    "\n",
    "            if (not np.isfinite(den)) or den == 0.0:\n",
    "                D_val.loc[k] = T_val.loc[k]\n",
    "            else:\n",
    "                tmp = num / den\n",
    "                D_val.loc[k] = tmp if np.isfinite(tmp) else T_val.loc[k]\n",
    "\n",
    "    # ── 5) Logistic link (β) and predict_proba ─────────────────────────────────\n",
    "    resp_rows = df[df[\"target_concept_code_int\"] == outcome]\n",
    "    n_t = float(pd.to_numeric(resp_rows[\"n_target\"],      errors=\"coerce\").max()) if not resp_rows.empty else np.nan\n",
    "    n_n = float(pd.to_numeric(resp_rows[\"n_no_target\"],   errors=\"coerce\").max()) if not resp_rows.empty else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    p_y = min(max(p_y, 1e-12), 1 - 1e-12)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))\n",
    "\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    beta_vals = np.log(D_clean.where(D_clean > 0.0)) \\\n",
    "                    .replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_vals.index) + [\"(intercept)\"],\n",
    "        \"beta\":      list(beta_vals.values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    predictors = list(beta_vals.index)\n",
    "    beta_vec = beta_vals.values\n",
    "\n",
    "    def predict_proba(Z):\n",
    "        \"\"\"\n",
    "        Compute P(Y=1|Z) using: logit P = β0 + Σ_k β_k Z_k.\n",
    "        Here Z keys should be concept_code_int IDs.\n",
    "        \"\"\"\n",
    "        def sigmoid(x):\n",
    "            x = np.clip(x, -700, 700)\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "        if isinstance(Z, pd.DataFrame):\n",
    "            M = Z.reindex(columns=predictors, fill_value=0.0) \\\n",
    "                 .astype(float).to_numpy()\n",
    "            return sigmoid(beta_0 + M @ beta_vec)\n",
    "\n",
    "        if isinstance(Z, (dict, pd.Series)):\n",
    "            v = np.array([float(Z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            return float(sigmoid(beta_0 + float(v @ beta_vec)))\n",
    "\n",
    "        arr = np.asarray(Z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            return float(sigmoid(beta_0 + float(arr @ beta_vec)))\n",
    "        if arr.ndim == 2:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*,{len(predictors)}), got {arr.shape}\")\n",
    "            return sigmoid(beta_0 + arr @ beta_vec)\n",
    "\n",
    "        raise ValueError(\"Unsupported input for predict_proba\")\n",
    "\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": events_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "    \n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For DB-derived data (magi_counts_published):\n",
    "      - n_no_code_no_target is present and valid\n",
    "      - total_effects is already computed in the DB\n",
    "\n",
    "    This helper just coerces the relevant columns to numeric types.\n",
    "    No recomputation or fallback is done here.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    num_cols = [\n",
    "        \"n_code_target\",\n",
    "        \"n_code_no_target\",\n",
    "        \"n_target_no_code\",\n",
    "        \"n_target\",\n",
    "        \"n_no_target\",\n",
    "        \"n_no_code_no_target\",\n",
    "        \"n_code\",\n",
    "        \"n_code_before_target\",\n",
    "        \"n_target_before_code\",\n",
    "        \"total_effects\",\n",
    "    ]\n",
    "\n",
    "    out[num_cols] = out[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Ensure *_int are numeric\n",
    "    out[\"target_concept_code_int\"] = pd.to_numeric(out[\"target_concept_code_int\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    out[\"concept_code_int\"]        = pd.to_numeric(out[\"concept_code_int\"],        errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# TOPK_HI/LO\n",
    "def _select_top_by_te_unique_k(k_to_T: pd.DataFrame, top_k: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select top predictors for a single outcome T from its Y→k subgraph `k_to_T`,\n",
    "    using DB-provided total_effects_rank (dense rank on total_effects_norm\n",
    "    partitioned by target_concept_code_int).\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = TOP_K\n",
    "\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    if \"total_effects_rank\" not in df.columns:\n",
    "        raise ValueError(\"Expected 'total_effects_rank' in k_to_T for rank-based selection.\")\n",
    "\n",
    "    df[\"total_effects_rank\"] = pd.to_numeric(df[\"total_effects_rank\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effects_rank\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with valid total_effects_rank; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # One row per k_int (RIGHT side)\n",
    "    best_per_k = (\n",
    "        df.sort_values(\"total_effects_rank\", ascending=True)\n",
    "          .drop_duplicates(subset=[\"concept_code_int\"], keep=\"first\")\n",
    "    )\n",
    "\n",
    "    selected = best_per_k.nsmallest(top_k, \"total_effects_rank\").copy()\n",
    "\n",
    "    print(\n",
    "        f\"[SELECT] (rank-based) total unique k={best_per_k['concept_code_int'].nunique():,}  \"\n",
    "        f\"selected(total)={selected['concept_code_int'].nunique()}  top_k={top_k}\"\n",
    "    )\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "\n",
    "def _fetch_k_to_T(conn, outcome_int: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch all rows whose LEFT (target) == outcome_int (concept_code_int).\n",
    "    These rows provide the single (Y, k) lines needed to compute T_{kY}.\n",
    "    \"\"\"\n",
    "    q = f\"\"\"\n",
    "      SELECT *\n",
    "      FROM {EDGE_TABLE}\n",
    "      WHERE target_concept_code_int = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[int(outcome_int)])\n",
    "\n",
    "def _fetch_k_to_T_in(conn, target_ints: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch rows whose LEFT (target_concept_code_int) is in target_ints.\n",
    "    Used to include alias outcomes (descendants/parents of T) on the LEFT.\n",
    "    \"\"\"\n",
    "    if not target_ints:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ph = \",\".join([\"?\"] * len(target_ints))\n",
    "    q = f\"\"\"\n",
    "      SELECT *\n",
    "      FROM {EDGE_TABLE}\n",
    "      WHERE target_concept_code_int IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[int(x) for x in target_ints])\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list_int: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch the induced subgraph for the event set on the LEFT side (INT codes).\n",
    "\n",
    "    This captures all rows (X, k) where X ∈ events_set (as *_int),\n",
    "    regardless of what k is. Later we filter k to selected_k.\n",
    "    \"\"\"\n",
    "    if not events_list_int:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ph = \",\".join([\"?\"] * len(events_list_int))\n",
    "    q = f\"\"\"\n",
    "      SELECT *\n",
    "      FROM {EDGE_TABLE}\n",
    "      WHERE target_concept_code_int IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[int(e) for e in events_list_int])\n",
    "\n",
    "def expand_targets_with_descendants(targets: List[str]) -> Dict[str, Set[str]]:\n",
    "    out: Dict[str, Set[str]] = {}\n",
    "    for T in targets:\n",
    "        aliases, _name_map = snomed_aliases_for_outcome(T)\n",
    "        out[T] = aliases if aliases is not None else {T}\n",
    "    return out\n",
    "\n",
    "def print_full_targets(targets: List[str], *, save_csv: bool = True, out_dir: str = \"./\"):\n",
    "    \"\"\"\n",
    "    Pretty-print the expanded target definitions and (optionally) save a CSV with all rows:\n",
    "      root_target, alias_code, alias_conceptId, is_root\n",
    "    \"\"\"\n",
    "    expanded = expand_targets_with_descendants(targets)\n",
    "\n",
    "    # pretty print to console\n",
    "    for T, alias_set in expanded.items():\n",
    "        # count and show a short sample\n",
    "        n = len(alias_set)\n",
    "        sample = \", \".join(sorted(list(alias_set))[:10])\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"[TARGET] {T}\")\n",
    "        print(f\"  aliases (including descendants): {n}\")\n",
    "        print(f\"  sample: {sample}{' ...' if n > 10 else ''}\")\n",
    "\n",
    "    # optional CSV dump\n",
    "    if save_csv:\n",
    "        rows = []\n",
    "        for T, alias_set in expanded.items():\n",
    "            root_id = extract_snomed_id(T)\n",
    "            for alias in sorted(alias_set):\n",
    "                alias_id = extract_snomed_id(alias)\n",
    "                rows.append({\n",
    "                    \"root_target\": T,\n",
    "                    \"alias_code\": alias,\n",
    "                    \"alias_conceptId\": alias_id if alias_id else \"\",\n",
    "                    \"is_root\": (alias == T)\n",
    "                })\n",
    "        df = pd.DataFrame(rows, columns=[\"root_target\",\"alias_code\",\"alias_conceptId\",\"is_root\"])\n",
    "        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        path = os.path.join(out_dir, f\"targets_expanded_{ts}.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"\\n[SAVED] Full target expansions → {path}\")\n",
    "\n",
    "    return expanded\n",
    "\n",
    "# ========= main loop (remove dup) =========\n",
    "if __name__ == '__main__':\n",
    "    uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "    if not os.path.exists(MAGI_DB_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"MAGI_DB_PATH not found: {MAGI_DB_PATH}. \"\n",
    "            f\"Set MAGI_DB_PATH to a valid SQLite DB or run with CSV input.\"\n",
    "        )\n",
    "\n",
    "    # ===== BUILD PARENT/DESCENDANT CONFIG FOR ALL TARGETS (STRING LEVEL) =====\n",
    "    PARENT_CONFIG = {}\n",
    "    for T in TARGETS:  # T is 'aa_meas_citalopram_rem', etc.\n",
    "        parent_outcome, desc_blocklist = parent_target_and_blocklist_for_T(T)\n",
    "        PARENT_CONFIG[T] = {\n",
    "            \"parent_outcome\": parent_outcome,   # string dx_SNOMED_...\n",
    "            \"desc_blocklist\": desc_blocklist,   # set of string dx_SNOMED_...\n",
    "        }\n",
    "\n",
    "    print(\"\\n[SNOMED parent promotion]\")\n",
    "    for T in TARGETS:\n",
    "        cfg = PARENT_CONFIG[T]\n",
    "        if cfg[\"parent_outcome\"] != T:\n",
    "            print(\n",
    "                f\"  {T} → parent outcome {cfg['parent_outcome']}, \"\n",
    "                f\"desc_blocklist_size={len(cfg['desc_blocklist'])}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"  {T} (non-SNOMED or no parents) kept as outcome; \"\n",
    "                f\"desc_blocklist_size={len(cfg['desc_blocklist'])}\"\n",
    "            )\n",
    "\n",
    "    with sqlite3.connect(uri, uri=True) as conn:\n",
    "        # load mapping tables once\n",
    "        code2int, int2code = load_code_maps(conn)\n",
    "\n",
    "        # expanded targets: alias families (string level)\n",
    "        expanded_targets = print_full_targets(TARGETS, save_csv=True, out_dir=OUT_DIR)\n",
    "\n",
    "        for T in TARGETS:\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "            cfg = PARENT_CONFIG.get(T, {\"parent_outcome\": T, \"desc_blocklist\": set()})\n",
    "            outcome_T_str = cfg[\"parent_outcome\"]          # string\n",
    "            desc_blocklist_str = cfg[\"desc_blocklist\"]     # set[str]\n",
    "\n",
    "            # --- map outcome and blocklist to INTs ---\n",
    "            if outcome_T_str not in code2int:\n",
    "                print(f\"[WARN] Outcome {outcome_T_str} not found in concept_names; skipping.\")\n",
    "                continue\n",
    "            outcome_T_int = int(code2int[outcome_T_str])\n",
    "\n",
    "            desc_blocklist_int = {\n",
    "                int(code2int[c]) for c in desc_blocklist_str if c in code2int\n",
    "            }\n",
    "\n",
    "            print(\n",
    "                f\"[TARGET CONFIG] outcome_str = {outcome_T_str}, outcome_int = {outcome_T_int}, \"\n",
    "                f\"desc_blocklist_size_str = {len(desc_blocklist_str)}, \"\n",
    "                f\"desc_blocklist_size_int = {len(desc_blocklist_int)}\"\n",
    "            )\n",
    "\n",
    "            # 0) SNOMED alias family (string level)\n",
    "            aliases_codes_str = expanded_targets.get(T, set())\n",
    "            if aliases_codes_str:\n",
    "                alias_ints = {int(code2int[a]) for a in aliases_codes_str if a in code2int}\n",
    "            else:\n",
    "                alias_ints = set()\n",
    "\n",
    "            # 1) k→T rows: allow target_concept_code_int ∈ alias_ints (if SNOMED), else outcome_T_int only\n",
    "            if alias_ints:\n",
    "                k_to_T = _fetch_k_to_T_in(conn, sorted(alias_ints))\n",
    "            else:\n",
    "                k_to_T = _fetch_k_to_T(conn, outcome_T_int)\n",
    "\n",
    "            if k_to_T.empty:\n",
    "                print(f\"[WARN] No k→T rows for {T}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # derive / coerce\n",
    "            k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "            # 3) select predictors: top-K by TE / rank (one row per k_int)\n",
    "            sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "            if sel_rows.empty:\n",
    "                print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            selected_k = set(int(x) for x in sel_rows[\"concept_code_int\"].dropna().unique())\n",
    "\n",
    "            # remove descendants of T (INT blocklist)\n",
    "            if desc_blocklist_int:\n",
    "                before_sel = len(selected_k)\n",
    "                selected_k -= desc_blocklist_int\n",
    "                removed_sel = before_sel - len(selected_k)\n",
    "                print(\n",
    "                    f\"[FILTER] Removed {removed_sel} predictors in descendant \"\n",
    "                    f\"blocklist of {T}\"\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"[SELECT] unique k available={k_to_T['concept_code_int'].nunique():,}  \"\n",
    "                f\"selected={len(selected_k):,}\"\n",
    "            )\n",
    "            if not selected_k:\n",
    "                print(\n",
    "                    f\"[WARN] All candidate predictors for {T} \"\n",
    "                    f\"are in descendant blocklist; skipping.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # 4) build subgraph among {outcome_T_int} ∪ selected_k\n",
    "            events_set_int = selected_k | {outcome_T_int}\n",
    "\n",
    "            df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set_int))\n",
    "            df_trim = df_trim[\n",
    "                df_trim[\"target_concept_code_int\"].isin(events_set_int)\n",
    "                & df_trim[\"concept_code_int\"].isin(selected_k)\n",
    "            ].copy()\n",
    "\n",
    "            before = len(df_trim)\n",
    "            df_trim = df_trim.drop_duplicates()\n",
    "            after = len(df_trim)\n",
    "            if after < before:\n",
    "                print(\n",
    "                    f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  \"\n",
    "                    f\"(kept {after})\"\n",
    "                )\n",
    "\n",
    "            df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "            k_to_T_count = int((df_trim[\"target_concept_code_int\"] == outcome_T_int).sum())\n",
    "            T_to_j_count = int((df_trim[\"concept_code_int\"] == outcome_T_int).sum())\n",
    "            print(\n",
    "                f\"[TRIM] rows={len(df_trim):,}  events={len(events_set_int)}  \"\n",
    "                f\"k→T rows={k_to_T_count}  outcome_T→j rows={T_to_j_count}\"\n",
    "            )\n",
    "\n",
    "            sub_csv = os.path.join(\n",
    "                OUT_DIR,\n",
    "                f\"magi_subgraph_{T}_parent_{outcome_T_str}_int.csv\"\n",
    "            )\n",
    "            df_trim.to_csv(sub_csv, index=False)\n",
    "            print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "            # 6) run MAGI, forcing the INT outcome\n",
    "            try:\n",
    "                res = analyze_causal_sequence_py(\n",
    "                    df_trim,\n",
    "                    name_map=None,             # ignored in _int version\n",
    "                    events=None,               # let it auto-detect from *_int\n",
    "                    force_outcome=outcome_T_int,\n",
    "                    lambda_min_count=15,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"[ERROR] MAGI failed:\", e)\n",
    "                continue\n",
    "\n",
    "            # 7) save NON-ZERO coefficients with predictor mapped back to concept_code\n",
    "            coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "            coef_col = \"coef\" if \"coef\" in coef_df.columns else (\n",
    "                \"beta\" if \"beta\" in coef_df.columns else None\n",
    "            )\n",
    "            if coef_col is None:\n",
    "                raise KeyError(\n",
    "                    \"Coefficient column not found in coef_df \"\n",
    "                    \"(expected 'coef' or 'beta').\"\n",
    "                )\n",
    "\n",
    "            eps = 1e-12\n",
    "            coef_df[coef_col] = coef_df[coef_col].astype(float)\n",
    "            mask_nz = coef_df[coef_col].abs() > eps\n",
    "            coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "            pred_col = \"predictor\"\n",
    "            if pred_col not in coef_nz.columns:\n",
    "                raise KeyError(\"Expected 'predictor' column in coef_df.\")\n",
    "\n",
    "            # split intercept vs real predictors\n",
    "            is_intercept = coef_nz[pred_col].astype(str) == \"(intercept)\"\n",
    "            coef_intercept = coef_nz[is_intercept].copy()\n",
    "            coef_preds = coef_nz[~is_intercept].copy()\n",
    "\n",
    "            if not coef_preds.empty:\n",
    "                coef_preds[\"predictor_int\"] = coef_preds[pred_col].astype(int)\n",
    "                coef_preds[\"predictor\"] = coef_preds[\"predictor_int\"].map(int2code)\n",
    "                # fallback if some ints not in map\n",
    "                missing = coef_preds[\"predictor\"].isna()\n",
    "                if missing.any():\n",
    "                    coef_preds.loc[missing, \"predictor\"] = (\n",
    "                        coef_preds.loc[missing, \"predictor_int\"].astype(str)\n",
    "                    )\n",
    "            else:\n",
    "                coef_preds[\"predictor_int\"] = []\n",
    "                coef_preds[\"predictor\"] = []\n",
    "\n",
    "            if not coef_intercept.empty:\n",
    "                coef_intercept[\"predictor_int\"] = np.nan\n",
    "                coef_intercept[\"predictor\"] = \"(intercept)\"\n",
    "\n",
    "            coef_out = pd.concat([coef_intercept, coef_preds], ignore_index=True)\n",
    "\n",
    "            coef_dir = os.path.join(OUT_DIR, \"Coef_Yk\")\n",
    "            os.makedirs(coef_dir, exist_ok=True)\n",
    "\n",
    "            coef_csv = os.path.join(\n",
    "                coef_dir,\n",
    "                f\"magi_coef_{T}_parent_{outcome_T_str}_int_nonzero.csv\"\n",
    "            )\n",
    "            coef_out.to_csv(coef_csv, index=False)\n",
    "\n",
    "            print(\n",
    "                f\"[SAVED] Coefficients (non-zero only, descendant predictors removed={len(desc_blocklist_int)}) \"\n",
    "                f\"→ {coef_csv}  | kept={len(coef_out):,} of {len(coef_df):,}  \"\n",
    "                f\"| nodes={len(res.get('order_used', [])) - 1}\"\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
