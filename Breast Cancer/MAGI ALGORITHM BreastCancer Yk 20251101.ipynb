{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b60b86",
   "metadata": {},
   "source": [
    "1. Builds the expanded outcome family = {target SNOMED} ∪ descendants ∪ any Maps-to sources to any member of that set.\n",
    "\n",
    "2. Excludes candidate predictors that are in that family, map to that family, or have descendants intersecting that family.\n",
    "\n",
    "3. Runs the sibling test: if a predictor maps to the same standard as the target → drop; else compute Jaccard(k,Y) and PPV(Y|k) from your Y–k row and drop if they exceed thresholds (defaults: Jaccard ≥ 0.88, PPV ≥ 0.90; tweakable)\n",
    "\n",
    "4. Adds an optional name-based safety net with strict disease-name patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import math\n",
    "from typing import Dict, List, Union, Any, Optional, Set, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, re\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = os.getenv(\"MAGI_DB_PATH\", \"/projects/klybarge/pcori_ad/magi/magi_db/magi.db\")  # read-only; override via env\n",
    "OUT_DIR = \"./BreastCancer\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_top500\"   \n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "\n",
    "TOP_K = 500  # 500 for others\n",
    "\n",
    "OMOP_DB_PATH           = \"./omop_athena.db\"\n",
    "\n",
    "TARGETS = [\n",
    "    \"dx_SNOMED_254837009\",\n",
    "]\n",
    "\n",
    "# Updated 20251101 MAGI AUC=0.7172\n",
    "# T_{k,Y}  (target=Y, concept=k).\n",
    "# λ_{k,j}  (target=j, concept=k).\n",
    "# ===== SNOMED parents+descendants support =====\n",
    "IS_A = \"116680003\"\n",
    "CHAR_TYPES = {\n",
    "    \"inferred\": \"900000000000011006\",  # classification hierarchy\n",
    "    \"stated\":   \"900000000000010007\",\n",
    "}\n",
    "\n",
    "SNOMED_REL_FULL_US = \"/projects/klybarge/pcori_ad/magi/Test/Test/RareCancer/sct2_Relationship_Full_US1000124_20250901.txt\"\n",
    "\n",
    "def build_is_a_snapshot(full_rel_path: str, characteristic: str = \"inferred\") -> pd.DataFrame:\n",
    "    \"\"\"From a Full RF2 file, return the *current* active IS-A rows.\"\"\"\n",
    "    use_cols = [\n",
    "        \"id\",\"effectiveTime\",\"active\",\"moduleId\",\n",
    "        \"sourceId\",\"destinationId\",\"relationshipGroup\",\n",
    "        \"typeId\",\"characteristicTypeId\",\"modifierId\"\n",
    "    ]\n",
    "    df = pd.read_csv(full_rel_path, sep=\"\\t\", dtype=str, usecols=use_cols)\n",
    "    df = df[(df[\"typeId\"] == IS_A) & (df[\"characteristicTypeId\"] == CHAR_TYPES[characteristic])]\n",
    "    df[\"effectiveTime_num\"] = df[\"effectiveTime\"].astype(int)\n",
    "    idx = df.groupby(\"id\")[\"effectiveTime_num\"].idxmax()\n",
    "    snap = df.loc[idx]\n",
    "    snap = snap[snap[\"active\"] == \"1\"][[\"sourceId\",\"destinationId\"]].reset_index(drop=True)\n",
    "    return snap\n",
    "\n",
    "# Build parent→children and child→parent maps once (lazy init so the script still runs if file missing)\n",
    "__SNAP_REL__ = None\n",
    "__P2C__ = None\n",
    "__C2P__ = None\n",
    "\n",
    "def _ensure_graph():\n",
    "    global __SNAP_REL__, __P2C__, __C2P__\n",
    "    if __SNAP_REL__ is None:\n",
    "        __SNAP_REL__ = build_is_a_snapshot(SNOMED_REL_FULL_US, characteristic=\"inferred\")\n",
    "    if __P2C__ is None or __C2P__ is None:\n",
    "        __P2C__ = defaultdict(set)\n",
    "        __C2P__ = defaultdict(set)\n",
    "        # RF2: destinationId = parent, sourceId = child\n",
    "        for parent, child in zip(__SNAP_REL__[\"destinationId\"], __SNAP_REL__[\"sourceId\"]):\n",
    "            __P2C__[parent].add(child)\n",
    "            __C2P__[child].add(parent)\n",
    "\n",
    "def find_descendants_sct(concept_id: str) -> set:\n",
    "    \"\"\"All is-a descendants (children, grandchildren, ...) for a SNOMED conceptId.\"\"\"\n",
    "    _ensure_graph()\n",
    "    out = set()\n",
    "    q = deque([concept_id])\n",
    "    while q:\n",
    "        cur = q.popleft()\n",
    "        for kid in __P2C__.get(cur, ()):\n",
    "            if kid not in out:\n",
    "                out.add(kid)\n",
    "                q.append(kid)\n",
    "    return out\n",
    "\n",
    "def find_ancestors_sct(concept_id: str) -> set:\n",
    "    \"\"\"All is-a ancestors (parents, grandparents, ...) for a SNOMED conceptId.\"\"\"\n",
    "    _ensure_graph()\n",
    "    out = set()\n",
    "    q = deque([concept_id])\n",
    "    while q:\n",
    "        cur = q.popleft()\n",
    "        for mom in __C2P__.get(cur, ()):\n",
    "            if mom not in out:\n",
    "                out.add(mom)\n",
    "                q.append(mom)\n",
    "    return out\n",
    "\n",
    "def extract_snomed_id(code: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    If code looks like 'dx_SNOMED_<digits>' return '<digits>', else None.\n",
    "    \"\"\"\n",
    "    m = re.fullmatch(r\"dx_SNOMED_(\\d+)\", str(code))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def snomed_aliases_for_outcome(outcome_code: str, *, include_parents: bool = True) -> Tuple[Optional[Set[str]], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Given an outcome like 'dx_SNOMED_254645002', return:\n",
    "      - aliases_codes: {'dx_SNOMED_<id>', ...} including the root itself (or None if not SNOMED)\n",
    "      - name_map: mapping every alias_code -> outcome_code (canonical)\n",
    "    If include_parents=True, add all ancestors to the alias family as well.\n",
    "    \"\"\"\n",
    "    root_id = extract_snomed_id(outcome_code)\n",
    "    if root_id is None:\n",
    "        return None, {}\n",
    "\n",
    "    # descendants\n",
    "    desc = find_descendants_sct(root_id)\n",
    "    all_ids = {root_id} | set(desc)\n",
    "\n",
    "    # parents/ancestors (optional)\n",
    "    if include_parents:\n",
    "        ancs = find_ancestors_sct(root_id)\n",
    "        all_ids |= set(ancs)\n",
    "\n",
    "    aliases_codes: Set[str] = {f\"dx_SNOMED_{sid}\" for sid in all_ids}\n",
    "    # map every alias (except the canonical root code string itself) back to outcome_code\n",
    "    name_map: Dict[str, str] = {alias: outcome_code for alias in aliases_codes if alias != outcome_code}\n",
    "    return aliases_codes, name_map\n",
    "\n",
    "\n",
    "## Update 10 23 25\n",
    "def analyze_causal_sequence_py(\n",
    "    df_in: Union[str, pd.DataFrame],\n",
    "    *,\n",
    "    name_map: Dict[str, str] = None,     # raw_code -> friendly label (applied to BOTH columns)\n",
    "    events: List[str] = None,            # event names to KEEP (AFTER recoding). If None: auto-detect\n",
    "    force_outcome: str = None,           # if provided and present, force this to be the FINAL node (Y)\n",
    "    lambda_min_count: int = 15           # L-threshold for λ: if n_code < L ⇒ λ_{k,j}=0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    MAGI (Python): Reference routine with explicit comments.\n",
    "\n",
    "    ─────────────────────────────────────────────────────────────────────────────\n",
    "    COLUMN CONVENTION PER ROW:\n",
    "      Left  column: target_concept_code  (call this X for this row)\n",
    "      Right column: concept_code         (call this k for this row)\n",
    "\n",
    "      n_code_target        ≡  k ∧ X\n",
    "      n_code_no_target     ≡  k ∧ ¬X\n",
    "      n_target_no_code     ≡  ¬k ∧ X\n",
    "      n_no_target          ≡  total(¬X)\n",
    "      n_target             ≡  total(X)\n",
    "      n_code               ≡  total(k)\n",
    "      n_code_before_target ≡  count(k before X)\n",
    "      n_target_before_code ≡  count(X before k)\n",
    "\n",
    "    ORIENTATION (locked to your spec):\n",
    "      • Total effect T_{kY}:   read row (target = Y, code = k).\n",
    "      • Lambda     λ_{k,j}:    read row (target = j, code = k), and compute\n",
    "                               λ_{k,j} = n_code_target / n_code  with L-threshold on n_code.\n",
    "\n",
    "    TEMPORAL SCORE for each node Zi:\n",
    "      Score(Z_i) = Σ_{j≠i} [ C(Z_i≺Z_j) - C(Z_j≺Z_i) + C(Z_i∧¬Z_j) - C(Z_j∧¬Z_i) ]\n",
    "      Read from row (target=Z_i, code=Z_j):\n",
    "        - n_code_before_target      → C(Z_j≺Z_i)\n",
    "        - n_target_before_code      → C(Z_i≺Z_j)\n",
    "        - n_code_no_target          → C(Z_j∧¬Z_i)\n",
    "        - n_target_no_code          → C(Z_i∧¬Z_j)\n",
    "\n",
    "    T_{kY} from row (Y, k):\n",
    "      a = n_code_target        (k ∧ Y)\n",
    "      b = n_code_no_target     (k ∧ ¬Y)\n",
    "      c = n_target_no_code     (¬k ∧ Y)\n",
    "      d = n_no_target - b      (¬k ∧ ¬Y)   ← computed on the fly (no extra column needed)\n",
    "      With sample-size–anchored odds:\n",
    "         odds_k1 = a/b with guards; odds_k0 = c/d with guards; T = odds_k1 / odds_k0\n",
    "\n",
    "    DIRECT EFFECTS via backward recursion:\n",
    "      D_{k,Y} = ( T_{k,Y} - Σ_j λ_{k,j} D_{j,Y} ) / ( 1 - Σ_j λ_{k,j} ),\n",
    "      where j are downstream nodes between k and Y in the temporal order.\n",
    "\n",
    "    LOGISTIC LINK:\n",
    "      logit P(Y=1 | Z) = β0 + Σ_k β_k Z_k  with β_k = log D_{k,Y};\n",
    "      invalid/nonpositive D map to β_k=0.\n",
    "\n",
    "    RETURNS a dict with:\n",
    "      sorted_scores, temporal_order, order_used,\n",
    "      T_val, D_val, lambda_l, coef_df, beta_0, beta, logit_predictors, predict_proba\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 0) Ingest & validate ───────────────────────────────────────────────────\n",
    "    df = pd.read_csv(df_in) if isinstance(df_in, str) else df_in.copy()\n",
    "\n",
    "    need_cols = [\n",
    "        \"target_concept_code\", \"concept_code\",\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\",\n",
    "        \"n_target_no_code\",\n",
    "        \"n_code\",                                 # for λ denominator\n",
    "        \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # Optional recoding to friendly labels (applied to BOTH endpoints)\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "\n",
    "    # Limit to selected events (union if auto)\n",
    "    if events is None:\n",
    "        ev_t = df[\"target_concept_code\"].astype(str).unique().tolist()\n",
    "        ev_c = df[\"concept_code\"].astype(str).unique().tolist()\n",
    "        events = sorted(set(ev_t) | set(ev_c))\n",
    "    else:\n",
    "        # Normalize types to avoid silent mismatches\n",
    "        events = [str(e) for e in events]\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].astype(str)\n",
    "        df[\"concept_code\"] = df[\"concept_code\"].astype(str)\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events.\")\n",
    "\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numerics; NA→0 to allow safe sums/max\n",
    "    num_cols = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\", \"n_target_no_code\",\n",
    "        \"n_code\", \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # ── 1) Temporal score (read rows: target=Z_i, code=Z_j) ────────────────────\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zi in events:\n",
    "        s = 0.0\n",
    "        for zj in (x for x in events if x != zi):\n",
    "            # Row oriented as (Zi, Zj)\n",
    "            pair = df[(df[\"target_concept_code\"] == zi) & (df[\"concept_code\"] == zj)]\n",
    "            if pair.empty:\n",
    "                continue\n",
    "            c_i_before_j = float(pair[\"n_target_before_code\"].sum(skipna=True))   # Zi before Zj\n",
    "            c_j_before_i = float(pair[\"n_code_before_target\"].sum(skipna=True))   # Zj before Zi\n",
    "            c_i_and_not_j = float(pair[\"n_target_no_code\"].sum(skipna=True))      # Zi ∧ ¬Zj\n",
    "            c_j_and_not_i = float(pair[\"n_code_no_target\"].sum(skipna=True))      # Zj ∧ ¬Zi\n",
    "            s += (c_i_before_j - c_j_before_i + c_i_and_not_j - c_j_and_not_i)\n",
    "        scores[zi] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Choose outcome Y: either forced or top-scoring node\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "    else:\n",
    "        outcome = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "\n",
    "    events_order = temporal_order              # earliest … → Y\n",
    "    nodes = events_order[:-1]            # everything before Y\n",
    "\n",
    "    # ── 2) T and λ (row orientations locked) ───────────────────────────────────\n",
    "    T_val = pd.Series(0.0, index=nodes, dtype=float)\n",
    "    D_val = pd.Series(np.nan, index=nodes, dtype=float)\n",
    "    lambda_l: Dict[str, pd.Series] = {}\n",
    "\n",
    "    for k in nodes:\n",
    "        # ---- T_{kY} from the single row (target=Y, code=k) ----\n",
    "        row_Yk = df[(df[\"target_concept_code\"] == outcome) & (df[\"concept_code\"] == k)]\n",
    "\n",
    "        # 2×2 cells (a,b,c,d) at (Y,k):\n",
    "        a = float(row_Yk[\"n_code_target\"].sum(skipna=True))        # k ∧ Y\n",
    "        b = float(row_Yk[\"n_code_no_target\"].sum(skipna=True))     # k ∧ ¬Y\n",
    "        c = float(row_Yk[\"n_target_no_code\"].sum(skipna=True))     # ¬k ∧ Y\n",
    "        # d is not in data; compute from the same row: d = total(¬Y) − (k ∧ ¬Y)\n",
    "        n_noY = float(row_Yk[\"n_no_target\"].max(skipna=True)) if not row_Yk.empty else 0.0\n",
    "        d = max(n_noY - b, 0.0)                                    # ¬k ∧ ¬Y\n",
    "\n",
    "        # Stratum sizes\n",
    "        N1 = a + b                   # k = 1\n",
    "        N0 = c + d                   # k = 0\n",
    "\n",
    "        # Sample-size–anchored odds for k=1 and k=0\n",
    "        if N1 == 0:\n",
    "            odds_k1 = 1.0\n",
    "        else:\n",
    "            if a == 0:\n",
    "                odds_k1 = 1.0 / (N1 + 1.0)\n",
    "            elif b == 0:\n",
    "                odds_k1 = (N1 + 1.0)\n",
    "            else:\n",
    "                odds_k1 = a / b\n",
    "\n",
    "        if N0 == 0:\n",
    "            odds_k0 = 1.0\n",
    "        else:\n",
    "            if c == 0:\n",
    "                odds_k0 = 1.0 / (N0 + 1.0)\n",
    "            elif d == 0:\n",
    "                odds_k0 = (N0 + 1.0)\n",
    "            else:\n",
    "                odds_k0 = c / d\n",
    "\n",
    "        T_val.loc[k] = float(odds_k1 / odds_k0) if odds_k0 > 0 else (N1 + 1.0)\n",
    "\n",
    "        # ---- λ_{k,j} from rows (target=j, code=k): λ = n_code_target / n_code ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_jk = df[(df[\"target_concept_code\"] == j) & (df[\"concept_code\"] == k)]\n",
    "            if row_jk.empty:\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            num = float(pd.to_numeric(row_jk[\"n_code_target\"], errors=\"coerce\").fillna(0.0).sum())\n",
    "            den = float(pd.to_numeric(row_jk[\"n_code\"],        errors=\"coerce\").fillna(0.0).sum())\n",
    "\n",
    "            # L-threshold on the conditioning size n_code (count of k)\n",
    "            if (den <= 0) or (den < lambda_min_count):\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            lam = num / den\n",
    "            lam = 0.0 if not np.isfinite(lam) else float(min(max(lam, 0.0), 1.0))\n",
    "            lam_pairs.append((j, lam))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ── 3) Backward recursion for direct effects D ─────────────────────────────\n",
    "    # Last node (just before Y): no downstream → D = T\n",
    "    if len(nodes) >= 1:\n",
    "        last_anc = nodes[-1]\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "\n",
    "    # Walk backward for the rest\n",
    "    if len(nodes) > 1:\n",
    "        for k in list(reversed(nodes[:-1])):\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            downstream = list(lam_vec.index)  # j nodes after k (already resolved)\n",
    "            lam_vals = lam_vec.reindex(downstream).fillna(0.0).to_numpy()\n",
    "            D_down  = pd.to_numeric(D_val.reindex(downstream), errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vals * D_down))\n",
    "            den = 1.0 - float(np.nansum(lam_vals))\n",
    "\n",
    "            if (not np.isfinite(den)) or den == 0.0:\n",
    "                D_val.loc[k] = T_val.loc[k]            # neutralize if pathological\n",
    "            else:\n",
    "                tmp = num / den\n",
    "                D_val.loc[k] = tmp if np.isfinite(tmp) else T_val.loc[k]\n",
    "\n",
    "    # ── 4) Logistic link (β) and predict_proba ─────────────────────────────────\n",
    "    # Intercept β0 from marginal prevalence of Y (rows with target == Y)\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    n_t = float(resp_rows[\"n_target\"].max(skipna=True)) if not resp_rows.empty else np.nan\n",
    "    n_n = float(resp_rows[\"n_no_target\"].max(skipna=True)) if not resp_rows.empty else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    p_y = min(max(p_y, 1e-12), 1 - 1e-12)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))\n",
    "\n",
    "    # β_k = log D_{k,Y}; invalid/nonpositive → 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    beta_vals = np.log(D_clean.where(D_clean > 0.0)).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_vals.index) + [\"(intercept)\"],\n",
    "        \"beta\":      list(beta_vals.values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # Vectorized predict_proba\n",
    "    predictors = list(beta_vals.index)\n",
    "    beta_vec = beta_vals.values\n",
    "\n",
    "    def predict_proba(Z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Compute P(Y=1|Z) using: logit P = β0 + Σ_k β_k Z_k.\n",
    "        Z can be:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame containing any/all of `predictors` (others ignored)\n",
    "        \"\"\"\n",
    "        def sigmoid(x):\n",
    "            x = np.clip(x, -700, 700)  # numerical stability for large |η|\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "        if isinstance(Z, pd.DataFrame):\n",
    "            M = Z.reindex(columns=predictors, fill_value=0.0).astype(float).to_numpy()\n",
    "            return sigmoid(beta_0 + M @ beta_vec)\n",
    "\n",
    "        if isinstance(Z, (dict, pd.Series)):\n",
    "            v = np.array([float(Z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            return float(sigmoid(beta_0 + float(v @ beta_vec)))\n",
    "\n",
    "        arr = np.asarray(Z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            return float(sigmoid(beta_0 + float(arr @ beta_vec)))\n",
    "        if arr.ndim == 2:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*,{len(predictors)}), got {arr.shape}\")\n",
    "            return sigmoid(beta_0 + arr @ beta_vec)\n",
    "\n",
    "        raise ValueError(\"Unsupported input for predict_proba\")\n",
    "\n",
    "    # ── 5) Package results ─────────────────────────────────────────────────────\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "\n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only coerce numerics and compute total_effect if absent, using provided cells and\n",
    "    computing d = n_no_target - n_code_no_target on the fly (since d isn't in data).\n",
    "    \"\"\"\n",
    "    required = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target_no_code\", \"n_target\", \"n_no_target\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns for TE: {', '.join(missing)}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    for c in required:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    if \"total_effect\" not in out.columns:\n",
    "        a = out[\"n_code_target\"].astype(float)\n",
    "        b = out[\"n_code_no_target\"].astype(float)\n",
    "        c = out[\"n_target_no_code\"].astype(float)\n",
    "        d = (out[\"n_no_target\"] - out[\"n_code_no_target\"]).astype(float)\n",
    "        d = np.maximum(d, 0.0)\n",
    "\n",
    "        N1 = a + b\n",
    "        N0 = c + d\n",
    "\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            odds_k1 = np.where(\n",
    "                N1 == 0, 1.0,\n",
    "                np.where((a > 0) & (b > 0), a / b,\n",
    "                         np.where(b == 0, N1 + 1.0, 1.0 / (N1 + 1.0)))\n",
    "            )\n",
    "            odds_k0 = np.where(\n",
    "                N0 == 0, 1.0,\n",
    "                np.where((c > 0) & (d > 0), c / d,\n",
    "                         np.where(d == 0, N0 + 1.0, 1.0 / (N0 + 1.0)))\n",
    "            )\n",
    "            te = odds_k1 / odds_k0\n",
    "\n",
    "        out[\"total_effect\"] = np.where(np.isfinite(te), te, 1.0).astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# TOPK_HI/LO\n",
    "def _select_top_by_te_unique_k(k_to_T: pd.DataFrame, top_k: int = TOP_K,\n",
    "                               hi: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced selection without fallback:\n",
    "      - Take up to top_k//2 strongest risk (TE >= hi)\n",
    "      - Take up to top_k - top_k//2 strongest protective (TE <= 1/hi)\n",
    "      - If either side is short, do NOT fill from elsewhere; total may be < top_k\n",
    "      - Return one row per k (**concept_code**), ranked by extremeness\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    # Ensure total_effect numeric\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with total_effect; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # Extremeness symmetrical around 1\n",
    "    df[\"effect_strength\"] = np.where(df[\"total_effect\"] >= 1.0,\n",
    "                                     df[\"total_effect\"],\n",
    "                                     1.0 / df[\"total_effect\"])\n",
    "\n",
    "    # One row per k: **k is concept_code in (target=Y, code=k)**\n",
    "    best_per_k = (df.sort_values(\"effect_strength\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"concept_code\"], keep=\"first\"))\n",
    "\n",
    "    HI = hi\n",
    "    LO = 1.0 / hi\n",
    "\n",
    "    risk_pool = best_per_k[best_per_k[\"total_effect\"] >= HI].copy()\n",
    "    prot_pool = best_per_k[best_per_k[\"total_effect\"] <= LO].copy()\n",
    "\n",
    "    want_risk = top_k // 2\n",
    "    want_prot = top_k - want_risk\n",
    "\n",
    "    sel_risk = risk_pool.nlargest(min(len(risk_pool), want_risk), \"effect_strength\")\n",
    "    sel_prot = prot_pool.nlargest(min(len(prot_pool), want_prot), \"effect_strength\")\n",
    "\n",
    "    selected = pd.concat([sel_risk, sel_prot], ignore_index=True)\n",
    "\n",
    "    print(f\"[SELECT] total unique k={best_per_k['concept_code'].nunique():,}  \"\n",
    "          f\"risk={len(risk_pool):,}  prot={len(prot_pool):,}  \"\n",
    "          f\"selected(total)={selected['concept_code'].nunique()}  \"\n",
    "          f\"with: risk={len(sel_risk)}, prot={len(sel_prot)}\")\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _fetch_k_to_T(conn, outcome_code: str) -> pd.DataFrame:\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[outcome_code])\n",
    "\n",
    "def _fetch_k_to_T_in(conn, target_codes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch rows whose LEFT (target) is in target_codes.\n",
    "    This is used to include alias outcomes (descendants of T) on the LEFT.\n",
    "    \"\"\"\n",
    "    tmp_name = \"tmp_targets_magi\"\n",
    "    with conn:\n",
    "        conn.execute(f\"DROP TABLE IF EXISTS {tmp_name}\")\n",
    "        conn.execute(f\"CREATE TEMP TABLE {tmp_name}(concept_code TEXT)\")\n",
    "        conn.executemany(f\"INSERT INTO {tmp_name}(concept_code) VALUES (?)\", [(c,) for c in target_codes])\n",
    "\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- LEFT  (Y or Y-alias)\n",
    "             ccn.concept_code AS concept_code           -- RIGHT (k)\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      JOIN {tmp_name} tmp     ON tcn.concept_code         = tmp.concept_code   -- <<< LEFT filter\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn)\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list):\n",
    "    \"\"\"\n",
    "    Induced subgraph where LEFT is in events_list.\n",
    "    \"\"\"\n",
    "    ph = \",\".join([\"?\"] * len(events_list))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- LEFT\n",
    "             ccn.concept_code AS concept_code           -- RIGHT\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=list(events_list))\n",
    "\n",
    "def expand_targets_with_descendants(targets: List[str]) -> Dict[str, Set[str]]:\n",
    "    out: Dict[str, Set[str]] = {}\n",
    "    for T in targets:\n",
    "        aliases, _name_map = snomed_aliases_for_outcome(T, include_parents=True)\n",
    "        out[T] = aliases if aliases is not None else {T}\n",
    "    return out\n",
    "\n",
    "def print_full_targets(targets: List[str], *, save_csv: bool = True, out_dir: str = \"./\"):\n",
    "    \"\"\"\n",
    "    Pretty-print the expanded target definitions and (optionally) save a CSV with all rows:\n",
    "      root_target, alias_code, alias_conceptId, is_root\n",
    "    \"\"\"\n",
    "    expanded = expand_targets_with_descendants(targets)\n",
    "\n",
    "    # pretty print to console\n",
    "    for T, alias_set in expanded.items():\n",
    "        n = len(alias_set)\n",
    "        sample = \", \".join(sorted(list(alias_set))[:10])\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"[TARGET] {T}\")\n",
    "        print(f\"  aliases (parents + descendants + root): {n}\")\n",
    "        print(f\"  sample: {sample}{' ...' if n > 10 else ''}\")\n",
    "\n",
    "    # optional CSV dump\n",
    "    if save_csv:\n",
    "        rows = []\n",
    "        for T, alias_set in expanded.items():\n",
    "            root_id = extract_snomed_id(T)\n",
    "            for alias in sorted(alias_set):\n",
    "                alias_id = extract_snomed_id(alias)\n",
    "                rows.append({\n",
    "                    \"root_target\": T,\n",
    "                    \"alias_code\": alias,\n",
    "                    \"alias_conceptId\": alias_id if alias_id else \"\",\n",
    "                    \"is_root\": (alias == T)\n",
    "                })\n",
    "        df = pd.DataFrame(rows, columns=[\"root_target\",\"alias_code\",\"alias_conceptId\",\"is_root\"])\n",
    "        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        path = os.path.join(out_dir, f\"targets_expanded_{ts}.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"\\n[SAVED] Full target expansions → {path}\")\n",
    "\n",
    "    return expanded\n",
    "\n",
    "OMOP_DB_PATH = OMOP_DB_PATH if 'OMOP_DB_PATH' in globals() else './omop_athena.db'\n",
    "\n",
    "def _ensure_omop_attached(conn, omop_db_path: str):\n",
    "    # attach omop if not already\n",
    "    schemas = conn.execute(\"PRAGMA database_list\").fetchall()\n",
    "    if not any(row[1] == 'omop' for row in schemas):\n",
    "        conn.execute(f\"ATTACH DATABASE '{omop_db_path}' AS omop\")\n",
    "\n",
    "def _load_temp_codes(conn, table_name: str, codes: set):\n",
    "    conn.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    conn.execute(f\"CREATE TEMP TABLE {table_name}(concept_code TEXT PRIMARY KEY)\")\n",
    "    if codes:\n",
    "        conn.executemany(\n",
    "            f\"INSERT OR IGNORE INTO {table_name}(concept_code) VALUES (?)\",\n",
    "            [(str(c),) for c in codes]\n",
    "        )\n",
    "    conn.execute(f\"CREATE INDEX IF NOT EXISTS ix_{table_name}_code ON {table_name}(concept_code)\")\n",
    "\n",
    "def _fetch_subgraph_by_targets_fast(conn, events_set):\n",
    "    _load_temp_codes(conn, \"tmp_events\", set(map(str, events_set)))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      JOIN tmp_events te1    ON te1.concept_code          = tcn.concept_code\n",
    "      JOIN tmp_events te2    ON te2.concept_code          = ccn.concept_code\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn)\n",
    "\n",
    "\n",
    "# ====== OMOP \"Maps to\" helpers (defensive; return empty sets if tables missing) ======\n",
    "def _get_domain_vocab(conn, magi_code: str):\n",
    "    row = pd.read_sql_query(\"\"\"\n",
    "        SELECT domain_id, vocabulary_id\n",
    "        FROM omop.concept_names\n",
    "        WHERE concept_code = ?\n",
    "        LIMIT 1\n",
    "    \"\"\", conn, params=[magi_code])\n",
    "    if row.empty:\n",
    "        return None, None\n",
    "    return str(row[\"domain_id\"].iloc[0]), str(row[\"vocabulary_id\"].iloc[0])\n",
    "    \n",
    "def _get_maps_to_sources(conn, standard_codes: set) -> set:\n",
    "    \"\"\"\n",
    "    Given standard MAGI codes (e.g., dx_SNOMED_*), return all MAGI codes that map TO them.\n",
    "    Uses omop.concept_relationship with relationship_id in ('MAPS TO','MAPS TO VALUE').\n",
    "    Implemented via TEMP table join to avoid SQLite param limits.\n",
    "    \"\"\"\n",
    "    _ensure_omop_attached(conn, OMOP_DB_PATH)  # ensure 'omop' is attached and concept_names view exists\n",
    "    tmp = \"tmp_std_codes\"\n",
    "    _load_temp_codes(conn, tmp, standard_codes)\n",
    "\n",
    "    q = f\"\"\"\n",
    "      WITH std AS (\n",
    "        SELECT DISTINCT c.concept_id\n",
    "        FROM omop.concept_names cn\n",
    "        JOIN omop.concept c ON c.concept_id = cn.concept_id\n",
    "        JOIN {tmp} t        ON t.concept_code = cn.concept_code\n",
    "      )\n",
    "      SELECT DISTINCT src_cn.concept_code AS concept_code\n",
    "      FROM std\n",
    "      JOIN omop.concept_relationship cr ON cr.concept_id_2 = std.concept_id\n",
    "      JOIN omop.concept src            ON src.concept_id   = cr.concept_id_1\n",
    "      JOIN omop.concept_names src_cn   ON src_cn.concept_id= src.concept_id\n",
    "      WHERE cr.relationship_id IN ('Maps to','Maps to value','MAPS TO','MAPS TO VALUE')\n",
    "        AND IFNULL(cr.invalid_reason,'') = ''\n",
    "    \"\"\"\n",
    "    rows = pd.read_sql_query(q, conn)\n",
    "    return set(rows[\"concept_code\"].astype(str)) if not rows.empty else set()\n",
    "\n",
    "def _get_maps_to_targets(conn, src_codes: set, restrict_domain: str = None, restrict_vocab: str = None) -> set:\n",
    "    \"\"\"\n",
    "    Given MAGI codes (sources), return the MAGI-style standard codes they map TO.\n",
    "    Optional filters: restrict_domain/restrict_vocab on the STANDARD side.\n",
    "    Uses TEMP table join to avoid SQLite param limits.\n",
    "    \"\"\"\n",
    "    _ensure_omop_attached(conn, OMOP_DB_PATH)\n",
    "    tmp = \"tmp_src_codes\"\n",
    "    _load_temp_codes(conn, tmp, src_codes)\n",
    "\n",
    "    filt = []\n",
    "    if restrict_domain:\n",
    "        filt.append(\"UPPER(std.domain_id)=UPPER(?)\")\n",
    "    if restrict_vocab:\n",
    "        filt.append(\"UPPER(std.vocabulary_id)=UPPER(?)\")\n",
    "    where_extra = (\" AND \" + \" AND \".join(filt)) if filt else \"\"\n",
    "\n",
    "    params = []\n",
    "    if restrict_domain: params.append(restrict_domain)\n",
    "    if restrict_vocab:  params.append(restrict_vocab)\n",
    "\n",
    "    q = f\"\"\"\n",
    "      WITH src AS (\n",
    "        SELECT DISTINCT c.concept_id\n",
    "        FROM omop.concept_names cn\n",
    "        JOIN omop.concept c ON c.concept_id = cn.concept_id\n",
    "        JOIN {tmp} t        ON t.concept_code = cn.concept_code\n",
    "      )\n",
    "      SELECT DISTINCT std_cn.concept_code AS concept_code\n",
    "      FROM src\n",
    "      JOIN omop.concept_relationship cr ON cr.concept_id_1 = src.concept_id\n",
    "      JOIN omop.concept std            ON std.concept_id   = cr.concept_id_2\n",
    "      JOIN omop.concept_names std_cn   ON std_cn.concept_id= std.concept_id\n",
    "      WHERE cr.relationship_id IN ('Maps to','Maps to value','MAPS TO','MAPS TO VALUE')\n",
    "        AND IFNULL(cr.invalid_reason,'') = ''\n",
    "        {where_extra}\n",
    "    \"\"\"\n",
    "    rows = pd.read_sql_query(q, conn, params=params)\n",
    "    return set(rows[\"concept_code\"].astype(str)) if not rows.empty else set()\n",
    "\n",
    "def _name_like_disease(conn, codes: Set[str], like_patterns: List[str]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Drop guard: return codes whose concept_name matches any SQL LIKE pattern (case-insensitive).\n",
    "    Patterns should be raw SQL LIKE strings e.g., '%malignant neoplasm%breast%'.\n",
    "    \"\"\"\n",
    "    if not codes or not like_patterns:\n",
    "        return set()\n",
    "    try:\n",
    "        ph_codes = \",\".join([\"?\"] * len(codes))\n",
    "        ph_like  = \" OR \".join([f\"LOWER(concept_name) LIKE LOWER(?)\" for _ in like_patterns])\n",
    "        q = f\"\"\"\n",
    "          SELECT DISTINCT concept_code\n",
    "          FROM concept_names\n",
    "          WHERE concept_code IN ({ph_codes})\n",
    "            AND ({ph_like})\n",
    "        \"\"\"\n",
    "        params = list(codes) + like_patterns\n",
    "        rows = pd.read_sql_query(q, conn, params=params)\n",
    "        return set(rows[\"concept_code\"].astype(str)) if not rows.empty else set()\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "def _to_magi_sct(concept_id: str) -> str:\n",
    "    return f\"dx_SNOMED_{concept_id}\"\n",
    "\n",
    "def _from_magi_sct(magi_code: str) -> Optional[str]:\n",
    "    return extract_snomed_id(magi_code)\n",
    "\n",
    "def _descendants_of_codes(codes: Set[str]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    RF2-backed: for any MAGI codes, return all SNOMED descendants (as MAGI codes).\n",
    "    Non-SNOMED inputs are ignored.\n",
    "    \"\"\"\n",
    "    if not codes:\n",
    "        return set()\n",
    "    _ensure_graph()\n",
    "    out: Set[str] = set()\n",
    "    for code in codes:\n",
    "        sid = _from_magi_sct(code)\n",
    "        if not sid:\n",
    "            continue\n",
    "        for d in find_descendants_sct(sid):\n",
    "            out.add(_to_magi_sct(d))\n",
    "    return out\n",
    "\n",
    "def _descendants_intersect_family(k: str, family: Set[str]) -> bool:\n",
    "    \"\"\"\n",
    "    True if predictor k (SNOMED) has any descendant that lands in `family`.\n",
    "    \"\"\"\n",
    "    sid = _from_magi_sct(k)\n",
    "    if not sid:\n",
    "        return False\n",
    "    kids = find_descendants_sct(sid)\n",
    "    if not kids:\n",
    "        return False\n",
    "    kids_magi = {_to_magi_sct(x) for x in kids}\n",
    "    return bool(kids_magi & family)\n",
    "\n",
    "def _drop_k_with_descendants_in_family(conn, k_codes: set, family_codes: set) -> set:\n",
    "    # temp tables\n",
    "    _load_temp_codes(conn, \"tmp_k\", k_codes)\n",
    "    _load_temp_codes(conn, \"tmp_family\", family_codes)\n",
    "\n",
    "    q = \"\"\"\n",
    "      WITH k_ids AS (\n",
    "        SELECT cn.concept_id, cn.concept_code\n",
    "        FROM tmp_k t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "        WHERE cn.vocabulary_id='SNOMED' AND cn.domain_id='Condition'\n",
    "      ),\n",
    "      fam_ids AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_family t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "        WHERE cn.vocabulary_id='SNOMED' AND cn.domain_id='Condition'\n",
    "      )\n",
    "      SELECT DISTINCT k.concept_code\n",
    "      FROM k_ids k\n",
    "      JOIN omop.concept_ancestor ca ON ca.ancestor_concept_id = k.concept_id\n",
    "      JOIN fam_ids f               ON f.concept_id          = ca.descendant_concept_id\n",
    "    \"\"\"\n",
    "    rows = pd.read_sql_query(q, conn)\n",
    "    return set(rows[\"concept_code\"].astype(str)) if not rows.empty else set()\n",
    "\n",
    "def drop_related_predictors(conn, T: str, k_to_T: pd.DataFrame,\n",
    "                            alias_family_rf2: set = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes from k_to_T any predictor k that:\n",
    "      - is in the expanded outcome family (T ∪ parents ∪ descendants ∪ maps-to sources ∪ their descendants),\n",
    "      - maps to any concept in that family (same-standard peers),\n",
    "      - has SNOMED descendants intersecting the family.\n",
    "    \"\"\"\n",
    "    _ensure_omop_attached(conn, \"./omop_athena.db\")\n",
    "\n",
    "    # seed family: T plus RF2 family if you already computed it\n",
    "    family_seed = set(alias_family_rf2 or set()) | {T}\n",
    "    _load_temp_codes(conn, \"tmp_family_seed\", family_seed)\n",
    "\n",
    "    # all sources that map to the family (step 1: “any concept that Maps to the target (and their descendants)”)\n",
    "    q_src = \"\"\"\n",
    "      WITH fam AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_family_seed t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      )\n",
    "      SELECT DISTINCT src_cn.concept_code AS concept_code\n",
    "      FROM fam\n",
    "      JOIN omop.concept_relationship cr ON cr.concept_id_2 = fam.concept_id\n",
    "      JOIN omop.concept src            ON src.concept_id   = cr.concept_id_1\n",
    "      JOIN omop.concept_names src_cn   ON src_cn.concept_id= src.concept_id\n",
    "      WHERE cr.relationship_id IN ('Maps to','Maps to value','MAPS TO','MAPS TO VALUE')\n",
    "        AND IFNULL(cr.invalid_reason,'') = ''\n",
    "    \"\"\"\n",
    "    maps_to_sources = set(pd.read_sql_query(q_src, conn)[\"concept_code\"])\n",
    "\n",
    "    # descendants of those sources (SNOMED/Condition only)\n",
    "    _load_temp_codes(conn, \"tmp_maps_src\", maps_to_sources)\n",
    "    q_src_desc = \"\"\"\n",
    "      WITH src_ids AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_maps_src t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "        WHERE cn.vocabulary_id='SNOMED' AND cn.domain_id='Condition'\n",
    "      )\n",
    "      SELECT DISTINCT cn.concept_code\n",
    "      FROM src_ids s\n",
    "      JOIN omop.concept_ancestor ca ON ca.ancestor_concept_id = s.concept_id\n",
    "      JOIN omop.concept_names cn    ON cn.concept_id          = ca.descendant_concept_id\n",
    "    \"\"\"\n",
    "    maps_to_desc = set(pd.read_sql_query(q_src_desc, conn)[\"concept_code\"])\n",
    "\n",
    "    expanded_family = family_seed | maps_to_sources | maps_to_desc\n",
    "    _load_temp_codes(conn, \"tmp_family_all\", expanded_family)\n",
    "\n",
    "    # PREP: predictors set (RIGHT==k in your k→T)\n",
    "    k_codes = set(k_to_T[\"concept_code\"].astype(str))\n",
    "    _load_temp_codes(conn, \"tmp_k\", k_codes)\n",
    "\n",
    "    # (2a) drop k ∈ expanded family\n",
    "    drop_in_family = set(pd.read_sql_query(\"\"\"\n",
    "        SELECT t.concept_code FROM tmp_k t\n",
    "        INNER JOIN tmp_family_all f ON f.concept_code = t.concept_code\n",
    "    \"\"\", conn)[\"concept_code\"])\n",
    "\n",
    "    # (2b) drop k that maps-to any concept in expanded family\n",
    "    drop_maps_to_family = set(pd.read_sql_query(\"\"\"\n",
    "      WITH fam AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_family_all t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      ),\n",
    "      k_ids AS (\n",
    "        SELECT cn.concept_id AS k_id, cn.concept_code AS k_code\n",
    "        FROM tmp_k t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      )\n",
    "      SELECT DISTINCT k.k_code\n",
    "      FROM k_ids k\n",
    "      JOIN omop.concept_relationship cr ON cr.concept_id_1 = k.k_id\n",
    "      WHERE cr.relationship_id IN ('Maps to','Maps to value','MAPS TO','MAPS TO VALUE')\n",
    "        AND IFNULL(cr.invalid_reason,'') = ''\n",
    "        AND EXISTS (SELECT 1 FROM fam WHERE fam.concept_id = cr.concept_id_2)\n",
    "    \"\"\", conn)[\"k_code\"])\n",
    "\n",
    "    # (2c) drop k having SNOMED descendants intersecting expanded family\n",
    "    drop_desc_intersect = set(pd.read_sql_query(\"\"\"\n",
    "      WITH k_ids AS (\n",
    "        SELECT cn.concept_id, cn.concept_code\n",
    "        FROM tmp_k t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "        WHERE cn.vocabulary_id='SNOMED' AND cn.domain_id='Condition'\n",
    "      ),\n",
    "      fam_ids AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_family_all t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "        WHERE cn.vocabulary_id='SNOMED' AND cn.domain_id='Condition'\n",
    "      )\n",
    "      SELECT DISTINCT k.concept_code\n",
    "      FROM k_ids k\n",
    "      JOIN omop.concept_ancestor ca ON ca.ancestor_concept_id = k.concept_id\n",
    "      JOIN fam_ids f               ON f.concept_id          = ca.descendant_concept_id\n",
    "    \"\"\", conn)[\"concept_code\"])\n",
    "\n",
    "    # (3) sibling same-standard → exclude (map k to same standard in same domain/vocab as T)\n",
    "    td, tv = _get_domain_vocab(conn, T)  # your helper; returns ('Condition','SNOMED') for SNOMED dx\n",
    "    drop_same_std = set(pd.read_sql_query(\"\"\"\n",
    "      WITH fam AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_family_all t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      ),\n",
    "      k_ids AS (\n",
    "        SELECT cn.concept_id AS k_id, cn.concept_code AS k_code\n",
    "        FROM tmp_k t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      )\n",
    "      SELECT DISTINCT k.k_code\n",
    "      FROM k_ids k\n",
    "      JOIN omop.concept_relationship cr ON cr.concept_id_1 = k.k_id\n",
    "      JOIN omop.concept std            ON std.concept_id   = cr.concept_id_2\n",
    "      WHERE cr.relationship_id IN ('Maps to','Maps to value','MAPS TO','MAPS TO VALUE')\n",
    "        AND IFNULL(cr.invalid_reason,'') = ''\n",
    "        AND UPPER(std.domain_id)    = UPPER(?)\n",
    "        AND UPPER(std.vocabulary_id)= UPPER(?)\n",
    "        AND EXISTS (SELECT 1 FROM fam WHERE fam.concept_id = cr.concept_id_2)\n",
    "    \"\"\", conn, params=[td, tv])[\"k_code\"])\n",
    "\n",
    "    drops = drop_in_family | drop_maps_to_family | drop_desc_intersect | drop_same_std\n",
    "    keep  = k_codes - drops\n",
    "    return k_to_T[k_to_T[\"concept_code\"].isin(keep)].copy()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# ORIENTATION REMINDER (per your spec)\n",
    "#   LEFT  column  = target_concept_code\n",
    "#   RIGHT column  = concept_code\n",
    "#\n",
    "#   Total effect  T_{k,Y}  ⇒ read row where (target = Y,   code = k)\n",
    "#   Lambda        λ_{k,j}  ⇒ read row where (target = j,   code = k)\n",
    "#\n",
    "#   ⇒ Predictors k are ALWAYS on the RIGHT side (concept_code).\n",
    "#   \n",
    "#   Step-1 to seed with {T} ∪ descendants ∪ parents and then add Maps-to sources of any member of that set (but not descendants-of-sources).\n",
    "# --------------------------------------------------------------------\n",
    "def _apply_decision_tree_filters(\n",
    "    conn,\n",
    "    T: str,\n",
    "    k_to_T: pd.DataFrame,\n",
    "    *,\n",
    "    alias_family: Set[str],              # <- pass the {T}+descendants+parents set from main loop\n",
    "    jaccard_drop: float = 0.88,\n",
    "    ppv_drop: float = 0.90,\n",
    "    disease_like_patterns: Optional[List[str]] = None,\n",
    "    verbose: bool = True,\n",
    "    max_examples: int = 6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Decision tree Steps 1–4 (parents INCLUDED in Step-1 seed):\n",
    "\n",
    "      1) Expanded family = {T} ∪ descendants ∪ parents (RF2 alias_family you pass here)\n",
    "         ∪ MAPS-TO sources of any member of that set.\n",
    "         (No 'descendants of sources' expansion.)\n",
    "\n",
    "      2) Drop k if:\n",
    "           (a) k ∈ expanded family,\n",
    "           (b) k MAPS-TO any member of expanded family,\n",
    "           (c) k has DESCENDANTS (not parents) intersecting expanded family.\n",
    "\n",
    "      3) Sibling test:\n",
    "           • if k MAPS-TO the SAME standard as T (same domain+vocab) → drop\n",
    "           • else compute Jaccard(k,Y) and PPV(Y|k) from the Y→k row;\n",
    "             drop if Jaccard ≥ jaccard_drop or PPV ≥ ppv_drop.\n",
    "\n",
    "      4) Optional name-based filter (patterns).\n",
    "    \"\"\"\n",
    "    def _log(tag, items: Set[str]):\n",
    "        if not verbose: return\n",
    "        print(f\"[FILTER][{tag}] drop={len(items):,}\" +\n",
    "              (\"\" if not items else f\"  e.g., {', '.join(sorted(list(items))[:max_examples])}\"))\n",
    "\n",
    "    _ensure_omop_attached(conn, OMOP_DB_PATH)\n",
    "\n",
    "    # Universe of candidate predictors (RIGHT == k in Y→k rows)\n",
    "    k_codes_all: Set[str] = set(k_to_T[\"concept_code\"].astype(str))\n",
    "    if verbose:\n",
    "        print(f\"[FILTER][init] candidates={len(k_codes_all):,}\")\n",
    "\n",
    "    # ── Step 1: Expanded family (parents+descendants IN seed; no descendants-of-sources) ──\n",
    "    # alias_family is already {T} ∪ descendants ∪ parents from the main loop\n",
    "    outcome_family_std: Set[str] = set(alias_family) | {T}\n",
    "    _load_temp_codes(conn, \"tmp_family_seed\", outcome_family_std)\n",
    "\n",
    "    # Add Maps-to sources to any member of the seed\n",
    "    maps_to_sources: Set[str] = _get_maps_to_sources(conn, outcome_family_std)\n",
    "\n",
    "    # Expanded family: seed ∪ maps-to sources  (NO descendants-of-sources)\n",
    "    expanded_family: Set[str] = outcome_family_std | maps_to_sources\n",
    "    _load_temp_codes(conn, \"tmp_family_all\", expanded_family)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[FILTER][step1] seed(T+desc+parents)={len(outcome_family_std):,}  \"\n",
    "              f\"maps_to_sources={len(maps_to_sources):,}  \"\n",
    "              f\"expanded={len(expanded_family):,}\")\n",
    "\n",
    "    # ── Step 2: Structural drops ───────────────────────────────────────────────\n",
    "    _load_temp_codes(conn, \"tmp_k\", k_codes_all)\n",
    "\n",
    "    # (2a) k ∈ expanded family\n",
    "    drop_in_family = set(pd.read_sql_query(\"\"\"\n",
    "        SELECT t.concept_code\n",
    "        FROM tmp_k t\n",
    "        INNER JOIN tmp_family_all f ON f.concept_code = t.concept_code\n",
    "    \"\"\", conn)[\"concept_code\"])\n",
    "\n",
    "    # (2b) k MAPS-TO any member of expanded family\n",
    "    drop_maps_to_family = set(pd.read_sql_query(\"\"\"\n",
    "      WITH fam AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_family_all t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      ),\n",
    "      k_ids AS (\n",
    "        SELECT cn.concept_id AS k_id, cn.concept_code AS k_code\n",
    "        FROM tmp_k t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      )\n",
    "      SELECT DISTINCT k.k_code\n",
    "      FROM k_ids k\n",
    "      JOIN omop.concept_relationship cr ON cr.concept_id_1 = k.k_id\n",
    "      WHERE cr.relationship_id IN ('Maps to','Maps to value','MAPS TO','MAPS TO VALUE')\n",
    "        AND IFNULL(cr.invalid_reason,'') = ''\n",
    "        AND EXISTS (SELECT 1 FROM fam WHERE fam.concept_id = cr.concept_id_2)\n",
    "    \"\"\", conn)[\"k_code\"])\n",
    "\n",
    "    # (2c) k has DESCENDANTS (NOT parents) intersecting expanded family\n",
    "    drop_desc_intersect = set(pd.read_sql_query(\"\"\"\n",
    "      WITH k_ids AS (\n",
    "        SELECT cn.concept_id, cn.concept_code\n",
    "        FROM tmp_k t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "        WHERE cn.vocabulary_id='SNOMED' AND cn.domain_id='Condition'\n",
    "      ),\n",
    "      fam_ids AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_family_all t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "        WHERE cn.vocabulary_id='SNOMED' AND cn.domain_id='Condition'\n",
    "      )\n",
    "      SELECT DISTINCT k.concept_code\n",
    "      FROM k_ids k\n",
    "      JOIN omop.concept_ancestor ca ON ca.ancestor_concept_id = k.concept_id\n",
    "      JOIN fam_ids f               ON f.concept_id          = ca.descendant_concept_id\n",
    "    \"\"\", conn)[\"concept_code\"])\n",
    "\n",
    "    struct_drops = drop_in_family | drop_maps_to_family | drop_desc_intersect\n",
    "    _log(\"2a_in_family\", drop_in_family)\n",
    "    _log(\"2b_maps_to_family\", drop_maps_to_family)\n",
    "    _log(\"2c_desc_intersect_family\", drop_desc_intersect)\n",
    "\n",
    "    keep_after_struct = k_codes_all - struct_drops\n",
    "    if verbose:\n",
    "        print(f\"[FILTER][after step2] keep={len(keep_after_struct):,}\")\n",
    "    if not keep_after_struct:\n",
    "        return k_to_T.loc[[]].copy()\n",
    "\n",
    "    # ── Step 3: Sibling test ───────────────────────────────────────────────────\n",
    "    # (i) Same standard as T (same domain+vocab) → drop\n",
    "    t_domain, t_vocab = _get_domain_vocab(conn, T)\n",
    "    _load_temp_codes(conn, \"tmp_keep_struct\", keep_after_struct)\n",
    "    drop_same_std = set(pd.read_sql_query(\"\"\"\n",
    "      WITH fam AS (\n",
    "        SELECT cn.concept_id\n",
    "        FROM tmp_family_all t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      ),\n",
    "      k_ids AS (\n",
    "        SELECT cn.concept_id AS k_id, cn.concept_code AS k_code\n",
    "        FROM tmp_keep_struct t\n",
    "        JOIN omop.concept_names cn ON cn.concept_code = t.concept_code\n",
    "      )\n",
    "      SELECT DISTINCT k.k_code\n",
    "      FROM k_ids k\n",
    "      JOIN omop.concept_relationship cr ON cr.concept_id_1 = k.k_id\n",
    "      JOIN omop.concept std            ON std.concept_id   = cr.concept_id_2\n",
    "      WHERE cr.relationship_id IN ('Maps to','Maps to value','MAPS TO','MAPS TO VALUE')\n",
    "        AND IFNULL(cr.invalid_reason,'') = ''\n",
    "        AND UPPER(std.domain_id)     = UPPER(?)\n",
    "        AND UPPER(std.vocabulary_id) = UPPER(?)\n",
    "        AND EXISTS (SELECT 1 FROM fam WHERE fam.concept_id = cr.concept_id_2)\n",
    "    \"\"\", conn, params=[t_domain, t_vocab])[\"k_code\"])\n",
    "\n",
    "    _log(\"3a_same_standard_as_target\", drop_same_std)\n",
    "\n",
    "    keep_for_overlap = keep_after_struct - drop_same_std\n",
    "    if verbose:\n",
    "        print(f\"[FILTER][step3 prep] eligible for overlap test={len(keep_for_overlap):,}\")\n",
    "    if not keep_for_overlap:\n",
    "        return k_to_T.loc[[]].copy()\n",
    "\n",
    "    # (ii) Near-duplicate by Y→k overlap metrics (Jaccard & PPV)\n",
    "    k_rows = k_to_T[k_to_T[\"concept_code\"].isin(keep_for_overlap)].copy()\n",
    "    k_rows[\"a\"] = pd.to_numeric(k_rows[\"n_code_target\"], errors=\"coerce\").fillna(0.0)\n",
    "    k_rows[\"b\"] = pd.to_numeric(k_rows[\"n_code_no_target\"], errors=\"coerce\").fillna(0.0)\n",
    "    k_rows[\"c\"] = pd.to_numeric(k_rows[\"n_target_no_code\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    agg = (k_rows.groupby(\"concept_code\", as_index=False)[[\"a\",\"b\",\"c\"]].sum())\n",
    "    denom_jacc = (agg[\"a\"] + agg[\"b\"] + agg[\"c\"]).replace(0.0, np.nan)\n",
    "    denom_ppv  = (agg[\"a\"] + agg[\"b\"]).replace(0.0, np.nan)\n",
    "    agg[\"jaccard\"] = agg[\"a\"] / denom_jacc\n",
    "    agg[\"ppv\"]     = agg[\"a\"] / denom_ppv\n",
    "\n",
    "    near_dupe = set(agg.loc[(agg[\"jaccard\"] >= jaccard_drop) | (agg[\"ppv\"] >= ppv_drop), \"concept_code\"])\n",
    "    _log(\"3b_near_duplicate_overlap\", near_dupe)\n",
    "\n",
    "    keep_overlap = keep_for_overlap - near_dupe\n",
    "    if verbose:\n",
    "        print(f\"[FILTER][after step3] keep={len(keep_overlap):,}\")\n",
    "\n",
    "    # ── Step 4: Optional name filter ───────────────────────────────────────────\n",
    "    if disease_like_patterns:\n",
    "        name_hits = _name_like_disease(conn, keep_overlap, disease_like_patterns)\n",
    "        _log(\"4_name_based\", name_hits)\n",
    "        keep_final = keep_overlap - name_hits\n",
    "    else:\n",
    "        keep_final = keep_overlap\n",
    "        if verbose:\n",
    "            print(\"[FILTER][step4] skipped (no patterns)\")\n",
    "\n",
    "    if verbose:\n",
    "        dropped_total = len(k_codes_all - keep_final)\n",
    "        print(f\"[FILTER][summary] start={len(k_codes_all):,}  keep={len(keep_final):,}  drop_total={dropped_total:,}\")\n",
    "\n",
    "    return k_to_T[k_to_T[\"concept_code\"].isin(keep_final)].copy()\n",
    "\n",
    "\n",
    "# ========= main loop (Y→k for T_{k,Y}; j→k for λ_{k,j}) =========\n",
    "if __name__ == '__main__':\n",
    "    # 0) Runtime safety checks\n",
    "    uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "    if not os.path.exists(MAGI_DB_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"MAGI_DB_PATH not found: {MAGI_DB_PATH}. \"\n",
    "            \"Set MAGI_DB_PATH to a valid SQLite DB or run with CSV input.\"\n",
    "        )\n",
    "\n",
    "    with sqlite3.connect(uri, uri=True) as conn:\n",
    "        # Attach OMOP once (needed for maps-to & concept_ancestor SQL)\n",
    "        _ensure_omop_attached(conn, OMOP_DB_PATH)\n",
    "\n",
    "        # (Optional) log the expanded RF2 families (parents+descendants+root)\n",
    "        _ = print_full_targets(TARGETS, save_csv=True, out_dir=OUT_DIR)\n",
    "\n",
    "        for T in TARGETS:\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(f\"[RUN] Target (Y) = {T}\")\n",
    "\n",
    "            # 1) Pull rows with LEFT=Y and RIGHT=k (this is where we read T_{k,Y})\n",
    "            #    Orientation: T_{k,Y} uses row (target_concept_code==Y, concept_code==k)  ← Y→k\n",
    "            k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "            # DEDUP safeguard\n",
    "            before = len(k_to_T)\n",
    "            k_to_T = k_to_T.drop_duplicates()\n",
    "            after = len(k_to_T)\n",
    "            if after < before:\n",
    "                print(f\"[DEDUP] Y→k exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "            if k_to_T.empty:\n",
    "                print(f\"[WARN] No Y→k rows for {T}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 2) Add derived cells (incl. total_effect based on a,b,c,d with d computed on the fly)\n",
    "            k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "            # --- RF2 alias family for filtering seed (NO parents + descendants + root)\n",
    "            aliases_codes, _nm = snomed_aliases_for_outcome(T, include_parents=True)\n",
    "            alias_family = set(aliases_codes) if aliases_codes else {T}\n",
    "\n",
    "            # 3) Decision-tree filters (Steps 1–3 (+ optional 4)):\n",
    "            #    • Build expanded outcome family = {T} ∪ parents ∪ descendants ∪ maps-to sources ∪ descendants(sources)\n",
    "            #    • Drop k that are in family, map to family, OR whose parents/descendants intersect family\n",
    "            #    • Sibling test: if k maps to SAME standard (domain+vocab) as T → drop\n",
    "            #    • Else compute Jaccard(k,Y) & PPV(Y|k) from Y–k row; drop if Jaccard≥0.88 or PPV≥0.90\n",
    "            k_to_T = _apply_decision_tree_filters(\n",
    "                conn, T, k_to_T,\n",
    "                alias_family=alias_family,\n",
    "                jaccard_drop=0.88,\n",
    "                ppv_drop=0.90,\n",
    "                disease_like_patterns=None,  # set patterns list to enable optional step 4\n",
    "                verbose=True\n",
    "            )\n",
    "            if k_to_T.empty:\n",
    "                print(f\"[WARN] All predictors pruned by decision-tree for {T}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 4) Pick predictors by total_effect (RIGHT side = concept_code = k)\n",
    "            sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "            if sel_rows.empty:\n",
    "                print(f\"[WARN] No predictors selected for {T} after filtering; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Audit selected risk/protective list\n",
    "            os.makedirs(OUT_DIR, exist_ok=True)\n",
    "            audit_csv = os.path.join(OUT_DIR, f\"risk_prot_{T}.csv\")\n",
    "            sel_rows.to_csv(audit_csv, index=False)\n",
    "            print(f\"[SAVED] Factors (post-filters, pre-MAGI) → {audit_csv}\")\n",
    "\n",
    "            # Whitelist of predictors (do not include T itself)\n",
    "            selected_k = set(sel_rows[\"concept_code\"].astype(str))\n",
    "            if T in selected_k:\n",
    "                selected_k.remove(T)\n",
    "\n",
    "            print(f\"[SELECT] unique k available={k_to_T['concept_code'].nunique():,}  \"\n",
    "                  f\"selected={len(selected_k):,}\")\n",
    "            if not selected_k:\n",
    "                print(f\"[WARN] No predictors after removing target {T}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 5) Build subgraph for λ_{k,j}:\n",
    "            #    • LEFT ∈ {T} ∪ selected_k  (targets are Y and all j)\n",
    "            #    • RIGHT ∈ selected_k        (codes are predictors k only; exclude RIGHT=T)\n",
    "            events_set    = selected_k | {T}\n",
    "            right_allowed = selected_k\n",
    "\n",
    "            df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "            df_trim = df_trim[df_trim[\"concept_code\"].isin(right_allowed)].copy()\n",
    "\n",
    "            # DEDUP again on trimmed subgraph\n",
    "            before = len(df_trim)\n",
    "            df_trim = df_trim.drop_duplicates()\n",
    "            after = len(df_trim)\n",
    "            if after < before:\n",
    "                print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "            # Ensure derived columns present/consistent\n",
    "            df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "            # Sanity counts by orientation\n",
    "            # • Y→k rows present for TE\n",
    "            y_to_k = int((df_trim[\"target_concept_code\"] == T).sum())\n",
    "            # • j→k rows present for λ (targets not T, rights ∈ selected_k)\n",
    "            j_to_k = int(((df_trim[\"target_concept_code\"] != T) &\n",
    "                          (df_trim[\"concept_code\"].isin(right_allowed))).sum())\n",
    "            print(f\"[TRIM] rows={len(df_trim):,} events={len(events_set)}  Y→k={y_to_k}  j→k={j_to_k}\")\n",
    "\n",
    "            # Save subgraph (audit)\n",
    "            sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "            df_trim.to_csv(sub_csv, index=False)\n",
    "            print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "            # 6) Run MAGI with outcome forced to T\n",
    "            #    Internals:\n",
    "            #      • T_{k,Y} read from row (target=Y, code=k)         ← our Y→k rows\n",
    "            #      • λ_{k,j} read from row (target=j, code=k)         ← our j→k rows\n",
    "            try:\n",
    "                res = analyze_causal_sequence_py(\n",
    "                    df_trim,\n",
    "                    events=None,     # auto-detect from df_trim\n",
    "                    name_map=None,   # don't canonicalize aliases here\n",
    "                    force_outcome=T\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"[ERROR] MAGI failed:\", e)\n",
    "                continue\n",
    "\n",
    "            # 7) Save NON-ZERO coefficients, restricted to whitelist (selected_k) + intercept\n",
    "            outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "            coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "            # accept 'coef' or 'beta'\n",
    "            coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "            if coef_col is None:\n",
    "                raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "            eps = 1e-12\n",
    "            coef_df[coef_col] = pd.to_numeric(coef_df[coef_col], errors=\"coerce\").fillna(0.0)\n",
    "            coef_nz = coef_df[coef_df[coef_col].abs() > eps].copy()\n",
    "\n",
    "            # predictor column (usually 'predictor')\n",
    "            pred_col = \"predictor\" if \"predictor\" in coef_nz.columns else None\n",
    "            if pred_col is None:\n",
    "                # fallback: first non-coef column\n",
    "                maybe = [c for c in coef_nz.columns if c != coef_col]\n",
    "                pred_col = maybe[0] if maybe else None\n",
    "\n",
    "            # whitelist to selected_k (plus intercept); final guard: drop any alias-family codes if present\n",
    "            if pred_col:\n",
    "                is_intercept = coef_nz[pred_col].astype(str) == \"(intercept)\"\n",
    "                in_whitelist = coef_nz[pred_col].astype(str).isin(selected_k)\n",
    "                is_alias     = coef_nz[pred_col].astype(str).isin(alias_family)\n",
    "                keep_mask = is_intercept | (in_whitelist & ~is_alias)\n",
    "                dropped = int((~keep_mask).sum())\n",
    "                if dropped:\n",
    "                    print(f\"[FILTER] Dropping {dropped} non-whitelisted or alias-family predictor(s).\")\n",
    "                coef_nz = coef_nz.loc[keep_mask]\n",
    "\n",
    "            # Save\n",
    "            coef_dir = os.path.join(OUT_DIR, \"Coef_Yk\")\n",
    "            os.makedirs(coef_dir, exist_ok=True)\n",
    "            coef_csv = os.path.join(coef_dir, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "            coef_nz.to_csv(coef_csv, index=False)\n",
    "            print(f\"[SAVED] Coefficients (non-zero only) → {coef_csv}  \"\n",
    "                  f\"| kept={len(coef_nz):,} of {len(coef_df):,}  \"\n",
    "                  f\"| nodes={len(res.get('order_used', [])) - 1}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
